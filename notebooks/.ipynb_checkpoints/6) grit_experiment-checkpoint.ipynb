{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pytorch\n",
    "#cnnlstm on sequence level (not frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # order devices by bus id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # only make device 0 visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from platform import python_version\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import itertools\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import image as matplotimage\n",
    "from os import listdir\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import time\n",
    "import re\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "from torch.nn import Module\n",
    "from torch.nn import LSTM\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import orthogonal_\n",
    "from torch.nn.init import normal_\n",
    "from torch.nn.init import calculate_gain\n",
    "from torch.nn.init import zeros_\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, ChainDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torch import nn\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running python: 3.6.9\n"
     ]
    }
   ],
   "source": [
    "print(\"running python: \" + python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"Could not find CUDA, possibly encountering problems with current CUDA version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs available: \", torch.cuda.device_count()) # show number of cuda devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The currently selected GPU is number: 0 , it's a  NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print(\"The currently selected GPU is number:\", torch.cuda.current_device(),\n",
    "      \", it's a \", torch.cuda.get_device_name(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  1\n",
      "Device 0 : NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs available: \", torch.cuda.device_count())\n",
    "for device in range(torch.cuda.device_count()):\n",
    "    print(\"Device\",device, \":\", torch.cuda.get_device_name(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abort' 'circle' 'hello' 'no' 'stop' 'turn' 'turn_left' 'turn_right'\n",
      " 'warn']\n"
     ]
    }
   ],
   "source": [
    "classes = ['abort','circle', 'hello', 'no', 'stop', 'turn_left', 'turn_right', \n",
    "         'turn', 'warn']\n",
    "l_e = LabelEncoder()\n",
    "l_e.fit(classes)\n",
    "print(l_e.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on sequence level\n",
    "class GRITDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path='/path/to/grit_preprocessed_frames',\n",
    "                kendon_path='/path/to/grit_kendon',\n",
    "                participants=[1,2,3,4,5,6]\n",
    "                ):\n",
    "        self.x_samples = []\n",
    "        self.y_labels = []\n",
    "        self.gesture_peak = []\n",
    "        self.gesture_peak_original = []\n",
    "        self.x_lengths = []\n",
    "        self.file_names = []\n",
    "        # load all images in a directory\n",
    "        for folder in listdir(path): #abort, circle, etc.\n",
    "            if folder in classes:\n",
    "                print(folder)\n",
    "                for subfolder in listdir(os.path.join(path,folder)): #1_1, 1_2, etc.\n",
    "                    if int(subfolder[0]) in participants:\n",
    "                        print(subfolder)\n",
    "                        loaded_images = [] #has all images in folder\n",
    "                        loaded_labels = []\n",
    "                        for filename in sorted(listdir(os.path.join(path,folder,subfolder)) ,key=lambda x: int(os.path.splitext(x)[0])): #1, 2, etc.\n",
    "                            #print(filename)\n",
    "                            # load image\n",
    "                            img_file = Image.open(os.path.join(path, folder, subfolder, filename))\n",
    "                            img_data = np.asarray(img_file)\n",
    "                            # store loaded image  \n",
    "                            if (len(loaded_images) < 100): #100 or any large number, to put everything inside array\n",
    "                                loaded_images.append(np.transpose(img_data / 255.0))\n",
    "                                loaded_labels.append(folder)\n",
    "                            #print('> loaded %s %s' % (filename, img_data.shape))\n",
    "                        #print(np.shape(loaded_images))\n",
    "                        if (len(loaded_images) > 22): #22 here is the wanted sequence length\n",
    "                            #print(\"#frames: \", len(loaded_images)) \n",
    "                            #take first frames again\n",
    "                            loaded_images = loaded_images[:22]\n",
    "                            self.x_lengths.append(22)\n",
    "                        else:\n",
    "                            self.x_lengths.append(len(loaded_images))\n",
    "                            loaded_images = np.pad(loaded_images, ((22-len(loaded_images), 0),(0,0),(0,0)), 'constant', constant_values=[0])\n",
    "\n",
    "                        self.y_labels.append(folder)\n",
    "                        self.file_names.append(folder+subfolder)\n",
    "                        img_file_kendon = Image.open(os.path.join(kendon_path, folder, subfolder, \"1_hand.jpg\")).convert('L')\n",
    "                        self.gesture_peak_original.append(img_file_kendon)\n",
    "                        img_data_kendon = np.asarray(img_file_kendon)\n",
    "                        self.gesture_peak.append(np.reshape(np.transpose(img_data_kendon / 255.0), (1, 64, 48)))\n",
    "                        loaded_images = np.array(loaded_images)\n",
    "                        #loaded_images = np.append(loaded_images, np.transpose(img_data_kendon / 255.0))\n",
    "                        print(np.shape(loaded_images))\n",
    "                        loaded_images = np.reshape(loaded_images, (-1, 1, 64, 48))\n",
    "                        print(np.shape(loaded_images))\n",
    "                        self.x_samples.append(loaded_images)\n",
    "        self.y_labels = l_e.fit_transform(self.y_labels)\n",
    "    def __len__(self):\n",
    "        return len(self.x_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #return self.x_samples[idx], self.y_labels[idx]\n",
    "        return self.x_samples[idx], self.y_labels[idx], self.gesture_peak[idx]#, self.gesture_peak_original[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circle\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "hello\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "warn\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "no\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "turn_right\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "stop\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "turn\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_12\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "abort\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "turn_left\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n"
     ]
    }
   ],
   "source": [
    "gritdataset = GRITDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 22, 1, 64, 48)\n",
      "(543,)\n",
      "(543, 1, 64, 48)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(gritdataset.x_samples))\n",
    "print(np.shape(gritdataset.y_labels))\n",
    "print(np.shape(gritdataset.gesture_peak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 22, 1, 64, 48)\n",
      "(1, 64, 48)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8f430d7a90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.shape(gritdataset.x_samples))\n",
    "print(np.shape(gritdataset.x_samples[0][10]))\n",
    "plt.imshow(np.transpose(np.reshape(gritdataset.x_samples[10][21], (64, 48))), cmap=\"gray\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting sequences as wholes\n",
    "train_size = int(0.7 * len(gritdataset))\n",
    "test_size = len(gritdataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(gritdataset, [train_size, test_size], generator=torch.Generator().manual_seed(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 22, 1, 64, 48)\n",
      "(543,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(gritdataset.x_samples))\n",
    "print(np.shape(gritdataset.y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "cnn_init1 = None\n",
    "linear_init = None\n",
    "cnn_init5 = None\n",
    "linear_init64 = None\n",
    "linear_init1650 = None\n",
    "lstm_init = None\n",
    "def weights_init(m):\n",
    "    global linear_init\n",
    "    global linear_init64\n",
    "    global linear_init1650\n",
    "    global lstm_init\n",
    "    global cnn_init1\n",
    "    global cnn_init5\n",
    "    if isinstance(m, Conv2d):\n",
    "        zeros_(m.bias)\n",
    "        if m.in_channels == 1:\n",
    "            if cnn_init1 is None:\n",
    "                cnn_init1 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = cnn_init1\n",
    "        elif m.in_channels == 5:\n",
    "            if cnn_init5 is None:\n",
    "                cnn_init5 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = cnn_init5\n",
    "    elif isinstance(m, Linear):\n",
    "        zeros_(m.bias)\n",
    "        if m.in_features == 64:\n",
    "            if linear_init64 is None:\n",
    "                linear_init64 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init64\n",
    "        elif m.in_features == 1650:\n",
    "            if linear_init1650 is None:\n",
    "                linear_init1650 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init1650\n",
    "        else:\n",
    "            if linear_init is None:\n",
    "                linear_init = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init\n",
    "        \n",
    "    elif isinstance(m, LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                    zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                if lstm_init is None:\n",
    "                    lstm_init = xavier_uniform_(param)\n",
    "                else:\n",
    "                    param = lstm_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 5, (3,3))\n",
    "        weights_init(self.conv1)\n",
    "        self.act1 = Tanh()\n",
    "        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = Conv2d(5, 10, (2,2))\n",
    "        weights_init(self.conv2)\n",
    "        self.act2 = Tanh()\n",
    "        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc1 = Linear(10*15*11, 500)\n",
    "        weights_init(self.fc1)\n",
    "        self.act3 = Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.pool1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.pool2(X)\n",
    "        X = X.view(-1, 10*15*11)\n",
    "        X = self.fc1(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snap(Module):\n",
    "    def __init__(self):\n",
    "        super(Snap, self).__init__()\n",
    "        #init vals?\n",
    "        self.conv1 = Conv2d(1, 5, (3,3))\n",
    "        self.act1 = Tanh()\n",
    "        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = Conv2d(5, 10, (2,2))\n",
    "        self.act2 = Tanh()\n",
    "        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.conv1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.pool1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.pool2(X)\n",
    "        X = X.view(-1, 10*15*11)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence level\n",
    "class CNNLSTM(Module):        \n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=500, \n",
    "            hidden_size=64,\n",
    "            hidden_size_snapture=200,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            num_units=64,\n",
    "            num_units_snapture=1714,\n",
    "            num_classes=9,\n",
    "            snapture=False\n",
    "    ):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        #cnn for static part\n",
    "        if snapture:\n",
    "            #cnn for frames\n",
    "            self.snap = Snap().to('cuda:0')\n",
    "            self.snap.double()\n",
    "    \n",
    "        #cnn for frames\n",
    "        self.cnn = CNN().to('cuda:0')\n",
    "        self.cnn.double()\n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.num_units=num_units\n",
    "        self.num_classes=num_classes\n",
    "        self.snapture=snapture\n",
    "        \n",
    "        self.rnn = LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first).to('cuda:0')\n",
    "        \n",
    "        weights_init(self.rnn)\n",
    "       \n",
    "        self.rnn.double()\n",
    "        self.linear = Linear(num_units, num_classes).to('cuda:0')\n",
    "        weights_init(self.linear)\n",
    "        self.linear.double()\n",
    "        if snapture:\n",
    "            self.act3 = Tanh()\n",
    "            self.linear2 = Linear(num_units_snapture,num_classes)\n",
    "            self.linear2.double()\n",
    "            self.linear2.to('cuda:0')\n",
    "\n",
    "    def forward(self, x, gesture_peak):\n",
    "        x = x.contiguous()\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        \n",
    "        r_out_check_activation = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        if self.snapture:\n",
    "            use_snapture=np.ones((9), dtype=bool)\n",
    "            max_activation_index = torch.argmax(r_out_check_activation).item()\n",
    "            if max_activation_index in [classes.index('circle'), classes.index('turn'), classes.index('warn')]:\n",
    "                use_snapture[max_activation_index] = False\n",
    "                \n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        if self.snapture:        \n",
    "            for iitem, item in enumerate(use_snapture):\n",
    "                if item:\n",
    "                    gesture_peak_maps = self.snap(gesture_peak)\n",
    "                    gesture_peak_maps = torch.cat((r_out[:, -1, :], gesture_peak_maps), dim=1)\n",
    "                    r_out2[iitem] = self.linear2(gesture_peak_maps[iitem])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels, peak) in enumerate(train_loader):\n",
    "            images, labels, peak = images.to('cuda:0'), labels.to('cuda:0'), peak.to('cuda:0')\n",
    "            outputs = model(images.double(), peak.double()).to('cuda:0') #statless\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for val_i, (val_images, val_labels, val_peak) in enumerate(val_loader):\n",
    "                    val_images, val_labels, val_peak = val_images.to('cuda:0'), val_labels.to('cuda:0'), val_peak.to('cuda:0')\n",
    "                    val_outputs = model(val_images.double(), val_peak.double()).to('cuda:0')\n",
    "                    temp = criterion(val_outputs, val_labels)\n",
    "                    val_loss+=temp.item()\n",
    "                val_loss = val_loss/len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%2 == 0:\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', loss, '\\t', 'val_loss :', val_loss)\n",
    "    return loss_list, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = torch.zeros(9, 9)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels, peak in test_loader:\n",
    "            data, labels, peak = data.to('cuda:0'), labels.to('cuda:0'), peak.to('cuda:0')\n",
    "            predictions = model(data.double(), peak.double()).to('cuda:0') #statelss\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc_on_test = float(num_correct)/float(total)\n",
    "    print(f\"Test Accuracy of the model: {acc_on_test*100:.2f}\")\n",
    "    return acc_on_test, [], [], confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=1650, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (rnn): LSTM(500, 64, num_layers=2, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = CNNLSTM()\n",
    "model.to('cuda:0')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_grit(num_trials, cv_split=None):\n",
    "    num_epochs = 40\n",
    "    \n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    all_confusion = []\n",
    "    all_lost = []\n",
    "    all_val_lost=[]\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        trial_scores = []\n",
    "        trial_times = []\n",
    "        trial_predictions = []\n",
    "        trial_ground_truth = []\n",
    "        trial_conf = []\n",
    "        trial_lost = []\n",
    "        trial_val_lost=[]\n",
    "        all_y = np.array([y for x, y, z in iter(gritdataset)])\n",
    "        cv_folds = StratifiedKFold(n_splits=cv_split, shuffle=True, random_state=i)\n",
    "        for cv_fold, (train_indices, test_indices) in enumerate(cv_folds.split(gritdataset, all_y)):\n",
    "            train_dataset = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "            test_dataset = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "            train_loader = DataLoader(gritdataset, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=train_dataset)\n",
    "            test_loader = DataLoader(gritdataset, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=test_dataset)\n",
    "\n",
    "            # defining the model\n",
    "            model = CNNLSTM()\n",
    "            model.to('cuda:0')\n",
    "            optimizer = Adam(model.parameters(), lr=0.001)\n",
    "            # defining the loss function\n",
    "            criterion = CrossEntropyLoss()\n",
    "            #print(model)\n",
    "            \n",
    "            start = time.process_time() \n",
    "            loss_list, val_losses = train_model(model, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "            trial_times.append(time.process_time() - start)\n",
    "            acc, preds, labels, confusion_matrix = test_model(model, test_loader, device)\n",
    "            trial_conf.append(confusion_matrix)\n",
    "            trial_predictions.append(preds)\n",
    "            trial_ground_truth.append(labels)\n",
    "            trial_scores.append(acc) #whole_sequence\n",
    "            trial_lost.append(loss_list)\n",
    "            trial_val_lost.append(val_losses)\n",
    "        all_lost.append(loss_list)\n",
    "        all_val_lost.append(val_losses)\n",
    "        test_scores.append(trial_scores)\n",
    "        run_times.append(trial_times)\n",
    "        pred_history.append(trial_predictions)\n",
    "        true_history.append(trial_ground_truth)\n",
    "        all_confusion.append(trial_conf)\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/7ali/mg101/venv/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(2.1258, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 2.1077730028336967\n",
      "Epoch :  3 \t loss : tensor(1.7730, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7225060439500368\n",
      "Epoch :  5 \t loss : tensor(1.1128, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.3341704841731523\n",
      "Epoch :  7 \t loss : tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.1409075877162433\n",
      "Epoch :  9 \t loss : tensor(0.8836, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9930228758841966\n",
      "Epoch :  11 \t loss : tensor(0.4843, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7551366198285242\n",
      "Epoch :  13 \t loss : tensor(0.4981, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7799552644231893\n",
      "Epoch :  15 \t loss : tensor(0.3106, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6828145593162351\n",
      "Epoch :  17 \t loss : tensor(0.1133, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5768668667321178\n",
      "Epoch :  19 \t loss : tensor(0.1612, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6278115740920468\n",
      "Epoch :  21 \t loss : tensor(0.1068, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5723536865456721\n",
      "Epoch :  23 \t loss : tensor(0.1469, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6060975905720136\n",
      "Epoch :  25 \t loss : tensor(0.1436, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7280719872470928\n",
      "Epoch :  27 \t loss : tensor(0.0432, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5988608662065086\n",
      "Epoch :  29 \t loss : tensor(0.0358, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5475280003867093\n",
      "Epoch :  31 \t loss : tensor(0.0199, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6246934237286864\n",
      "Epoch :  33 \t loss : tensor(0.0231, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.482126361127275\n",
      "Epoch :  35 \t loss : tensor(0.0844, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5808473280227779\n",
      "Epoch :  37 \t loss : tensor(0.0155, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5025294331350568\n",
      "Epoch :  39 \t loss : tensor(0.0129, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4924445823340429\n",
      "Test Accuracy of the model: 83.43\n",
      "Epoch :  1 \t loss : tensor(2.0075, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 2.018708068608427\n",
      "Epoch :  3 \t loss : tensor(1.5480, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.450704190434748\n",
      "Epoch :  5 \t loss : tensor(1.0713, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0400132315222657\n",
      "Epoch :  7 \t loss : tensor(0.6557, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7817218731026653\n",
      "Epoch :  9 \t loss : tensor(0.4719, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6333503407935314\n",
      "Epoch :  11 \t loss : tensor(0.3294, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5017808122677754\n",
      "Epoch :  13 \t loss : tensor(0.2191, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5448057048207006\n",
      "Epoch :  15 \t loss : tensor(0.1269, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4755820661378927\n",
      "Epoch :  17 \t loss : tensor(0.0313, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33240410942160964\n",
      "Epoch :  19 \t loss : tensor(0.1138, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35338085249780193\n",
      "Epoch :  21 \t loss : tensor(0.0758, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3525300477492887\n",
      "Epoch :  23 \t loss : tensor(0.0255, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3990627788433741\n",
      "Epoch :  25 \t loss : tensor(0.0256, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3253231043177349\n",
      "Epoch :  27 \t loss : tensor(0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39310432847832194\n",
      "Epoch :  29 \t loss : tensor(0.0115, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3315899317933979\n",
      "Epoch :  31 \t loss : tensor(0.0091, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37337432351905875\n",
      "Epoch :  33 \t loss : tensor(0.0051, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44593354467160246\n",
      "Epoch :  35 \t loss : tensor(0.0039, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42442222803899465\n",
      "Epoch :  37 \t loss : tensor(0.0028, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44260306999377047\n",
      "Epoch :  39 \t loss : tensor(0.0024, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43039109974697937\n",
      "Test Accuracy of the model: 90.61\n",
      "Epoch :  1 \t loss : tensor(1.9648, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.8678737099073668\n",
      "Epoch :  3 \t loss : tensor(1.2723, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.2492558521180703\n",
      "Epoch :  5 \t loss : tensor(0.7535, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8635681770228906\n",
      "Epoch :  7 \t loss : tensor(0.6920, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.536965283528432\n",
      "Epoch :  9 \t loss : tensor(0.2624, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40353949686270657\n",
      "Epoch :  11 \t loss : tensor(0.2077, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34151923327756145\n",
      "Epoch :  13 \t loss : tensor(0.1476, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4079426125866621\n",
      "Epoch :  15 \t loss : tensor(0.0440, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37419682338050436\n",
      "Epoch :  17 \t loss : tensor(0.0446, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.377654031980667\n",
      "Epoch :  19 \t loss : tensor(0.1322, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4322623827439825\n",
      "Epoch :  21 \t loss : tensor(0.0584, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3173172737151601\n",
      "Epoch :  23 \t loss : tensor(0.0235, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32889290021972767\n",
      "Epoch :  25 \t loss : tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2936459827541044\n",
      "Epoch :  27 \t loss : tensor(0.0086, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3153478529449622\n",
      "Epoch :  29 \t loss : tensor(0.0066, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31916493760897185\n",
      "Epoch :  31 \t loss : tensor(0.0044, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3281196170568701\n",
      "Epoch :  33 \t loss : tensor(0.0032, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34466747312126245\n",
      "Epoch :  35 \t loss : tensor(0.0024, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3475996554190113\n",
      "Epoch :  37 \t loss : tensor(0.0033, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3403198972657557\n",
      "Epoch :  39 \t loss : tensor(0.0027, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35107149346881306\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.9062, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.861345488398663\n",
      "Epoch :  3 \t loss : tensor(1.0412, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.2241627824694488\n",
      "Epoch :  5 \t loss : tensor(0.6776, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9060120981489704\n",
      "Epoch :  7 \t loss : tensor(0.3422, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6321731833666218\n",
      "Epoch :  9 \t loss : tensor(0.2361, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6042654600671006\n",
      "Epoch :  11 \t loss : tensor(0.1181, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5981569632606062\n",
      "Epoch :  13 \t loss : tensor(0.0378, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41378725829015756\n",
      "Epoch :  15 \t loss : tensor(0.0435, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4916791250760884\n",
      "Epoch :  17 \t loss : tensor(0.0392, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5390277496784924\n",
      "Epoch :  19 \t loss : tensor(0.1151, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5253900534944967\n",
      "Epoch :  21 \t loss : tensor(0.1227, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5392641879973706\n",
      "Epoch :  23 \t loss : tensor(0.0420, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5847982600069583\n",
      "Epoch :  25 \t loss : tensor(0.0209, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.577233922669001\n",
      "Epoch :  27 \t loss : tensor(0.0080, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5496729873929613\n",
      "Epoch :  29 \t loss : tensor(0.0231, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.585490731819683\n",
      "Epoch :  31 \t loss : tensor(0.0087, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.572533243310963\n",
      "Epoch :  33 \t loss : tensor(0.0030, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5356643542779153\n",
      "Epoch :  35 \t loss : tensor(0.0033, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5545950688128626\n",
      "Epoch :  37 \t loss : tensor(0.0021, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5415583527052142\n",
      "Epoch :  39 \t loss : tensor(0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5679468187102216\n",
      "Test Accuracy of the model: 90.06\n",
      "Epoch :  1 \t loss : tensor(1.8724, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.8259825149695237\n",
      "Epoch :  3 \t loss : tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0676425704508172\n",
      "Epoch :  5 \t loss : tensor(0.6417, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7042429757773375\n",
      "Epoch :  7 \t loss : tensor(0.3010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5044321063931211\n",
      "Epoch :  9 \t loss : tensor(0.2463, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.616719132085341\n",
      "Epoch :  11 \t loss : tensor(0.1577, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43322111780115424\n",
      "Epoch :  13 \t loss : tensor(0.0314, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41436299060317566\n",
      "Epoch :  15 \t loss : tensor(0.0497, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4699822745101347\n",
      "Epoch :  17 \t loss : tensor(0.0220, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4174524821607692\n",
      "Epoch :  19 \t loss : tensor(0.0090, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4417366566011325\n",
      "Epoch :  21 \t loss : tensor(0.0059, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4833657954515737\n",
      "Epoch :  23 \t loss : tensor(0.0132, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5138634299832837\n",
      "Epoch :  25 \t loss : tensor(0.0033, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47796026318555135\n",
      "Epoch :  27 \t loss : tensor(0.0029, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46014820612337254\n",
      "Epoch :  29 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4834893887577781\n",
      "Epoch :  31 \t loss : tensor(0.0016, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47903071575029815\n",
      "Epoch :  33 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4770573853086819\n",
      "Epoch :  35 \t loss : tensor(0.0015, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4778340188220472\n",
      "Epoch :  37 \t loss : tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46844596774706565\n",
      "Epoch :  39 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48123644385307757\n",
      "Test Accuracy of the model: 88.40\n",
      "Epoch :  1 \t loss : tensor(1.7303, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6502747877961088\n",
      "Epoch :  3 \t loss : tensor(0.8275, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.917702418943186\n",
      "Epoch :  5 \t loss : tensor(0.6221, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.568592925330377\n",
      "Epoch :  7 \t loss : tensor(0.2125, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3378287613507747\n",
      "Epoch :  9 \t loss : tensor(0.1769, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.45410498752741274\n",
      "Epoch :  11 \t loss : tensor(0.0758, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2797534682382846\n",
      "Epoch :  13 \t loss : tensor(0.0193, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22610633497306906\n",
      "Epoch :  15 \t loss : tensor(0.0167, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23062080579083324\n",
      "Epoch :  17 \t loss : tensor(0.0091, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2041703096130684\n",
      "Epoch :  19 \t loss : tensor(0.0026, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19745800451432557\n",
      "Epoch :  21 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.20217277440446324\n",
      "Epoch :  23 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.18957869638207078\n",
      "Epoch :  25 \t loss : tensor(0.0021, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.1993571330798852\n",
      "Epoch :  27 \t loss : tensor(0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2098285532018934\n",
      "Epoch :  29 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19500188614962347\n",
      "Epoch :  31 \t loss : tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.20600763975064965\n",
      "Epoch :  33 \t loss : tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.20385261065731705\n",
      "Epoch :  35 \t loss : tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19520410580996325\n",
      "Epoch :  37 \t loss : tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.20626266825000691\n",
      "Epoch :  39 \t loss : tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19663123946737773\n",
      "Test Accuracy of the model: 95.03\n",
      "Epoch :  1 \t loss : tensor(1.4891, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6037263749178614\n",
      "Epoch :  3 \t loss : tensor(0.5762, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8040872803034245\n",
      "Epoch :  5 \t loss : tensor(0.3818, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.504952128533889\n",
      "Epoch :  7 \t loss : tensor(0.1508, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35704388341490184\n",
      "Epoch :  9 \t loss : tensor(0.0713, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3478262332740146\n",
      "Epoch :  11 \t loss : tensor(0.1048, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24142042176691594\n",
      "Epoch :  13 \t loss : tensor(0.0477, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24455858113111262\n",
      "Epoch :  15 \t loss : tensor(0.1593, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2973750818938403\n",
      "Epoch :  17 \t loss : tensor(0.0253, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21898478927564735\n",
      "Epoch :  19 \t loss : tensor(0.0658, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2885833675657842\n",
      "Epoch :  21 \t loss : tensor(0.0226, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2333232472284655\n",
      "Epoch :  23 \t loss : tensor(0.0053, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22225176696327445\n",
      "Epoch :  25 \t loss : tensor(0.0030, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2195188947444012\n",
      "Epoch :  27 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23495730947570417\n",
      "Epoch :  29 \t loss : tensor(0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22930365005670683\n",
      "Epoch :  31 \t loss : tensor(0.0020, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23996603819948326\n",
      "Epoch :  33 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24262822831377742\n",
      "Epoch :  35 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.242571741830048\n",
      "Epoch :  37 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2324435004701736\n",
      "Epoch :  39 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23836132243116412\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.8052, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7281436361104292\n",
      "Epoch :  3 \t loss : tensor(0.8189, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9021253080141052\n",
      "Epoch :  5 \t loss : tensor(0.3777, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47444159494842414\n",
      "Epoch :  7 \t loss : tensor(0.1057, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.408350430504496\n",
      "Epoch :  9 \t loss : tensor(0.0794, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.345591412963806\n",
      "Epoch :  11 \t loss : tensor(0.0391, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35660742490517566\n",
      "Epoch :  13 \t loss : tensor(0.0123, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2760410707933816\n",
      "Epoch :  15 \t loss : tensor(0.0223, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33747217449942907\n",
      "Epoch :  17 \t loss : tensor(0.0186, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3806159238598916\n",
      "Epoch :  19 \t loss : tensor(0.0198, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.269333397318376\n",
      "Epoch :  21 \t loss : tensor(0.0070, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3506915517361093\n",
      "Epoch :  23 \t loss : tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3116396502917406\n",
      "Epoch :  25 \t loss : tensor(0.0076, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3041358320198637\n",
      "Epoch :  27 \t loss : tensor(0.0057, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37515970746368876\n",
      "Epoch :  29 \t loss : tensor(0.0294, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3568064945659036\n",
      "Epoch :  31 \t loss : tensor(0.0196, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5712550828555688\n",
      "Epoch :  33 \t loss : tensor(0.2052, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6148807779987391\n",
      "Epoch :  35 \t loss : tensor(0.0544, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35135995097276584\n",
      "Epoch :  37 \t loss : tensor(0.0673, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.45872454021075654\n",
      "Epoch :  39 \t loss : tensor(0.0113, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40303047804938824\n",
      "Test Accuracy of the model: 89.50\n",
      "Epoch :  1 \t loss : tensor(1.7630, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6792057691432927\n",
      "Epoch :  3 \t loss : tensor(0.8265, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8690591503481522\n",
      "Epoch :  5 \t loss : tensor(0.2908, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46163813725768793\n",
      "Epoch :  7 \t loss : tensor(0.1635, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4410336572929839\n",
      "Epoch :  9 \t loss : tensor(0.1334, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3902221226686113\n",
      "Epoch :  11 \t loss : tensor(0.0600, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.30364703127626996\n",
      "Epoch :  13 \t loss : tensor(0.0079, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2668743347958351\n",
      "Epoch :  15 \t loss : tensor(0.0081, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3036675154793491\n",
      "Epoch :  17 \t loss : tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2537189209924963\n",
      "Epoch :  19 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26664991904415875\n",
      "Epoch :  21 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2905858269666851\n",
      "Epoch :  23 \t loss : tensor(0.0012, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3119034833540694\n",
      "Epoch :  25 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3245451186763461\n",
      "Epoch :  27 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3360424795107601\n",
      "Epoch :  29 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33262158550434023\n",
      "Epoch :  31 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33390565827089413\n",
      "Epoch :  33 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34163694054412547\n",
      "Epoch :  35 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3528737060589709\n",
      "Epoch :  37 \t loss : tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35230673840036825\n",
      "Epoch :  39 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.38028741039161673\n",
      "Test Accuracy of the model: 90.61\n",
      "Epoch :  1 \t loss : tensor(1.7872, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6610573508894746\n",
      "Epoch :  3 \t loss : tensor(0.6304, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7687978172779756\n",
      "Epoch :  5 \t loss : tensor(0.2336, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5609170265170031\n",
      "Epoch :  7 \t loss : tensor(0.0643, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3942509200980213\n",
      "Epoch :  9 \t loss : tensor(0.0440, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42211942502549354\n",
      "Epoch :  11 \t loss : tensor(0.0851, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43039449069730606\n",
      "Epoch :  13 \t loss : tensor(0.0289, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42880927568316934\n",
      "Epoch :  15 \t loss : tensor(0.0144, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4433243007289656\n",
      "Epoch :  17 \t loss : tensor(0.0061, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5370967214935526\n",
      "Epoch :  19 \t loss : tensor(0.0020, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.560733174872089\n",
      "Epoch :  21 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5530411594872094\n",
      "Epoch :  23 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5370933746026514\n",
      "Epoch :  25 \t loss : tensor(0.0012, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5724860032043827\n",
      "Epoch :  27 \t loss : tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5709867542937624\n",
      "Epoch :  29 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5668282520707385\n",
      "Epoch :  31 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5971322816456609\n",
      "Epoch :  33 \t loss : tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5481329482444037\n",
      "Epoch :  35 \t loss : tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5727331728074926\n",
      "Epoch :  37 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.592847786647169\n",
      "Epoch :  39 \t loss : tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.564340007316482\n",
      "Test Accuracy of the model: 89.50\n",
      "Epoch :  1 \t loss : tensor(1.7888, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6802139070316484\n",
      "Epoch :  3 \t loss : tensor(0.7230, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8362864362954704\n",
      "Epoch :  5 \t loss : tensor(0.2835, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4943494105409026\n",
      "Epoch :  7 \t loss : tensor(0.1593, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.371959379911356\n",
      "Epoch :  9 \t loss : tensor(0.0400, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26100593283342705\n",
      "Epoch :  11 \t loss : tensor(0.0661, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2430487848446746\n",
      "Epoch :  13 \t loss : tensor(0.0121, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.1635392385851752\n",
      "Epoch :  15 \t loss : tensor(0.0086, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.16761394516194514\n",
      "Epoch :  17 \t loss : tensor(0.0189, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2165106309727912\n",
      "Epoch :  19 \t loss : tensor(0.0024, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22444591039856032\n",
      "Epoch :  21 \t loss : tensor(0.0052, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23247194051461376\n",
      "Epoch :  23 \t loss : tensor(0.0060, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23019006755783242\n",
      "Epoch :  25 \t loss : tensor(0.0015, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21990206674966517\n",
      "Epoch :  27 \t loss : tensor(0.0278, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2637971895548961\n",
      "Epoch :  29 \t loss : tensor(0.0023, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23632382897278437\n",
      "Epoch :  31 \t loss : tensor(0.0016, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2676611222571449\n",
      "Epoch :  33 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2539615798323873\n",
      "Epoch :  35 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24920094220482433\n",
      "Epoch :  37 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2452610180958578\n",
      "Epoch :  39 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23767918935681218\n",
      "Test Accuracy of the model: 92.82\n",
      "Epoch :  1 \t loss : tensor(1.5039, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.5874416205555615\n",
      "Epoch :  3 \t loss : tensor(0.6187, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7867216528063637\n",
      "Epoch :  5 \t loss : tensor(0.4035, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4992107597928464\n",
      "Epoch :  7 \t loss : tensor(0.0866, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3246540125206554\n",
      "Epoch :  9 \t loss : tensor(0.1311, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3946688597608487\n",
      "Epoch :  11 \t loss : tensor(0.0981, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.28160685818695325\n",
      "Epoch :  13 \t loss : tensor(0.0339, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3073358224663316\n",
      "Epoch :  15 \t loss : tensor(0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2696935648059274\n",
      "Epoch :  17 \t loss : tensor(0.1162, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41617318742100645\n",
      "Epoch :  19 \t loss : tensor(0.0442, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33143963023493017\n",
      "Epoch :  21 \t loss : tensor(0.0463, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.27151092545684413\n",
      "Epoch :  23 \t loss : tensor(0.0027, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.28420235238987773\n",
      "Epoch :  25 \t loss : tensor(0.0141, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35065727938215563\n",
      "Epoch :  27 \t loss : tensor(0.0042, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22969901170108542\n",
      "Epoch :  29 \t loss : tensor(0.0019, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22614858422072667\n",
      "Epoch :  31 \t loss : tensor(0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2203471296018523\n",
      "Epoch :  33 \t loss : tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2183805943912657\n",
      "Epoch :  35 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21646052314722156\n",
      "Epoch :  37 \t loss : tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21566918452835618\n",
      "Epoch :  39 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22788535476852942\n",
      "Test Accuracy of the model: 92.82\n",
      "Epoch :  1 \t loss : tensor(1.6753, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6617866289055316\n",
      "Epoch :  3 \t loss : tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7845038855083425\n",
      "Epoch :  5 \t loss : tensor(0.3380, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42189165440737053\n",
      "Epoch :  7 \t loss : tensor(0.0799, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3846754682923093\n",
      "Epoch :  9 \t loss : tensor(0.0454, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3318754060389471\n",
      "Epoch :  11 \t loss : tensor(0.0544, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32341804065548213\n",
      "Epoch :  13 \t loss : tensor(0.0202, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3832095987146915\n",
      "Epoch :  15 \t loss : tensor(0.0168, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.28707091796064854\n",
      "Epoch :  17 \t loss : tensor(0.0527, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.28507829621708963\n",
      "Epoch :  19 \t loss : tensor(0.0035, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3719394802312339\n",
      "Epoch :  21 \t loss : tensor(0.0115, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31317280173702006\n",
      "Epoch :  23 \t loss : tensor(0.0058, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46012988292534507\n",
      "Epoch :  25 \t loss : tensor(0.0098, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3385428938948671\n",
      "Epoch :  27 \t loss : tensor(0.0021, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32384831307911216\n",
      "Epoch :  29 \t loss : tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31408508019568765\n",
      "Epoch :  31 \t loss : tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.323003640139874\n",
      "Epoch :  33 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34272590759840327\n",
      "Epoch :  35 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3341354002538519\n",
      "Epoch :  37 \t loss : tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33949681876496896\n",
      "Epoch :  39 \t loss : tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33747973116547225\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.5749, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6375512625233337\n",
      "Epoch :  3 \t loss : tensor(0.8177, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9921107063781509\n",
      "Epoch :  5 \t loss : tensor(0.2492, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5750101517176806\n",
      "Epoch :  7 \t loss : tensor(0.0762, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.514101069021802\n",
      "Epoch :  9 \t loss : tensor(0.0516, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5194416884274253\n",
      "Epoch :  11 \t loss : tensor(0.0381, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4710124301900646\n",
      "Epoch :  13 \t loss : tensor(0.0092, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.470599620479875\n",
      "Epoch :  15 \t loss : tensor(0.0039, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5073484212661268\n",
      "Epoch :  17 \t loss : tensor(0.0017, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5164305214628707\n",
      "Epoch :  19 \t loss : tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5253137045185574\n",
      "Epoch :  21 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5439810550911335\n",
      "Epoch :  23 \t loss : tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5551308637662349\n",
      "Epoch :  25 \t loss : tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.58188562468342\n",
      "Epoch :  27 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5417512982525627\n",
      "Epoch :  29 \t loss : tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5640793706331539\n",
      "Epoch :  31 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5448939959594873\n",
      "Epoch :  33 \t loss : tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5652096918402951\n",
      "Epoch :  35 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5670335342026466\n",
      "Epoch :  37 \t loss : tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.555184053907057\n",
      "Epoch :  39 \t loss : tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5613752899516901\n",
      "Test Accuracy of the model: 90.06\n",
      "Epoch :  1 \t loss : tensor(1.6367, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.565579452173507\n",
      "Epoch :  3 \t loss : tensor(0.5952, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7512084058695375\n",
      "Epoch :  5 \t loss : tensor(0.2947, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5354358010543904\n",
      "Epoch :  7 \t loss : tensor(0.2623, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4003374785543981\n",
      "Epoch :  9 \t loss : tensor(0.0485, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3231430833792664\n",
      "Epoch :  11 \t loss : tensor(0.0181, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.25891291936471056\n",
      "Epoch :  13 \t loss : tensor(0.0067, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.261733778834953\n",
      "Epoch :  15 \t loss : tensor(0.0038, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2521239663335867\n",
      "Epoch :  17 \t loss : tensor(0.0030, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2317779027955801\n",
      "Epoch :  19 \t loss : tensor(0.0037, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2055721855190473\n",
      "Epoch :  21 \t loss : tensor(0.0090, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.28580895277311125\n",
      "Epoch :  23 \t loss : tensor(0.0026, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21550355561917198\n",
      "Epoch :  25 \t loss : tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23596069611230971\n",
      "Epoch :  27 \t loss : tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2833211913465166\n",
      "Epoch :  29 \t loss : tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29361872312288667\n",
      "Epoch :  31 \t loss : tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2767345039142594\n",
      "Epoch :  33 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2637630839342657\n",
      "Epoch :  35 \t loss : tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2951665715396365\n",
      "Epoch :  37 \t loss : tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29218220197549766\n",
      "Epoch :  39 \t loss : tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2800253937146992\n",
      "Test Accuracy of the model: 94.48\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 3\n",
    "run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, conf_mat = cnnlstm_grit(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83425414 0.90607735 0.91712707]\n",
      " [0.90055249 0.8839779  0.95027624]\n",
      " [0.93922652 0.89502762 0.90607735]\n",
      " [0.89502762 0.9281768  0.9281768 ]\n",
      " [0.91712707 0.90055249 0.94475138]]\n",
      "[0.88581952 0.91160221 0.91344383 0.91712707 0.92081031]\n",
      "0.9097605893186003\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "test_scores = np.asarray(test_scores)\n",
    "print(test_scores)\n",
    "#mean test results for each trial\n",
    "mean_test_scores_per_trial = np.mean(test_scores, axis=1)\n",
    "print(mean_test_scores_per_trial)\n",
    "print(np.mean(mean_test_scores_per_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140.74073744 139.6862591  141.05932564]\n",
      " [140.96871604 140.58847191 140.66067045]\n",
      " [140.21523853 140.89995366 140.74312951]\n",
      " [139.6459288  140.97354587 140.03371678]\n",
      " [140.51395147 142.22475406 140.22216041]]\n",
      "[140.49544073 140.73928614 140.61944056 140.21773048 140.98695531]\n",
      "140.61177064353333\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "run_times = np.asarray(run_times)\n",
    "print(run_times)\n",
    "#mean test results for each trial\n",
    "mean_run_times_per_trial = np.mean(run_times, axis=1)\n",
    "print(mean_run_times_per_trial)\n",
    "print(np.mean(mean_run_times_per_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1 = []\n",
    "for one_trial_confs in conf_mat:\n",
    "    one_trial_f1 = []\n",
    "    for one_trial_conf in one_trial_confs:\n",
    "        recall = np.diag(one_trial_conf.numpy()) / np.sum(one_trial_conf.numpy(), axis = 1)\n",
    "        precision = np.diag(one_trial_conf.numpy()) / np.sum(one_trial_conf.numpy(), axis = 0)\n",
    "        recall = np.mean(recall)\n",
    "        precision = np.mean(precision)\n",
    "        one_trial_f1.append(2 * (precision * recall) / (precision + recall))\n",
    "    all_f1.append(one_trial_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84043363 0.91177935 0.91964646]\n",
      " [0.90621076 0.88395816 0.95078671]\n",
      " [0.94090641 0.89884098 0.90998352]\n",
      " [0.89854008 0.92942875 0.92967056]\n",
      " [0.91854648 0.90488554 0.94806643]]\n",
      "[0.89061981 0.91365188 0.91657697 0.91921313 0.92383282]\n",
      "0.9127789211298383\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "all_f1 = np.asarray(all_f1)\n",
    "print(all_f1)\n",
    "#mean test results for each trial\n",
    "mean_f1_per_trial = np.mean(all_f1, axis=1)\n",
    "print(mean_f1_per_trial)\n",
    "print(np.mean(mean_f1_per_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 14.,  3.,  1.,  1.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  2., 15.,  2.,  0.,  0.,  0.,  2.],\n",
      "        [ 2.,  0.,  2.,  2., 14.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  2.,  1.,  0., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0., 19.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  0.,  0.,  0., 18.,  0.],\n",
      "        [ 0.,  0.,  2.,  2.,  0.,  0.,  0.,  0., 16.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 3.,  0., 13.,  2.,  1.,  0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0., 18.,  0.,  0.,  0.,  1.,  1.],\n",
      "        [ 1.,  0.,  0.,  4., 15.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., 19.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 17.,  0.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  4., 16.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  2., 17.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  0.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 19.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 17.,  3.,  0.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  3.,  3.,  0., 15.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 19.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 18.,  2.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 12.,  3.,  1.,  3.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  2., 14.,  0.,  1.,  0.,  0.,  3.],\n",
      "        [ 2.,  0.,  0.,  1., 16.,  0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., 20.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0., 19.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., 19.]])\n",
      "tensor([[18.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 15.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 20.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 19.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 16.,  1.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 17.,  2.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0., 19.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  2.,  0.,  0.,  0.,  0.,  0., 18.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 16.,  2.,  0.,  1.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1., 16.,  2.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  2.,  1., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0., 17.,  2.],\n",
      "        [ 0.,  1.,  0.,  2.,  0.,  0.,  0.,  0., 17.]])\n",
      "tensor([[18.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 17.,  2.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 15.,  0.,  3.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  3., 18.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  2.,  0., 18.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0., 19.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 15.,  2.,  1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 16.,  0.,  1.,  0.,  0.,  3.],\n",
      "        [ 5.,  0.,  1.,  1., 13.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  2.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 17.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 17.,  1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 19.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0., 19.,  0.],\n",
      "        [ 0.,  0.,  0.,  2.,  0.,  0.,  0.,  0., 18.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 19.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 17.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., 18.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  1.,  0., 18.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 14.,  3.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  3., 16.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  1., 18.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 14.,  3.,  1.,  1.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., 20.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  1.,  1., 16.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  1.,  0.,  0., 15.,  3.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 18.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  4., 15.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 20.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 19.]])\n"
     ]
    }
   ],
   "source": [
    "for one_trial_confs in conf_mat:\n",
    "    for one_trial_conf in one_trial_confs:\n",
    "        print(one_trial_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.333334    0.          0.33333334  0.          0.33333334  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 1.          0.         14.666667    1.6666666   1.3333334   0.33333334\n",
      "   0.          0.33333334  0.33333334]\n",
      " [ 0.          0.          2.         16.333334    0.6666667   0.\n",
      "   0.          0.33333334  1.3333334 ]\n",
      " [ 1.3333334   0.          0.6666667   2.6666667  15.333333    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  1.3333334   0.6666667   0.         18.333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.33333334\n",
      "  20.          0.          0.        ]\n",
      " [ 0.          0.          0.33333334  0.33333334  0.          0.\n",
      "   0.         19.          0.33333334]\n",
      " [ 0.          0.          0.6666667   1.          0.          0.\n",
      "   0.          0.         18.333334  ]]\n",
      "\n",
      "\n",
      "[[18.333334    0.          0.33333334  0.          0.33333334  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334 19.666666    0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334 15.333333    2.          0.6666667   1.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.          0.6666667  17.          1.3333334   0.33333334\n",
      "   0.          0.          1.3333334 ]\n",
      " [ 1.          0.          0.          0.33333334 18.          0.33333334\n",
      "   0.          0.33333334  0.        ]\n",
      " [ 0.          0.          1.          1.3333334   0.         18.666666\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.33333334  0.          0.          0.\n",
      "  20.          0.          0.33333334]\n",
      " [ 0.          0.          0.33333334  0.          0.          0.\n",
      "   0.         19.          0.6666667 ]\n",
      " [ 0.          0.          0.          0.6666667   0.          0.33333334\n",
      "   0.          0.         19.        ]]\n",
      "\n",
      "\n",
      "[[18.333334    0.          0.33333334  0.          0.33333334  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.          0.6666667   0.          0.          0.\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.33333334  0.         15.666667    1.          1.6666666   0.33333334\n",
      "   0.          0.          0.6666667 ]\n",
      " [ 0.          0.          1.6666666  17.          1.3333334   0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.          0.          0.33333334 19.666666    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.6666667   1.3333334   0.33333334 18.666666\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.\n",
      "  20.          0.          0.33333334]\n",
      " [ 0.          0.          0.          0.          0.6666667   0.\n",
      "   0.         18.666666    0.6666667 ]\n",
      " [ 0.          0.33333334  0.6666667   0.6666667   0.          0.\n",
      "   0.          0.         18.333334  ]]\n",
      "\n",
      "\n",
      "[[18.333334    0.          0.          0.33333334  0.33333334  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.666666    0.          0.          0.          0.\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.33333334  0.33333334 16.333334    1.6666666   0.6666667   0.33333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          1.3333334  17.          0.6666667   0.6666667\n",
      "   0.          0.          1.        ]\n",
      " [ 2.3333333   0.          0.33333334  0.33333334 17.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.6666667   1.          0.         19.333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "  20.          0.          0.33333334]\n",
      " [ 0.          0.          0.33333334  0.          0.          0.\n",
      "   0.         19.333334    0.33333334]\n",
      " [ 0.33333334  0.          0.          0.6666667   0.          0.\n",
      "   0.          0.         19.        ]]\n",
      "\n",
      "\n",
      "[[19.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.6666667  15.333333    2.          0.6666667   0.33333334\n",
      "   0.          0.          0.6666667 ]\n",
      " [ 0.          0.          2.3333333  17.          1.          0.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.6666667   0.          0.33333334  0.6666667  18.333334    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  0.33333334  0.6666667   0.33333334 19.\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "  20.333334    0.          0.        ]\n",
      " [ 0.          0.          0.          0.33333334  0.33333334  0.\n",
      "   0.         18.333334    1.        ]\n",
      " [ 0.33333334  0.          0.33333334  0.          0.          0.\n",
      "   0.          0.         19.333334  ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_conf_avg = []\n",
    "for one_trial_confs in conf_mat:\n",
    "    temp = []\n",
    "    for one_trial_conf in one_trial_confs:\n",
    "        temp.append(np.array(one_trial_conf))\n",
    "    all_conf_avg.append(np.mean(np.array(temp), axis=0))\n",
    "    print(np.mean(np.array(temp), axis=0))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.466667    0.          0.2         0.06666667  0.26666668  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.06666667 19.666666    0.13333334  0.          0.          0.\n",
      "   0.13333334  0.          0.        ]\n",
      " [ 0.33333334  0.26666668 15.466667    1.6666666   0.9999999   0.46666664\n",
      "   0.          0.06666667  0.4       ]\n",
      " [ 0.          0.          1.6        16.866667    1.          0.26666668\n",
      "   0.          0.06666667  0.8666667 ]\n",
      " [ 1.0666667   0.          0.26666668  0.8666666  17.666668    0.06666667\n",
      "   0.          0.06666667  0.        ]\n",
      " [ 0.          0.13333334  0.8000001   1.          0.13333334 18.8\n",
      "   0.06666667  0.          0.06666667]\n",
      " [ 0.          0.13333334  0.06666667  0.          0.          0.2\n",
      "  20.066668    0.          0.2       ]\n",
      " [ 0.          0.          0.2         0.13333334  0.2         0.\n",
      "   0.         18.866667    0.6       ]\n",
      " [ 0.13333334  0.06666667  0.33333334  0.6         0.          0.06666667\n",
      "   0.          0.         18.800001  ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array(all_conf_avg), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def displayConfMat(confusion_matrix, save_file_name):\n",
    "    #confusion_matrix = confusion_matrix.numpy()\n",
    "    #print(confusion_matrix)\n",
    "    font = {'size'   : 5}\n",
    "    plt.rc('font', **font)\n",
    "    figure(num=None, figsize=(1080, 1080), dpi=300, facecolor='w', edgecolor='k')\n",
    "    plt_conf = ConfusionMatrixDisplay(confusion_matrix=np.array(confusion_matrix),\n",
    "                                  display_labels=np.array(classes))\n",
    "    plt_conf.plot(xticks_rotation='vertical', cmap='Blues',values_format='.5g')\n",
    "    plt.gcf().subplots_adjust(bottom=0.19)\n",
    "    plt_conf.figure_.savefig(save_file_name, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Results for CNNLSTM - best case\n",
      "Accuracy: 0.92081\n",
      "F1: 0.92383\n",
      "===============================\n",
      "===============================\n",
      "Results for CNNLSTM - worst case\n",
      "Accuracy: 0.88582\n",
      "F1: 0.89062\n",
      "===============================\n",
      "===============================\n",
      "Results for CNNLSTM - avg case\n",
      "Accuracy: 0.90976\n",
      "F1: 0.91278\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 9\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - best case\")\n",
    "best = np.argmax(mean_test_scores_per_trial)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial[best], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial[best], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg[best].round(3), \"3_cross_just_cnnlstm_grit_best_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - worst case\")\n",
    "worst = np.argmin(mean_test_scores_per_trial)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial[worst], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial[worst], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg[worst].round(3), \"3_cross_just_cnnlstm_grit_worst_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - avg case\")\n",
    "worst = np.mean(mean_test_scores_per_trial)\n",
    "print(\"Accuracy:\", round(np.mean(mean_test_scores_per_trial), 5))\n",
    "print(\"F1:\", round(np.mean(mean_f1_per_trial), 5))\n",
    "print(\"===============================\")\n",
    "#temp = []\n",
    "#for conf in all_confusion:\n",
    "#    temp.append(np.array(conf))\n",
    "#print(np.mean(np.array(temp), axis=0))\n",
    "whatever = np.mean(np.array(all_conf_avg), axis=0)\n",
    "displayConfMat(whatever.round(3), \"3_cross_just_cnnlstm_grit_avg_30_70_5_9_2021_bigger.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (snap): Snap(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=1650, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (rnn): LSTM(500, 64, num_layers=2, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=9, bias=True)\n",
      "  (act3): Tanh()\n",
      "  (linear2): Linear(in_features=1714, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = CNNLSTM(snapture=True)\n",
    "model.to('cuda:0')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_grit_snapture(num_trials, cv_split=None):\n",
    "    num_epochs = 40\n",
    "    \n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    all_confusion = []\n",
    "    all_lost = []\n",
    "    all_val_lost=[]\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        trial_scores = []\n",
    "        trial_times = []\n",
    "        trial_predictions = []\n",
    "        trial_ground_truth = []\n",
    "        trial_conf = []\n",
    "        trial_lost = []\n",
    "        trial_val_lost=[]\n",
    "        all_y = np.array([y for x, y, z in iter(gritdataset)])\n",
    "        cv_folds = StratifiedKFold(n_splits=cv_split, shuffle=True, random_state=i)\n",
    "        for cv_fold, (train_indices, test_indices) in enumerate(cv_folds.split(gritdataset, all_y)):\n",
    "            train_dataset = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "            test_dataset = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "            train_loader = DataLoader(gritdataset, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=train_dataset)\n",
    "            test_loader = DataLoader(gritdataset, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=test_dataset)\n",
    "\n",
    "            # defining the model\n",
    "            model = CNNLSTM(snapture=True)\n",
    "            model.to('cuda:0')\n",
    "            optimizer = Adam(model.parameters(), lr=0.001)\n",
    "            # defining the loss function\n",
    "            criterion = CrossEntropyLoss()\n",
    "            #print(model)\n",
    "            \n",
    "            start = time.process_time() \n",
    "            loss_list, val_losses = train_model(model, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "            trial_times.append(time.process_time() - start)\n",
    "            acc, preds, labels, confusion_matrix = test_model(model, test_loader, device)\n",
    "            trial_conf.append(confusion_matrix)\n",
    "            trial_predictions.append(preds)\n",
    "            trial_ground_truth.append(labels)\n",
    "            trial_scores.append(acc) #whole_sequence\n",
    "            trial_lost.append(loss_list)\n",
    "            trial_val_lost.append(val_losses)\n",
    "        all_lost.append(loss_list)\n",
    "        all_val_lost.append(val_losses)\n",
    "        test_scores.append(trial_scores)\n",
    "        run_times.append(trial_times)\n",
    "        pred_history.append(trial_predictions)\n",
    "        true_history.append(trial_ground_truth)\n",
    "        all_confusion.append(trial_conf)\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(1.8141, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.720311953410584\n",
      "Epoch :  3 \t loss : tensor(0.9676, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9816824441154949\n",
      "Epoch :  5 \t loss : tensor(0.5491, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6638087659515298\n",
      "Epoch :  7 \t loss : tensor(0.4262, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5355776160384346\n",
      "Epoch :  9 \t loss : tensor(0.3513, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5719585211453088\n",
      "Epoch :  11 \t loss : tensor(0.3105, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49954642583208614\n",
      "Epoch :  13 \t loss : tensor(0.1776, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46326591941927897\n",
      "Epoch :  15 \t loss : tensor(0.2183, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.334178080049524\n",
      "Epoch :  17 \t loss : tensor(0.1457, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47436217519356144\n",
      "Epoch :  19 \t loss : tensor(0.0969, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3371744056477966\n",
      "Epoch :  21 \t loss : tensor(0.1819, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5673722954912948\n",
      "Epoch :  23 \t loss : tensor(0.1618, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4112690966203599\n",
      "Epoch :  25 \t loss : tensor(0.0461, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3892567489219683\n",
      "Epoch :  27 \t loss : tensor(0.0613, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3134475986899347\n",
      "Epoch :  29 \t loss : tensor(0.0620, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37719860000728467\n",
      "Epoch :  31 \t loss : tensor(0.0380, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4128597646927809\n",
      "Epoch :  33 \t loss : tensor(0.0153, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4077697480927476\n",
      "Epoch :  35 \t loss : tensor(0.0293, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39295189860349905\n",
      "Epoch :  37 \t loss : tensor(0.0299, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4795363718850778\n",
      "Epoch :  39 \t loss : tensor(0.0342, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36766595980179617\n",
      "Test Accuracy of the model: 88.40\n",
      "Epoch :  1 \t loss : tensor(1.7067, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.743891119361683\n",
      "Epoch :  3 \t loss : tensor(0.9005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0368790084575854\n",
      "Epoch :  5 \t loss : tensor(0.6211, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6871070257045097\n",
      "Epoch :  7 \t loss : tensor(0.5102, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6400183577283705\n",
      "Epoch :  9 \t loss : tensor(0.4640, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40683080671606237\n",
      "Epoch :  11 \t loss : tensor(0.3493, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4947161382284582\n",
      "Epoch :  13 \t loss : tensor(0.2009, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.30443487602628844\n",
      "Epoch :  15 \t loss : tensor(0.2370, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6559567441796909\n",
      "Epoch :  17 \t loss : tensor(0.2276, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37761229850275685\n",
      "Epoch :  19 \t loss : tensor(0.2000, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2918928374286576\n",
      "Epoch :  21 \t loss : tensor(0.1374, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34854968795723124\n",
      "Epoch :  23 \t loss : tensor(0.0868, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2921822796692835\n",
      "Epoch :  25 \t loss : tensor(0.0523, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2772389390320142\n",
      "Epoch :  27 \t loss : tensor(0.0761, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3400409673583093\n",
      "Epoch :  29 \t loss : tensor(0.0902, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.281112404176555\n",
      "Epoch :  31 \t loss : tensor(0.0777, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47038620254455715\n",
      "Epoch :  33 \t loss : tensor(0.0797, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4843871820619867\n",
      "Epoch :  35 \t loss : tensor(0.0389, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.30964620825785694\n",
      "Epoch :  37 \t loss : tensor(0.0215, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2598186110271594\n",
      "Epoch :  39 \t loss : tensor(0.0597, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3170825836134714\n",
      "Test Accuracy of the model: 92.27\n",
      "Epoch :  1 \t loss : tensor(1.8479, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6644384701516097\n",
      "Epoch :  3 \t loss : tensor(0.9497, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9799449048897899\n",
      "Epoch :  5 \t loss : tensor(0.7351, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.714210122614234\n",
      "Epoch :  7 \t loss : tensor(0.3993, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4886974888080225\n",
      "Epoch :  9 \t loss : tensor(0.3831, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46253939674035577\n",
      "Epoch :  11 \t loss : tensor(0.2627, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41090405370696265\n",
      "Epoch :  13 \t loss : tensor(0.2565, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35470168620264736\n",
      "Epoch :  15 \t loss : tensor(0.1443, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37211242223615265\n",
      "Epoch :  17 \t loss : tensor(0.1139, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.508520762470444\n",
      "Epoch :  19 \t loss : tensor(0.2155, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35726565904938196\n",
      "Epoch :  21 \t loss : tensor(0.2790, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5251720635069793\n",
      "Epoch :  23 \t loss : tensor(0.1438, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3698681102795232\n",
      "Epoch :  25 \t loss : tensor(0.0713, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35278801366350737\n",
      "Epoch :  27 \t loss : tensor(0.0997, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.338994037853058\n",
      "Epoch :  29 \t loss : tensor(0.0414, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2820888388386527\n",
      "Epoch :  31 \t loss : tensor(0.0294, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2795127301260569\n",
      "Epoch :  33 \t loss : tensor(0.0344, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36135860143134324\n",
      "Epoch :  35 \t loss : tensor(0.0204, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3055038080028143\n",
      "Epoch :  37 \t loss : tensor(0.0173, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23911955067268523\n",
      "Epoch :  39 \t loss : tensor(0.0312, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.27110441014103176\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.8180, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7444305947332515\n",
      "Epoch :  3 \t loss : tensor(0.9603, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9542632114101517\n",
      "Epoch :  5 \t loss : tensor(0.5878, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6605677543330546\n",
      "Epoch :  7 \t loss : tensor(0.4633, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5905729416678996\n",
      "Epoch :  9 \t loss : tensor(0.3110, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.559736499203541\n",
      "Epoch :  11 \t loss : tensor(0.4693, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.573741342352346\n",
      "Epoch :  13 \t loss : tensor(0.2401, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4368511260184453\n",
      "Epoch :  15 \t loss : tensor(0.2022, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5257408090088593\n",
      "Epoch :  17 \t loss : tensor(0.1728, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5553638038880039\n",
      "Epoch :  19 \t loss : tensor(0.1319, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4420599244763754\n",
      "Epoch :  21 \t loss : tensor(0.0989, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48947162162826835\n",
      "Epoch :  23 \t loss : tensor(0.1126, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48032231621135885\n",
      "Epoch :  25 \t loss : tensor(0.1208, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5628013163523617\n",
      "Epoch :  27 \t loss : tensor(0.2241, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6509320903729597\n",
      "Epoch :  29 \t loss : tensor(0.1127, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6145663263676243\n",
      "Epoch :  31 \t loss : tensor(0.1556, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.518391032846726\n",
      "Epoch :  33 \t loss : tensor(0.0529, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5565217104982502\n",
      "Epoch :  35 \t loss : tensor(0.0670, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43036147362110677\n",
      "Epoch :  37 \t loss : tensor(0.0263, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44501912790106385\n",
      "Epoch :  39 \t loss : tensor(0.0187, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5347608894085202\n",
      "Test Accuracy of the model: 89.50\n",
      "Epoch :  1 \t loss : tensor(1.7782, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7400905600168812\n",
      "Epoch :  3 \t loss : tensor(1.0288, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0087889650397412\n",
      "Epoch :  5 \t loss : tensor(0.6831, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7927868251676471\n",
      "Epoch :  7 \t loss : tensor(0.5989, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7192675484373822\n",
      "Epoch :  9 \t loss : tensor(0.4085, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5645328060787277\n",
      "Epoch :  11 \t loss : tensor(0.2762, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6674066748755786\n",
      "Epoch :  13 \t loss : tensor(0.2489, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.711251358110351\n",
      "Epoch :  15 \t loss : tensor(0.2388, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7718738545987968\n",
      "Epoch :  17 \t loss : tensor(0.2030, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6372899589463994\n",
      "Epoch :  19 \t loss : tensor(0.0820, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47110603670659196\n",
      "Epoch :  21 \t loss : tensor(0.1075, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.45982924670633457\n",
      "Epoch :  23 \t loss : tensor(0.0994, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47668321269353414\n",
      "Epoch :  25 \t loss : tensor(0.0965, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5956947508779654\n",
      "Epoch :  27 \t loss : tensor(0.2017, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.529167039380962\n",
      "Epoch :  29 \t loss : tensor(0.0759, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5460198283358562\n",
      "Epoch :  31 \t loss : tensor(0.1149, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5210426254733456\n",
      "Epoch :  33 \t loss : tensor(0.1489, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44650422211944135\n",
      "Epoch :  35 \t loss : tensor(0.0377, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6141287549380138\n",
      "Epoch :  37 \t loss : tensor(0.1128, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5723547081579808\n",
      "Epoch :  39 \t loss : tensor(0.0361, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4534100083373247\n",
      "Test Accuracy of the model: 88.40\n",
      "Epoch :  1 \t loss : tensor(1.8556, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.660701762041934\n",
      "Epoch :  3 \t loss : tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8645878084706533\n",
      "Epoch :  5 \t loss : tensor(0.6590, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5966365249833699\n",
      "Epoch :  7 \t loss : tensor(0.4570, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4776256951320906\n",
      "Epoch :  9 \t loss : tensor(0.3242, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3610453950966838\n",
      "Epoch :  11 \t loss : tensor(0.3302, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32638988999456925\n",
      "Epoch :  13 \t loss : tensor(0.2319, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3636600083247144\n",
      "Epoch :  15 \t loss : tensor(0.1318, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3148276343665562\n",
      "Epoch :  17 \t loss : tensor(0.2149, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3209682487032798\n",
      "Epoch :  19 \t loss : tensor(0.2300, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3317947628996305\n",
      "Epoch :  21 \t loss : tensor(0.2176, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3764210985214584\n",
      "Epoch :  23 \t loss : tensor(0.1670, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3131136946392648\n",
      "Epoch :  25 \t loss : tensor(0.1136, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41613219305013055\n",
      "Epoch :  27 \t loss : tensor(0.1426, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.505367145461071\n",
      "Epoch :  29 \t loss : tensor(0.0355, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35279096733673904\n",
      "Epoch :  31 \t loss : tensor(0.0490, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4220966812740167\n",
      "Epoch :  33 \t loss : tensor(0.0503, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.415151965434124\n",
      "Epoch :  35 \t loss : tensor(0.0301, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3660685916636192\n",
      "Epoch :  37 \t loss : tensor(0.0171, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.343570049536379\n",
      "Epoch :  39 \t loss : tensor(0.0276, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35286822221721564\n",
      "Test Accuracy of the model: 92.27\n",
      "Epoch :  1 \t loss : tensor(1.9613, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7481866316251888\n",
      "Epoch :  3 \t loss : tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9287709131481282\n",
      "Epoch :  5 \t loss : tensor(0.6295, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5881106330337551\n",
      "Epoch :  7 \t loss : tensor(0.4769, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5322954658635267\n",
      "Epoch :  9 \t loss : tensor(0.3746, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5305031677615725\n",
      "Epoch :  11 \t loss : tensor(0.3762, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4379558795720819\n",
      "Epoch :  13 \t loss : tensor(0.2238, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32828083185989715\n",
      "Epoch :  15 \t loss : tensor(0.1743, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44697044211924947\n",
      "Epoch :  17 \t loss : tensor(0.1436, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31779237848878705\n",
      "Epoch :  19 \t loss : tensor(0.0967, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40819110833408107\n",
      "Epoch :  21 \t loss : tensor(0.3224, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41275021590735034\n",
      "Epoch :  23 \t loss : tensor(0.4248, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40577616866593447\n",
      "Epoch :  25 \t loss : tensor(0.0975, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.354414056049888\n",
      "Epoch :  27 \t loss : tensor(0.1176, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4376260700106805\n",
      "Epoch :  29 \t loss : tensor(0.0415, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.363497370028075\n",
      "Epoch :  31 \t loss : tensor(0.0517, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3497680299978178\n",
      "Epoch :  33 \t loss : tensor(0.0324, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.305732914183183\n",
      "Epoch :  35 \t loss : tensor(0.1631, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40044198188983354\n",
      "Epoch :  37 \t loss : tensor(0.0252, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22983302179129883\n",
      "Epoch :  39 \t loss : tensor(0.0205, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26182886032153546\n",
      "Test Accuracy of the model: 95.03\n",
      "Epoch :  1 \t loss : tensor(1.8520, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7837138481382633\n",
      "Epoch :  3 \t loss : tensor(1.1109, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9678407653934832\n",
      "Epoch :  5 \t loss : tensor(0.6294, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5791027063735011\n",
      "Epoch :  7 \t loss : tensor(0.5387, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5732716268526766\n",
      "Epoch :  9 \t loss : tensor(0.4028, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46942990504334375\n",
      "Epoch :  11 \t loss : tensor(0.3899, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49057181239133696\n",
      "Epoch :  13 \t loss : tensor(0.2877, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4710227689878513\n",
      "Epoch :  15 \t loss : tensor(0.1772, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41833655258195446\n",
      "Epoch :  17 \t loss : tensor(0.1700, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33763596665124873\n",
      "Epoch :  19 \t loss : tensor(0.1847, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4931609815902673\n",
      "Epoch :  21 \t loss : tensor(0.1333, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5691849550369822\n",
      "Epoch :  23 \t loss : tensor(0.2448, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46088396739510723\n",
      "Epoch :  25 \t loss : tensor(0.2039, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39190850288110973\n",
      "Epoch :  27 \t loss : tensor(0.1143, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.30409001070371194\n",
      "Epoch :  29 \t loss : tensor(0.0667, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3914754097581115\n",
      "Epoch :  31 \t loss : tensor(0.0968, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3297914375330904\n",
      "Epoch :  33 \t loss : tensor(0.0643, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4434854805594628\n",
      "Epoch :  35 \t loss : tensor(0.0587, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3884406171901878\n",
      "Epoch :  37 \t loss : tensor(0.0672, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3934808857758194\n",
      "Epoch :  39 \t loss : tensor(0.0565, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.38627772593771614\n",
      "Test Accuracy of the model: 92.27\n",
      "Epoch :  1 \t loss : tensor(1.7461, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7706749791005285\n",
      "Epoch :  3 \t loss : tensor(1.0273, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.953600727837791\n",
      "Epoch :  5 \t loss : tensor(0.6736, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6727674710995183\n",
      "Epoch :  7 \t loss : tensor(0.5588, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6004228642626924\n",
      "Epoch :  9 \t loss : tensor(0.4348, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5341681950891486\n",
      "Epoch :  11 \t loss : tensor(0.2800, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.406107966346364\n",
      "Epoch :  13 \t loss : tensor(0.2219, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3523334965464889\n",
      "Epoch :  15 \t loss : tensor(0.1844, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39040406856559723\n",
      "Epoch :  17 \t loss : tensor(0.1191, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41225473625322834\n",
      "Epoch :  19 \t loss : tensor(0.1655, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6329794404412102\n",
      "Epoch :  21 \t loss : tensor(0.3383, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5613517661856608\n",
      "Epoch :  23 \t loss : tensor(0.3165, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4391899656309865\n",
      "Epoch :  25 \t loss : tensor(0.2911, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.377073632305436\n",
      "Epoch :  27 \t loss : tensor(0.1246, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3495360144736907\n",
      "Epoch :  29 \t loss : tensor(0.1111, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3202487049156062\n",
      "Epoch :  31 \t loss : tensor(0.0436, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5114974448013501\n",
      "Epoch :  33 \t loss : tensor(0.0368, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44277774321434\n",
      "Epoch :  35 \t loss : tensor(0.0320, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3615908969767368\n",
      "Epoch :  37 \t loss : tensor(0.0195, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44653770734527964\n",
      "Epoch :  39 \t loss : tensor(0.0148, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4985363306552743\n",
      "Test Accuracy of the model: 90.06\n",
      "Epoch :  1 \t loss : tensor(1.6161, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.658248734024917\n",
      "Epoch :  3 \t loss : tensor(0.9548, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0086849620288298\n",
      "Epoch :  5 \t loss : tensor(0.6069, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5699837050299075\n",
      "Epoch :  7 \t loss : tensor(0.4257, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5261583387881856\n",
      "Epoch :  9 \t loss : tensor(0.3742, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5086261758294213\n",
      "Epoch :  11 \t loss : tensor(0.3371, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5005086030918516\n",
      "Epoch :  13 \t loss : tensor(0.3223, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4529505636704909\n",
      "Epoch :  15 \t loss : tensor(0.2517, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.45007060521824344\n",
      "Epoch :  17 \t loss : tensor(0.3776, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5051952770073375\n",
      "Epoch :  19 \t loss : tensor(0.1463, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5778032507252142\n",
      "Epoch :  21 \t loss : tensor(0.1625, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44281437005345387\n",
      "Epoch :  23 \t loss : tensor(0.1005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3320670204185576\n",
      "Epoch :  25 \t loss : tensor(0.0681, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3681289030887615\n",
      "Epoch :  27 \t loss : tensor(0.0713, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3681800545820672\n",
      "Epoch :  29 \t loss : tensor(0.0450, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26260925432602533\n",
      "Epoch :  31 \t loss : tensor(0.0545, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.369971911719095\n",
      "Epoch :  33 \t loss : tensor(0.0292, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29380184112560326\n",
      "Epoch :  35 \t loss : tensor(0.0192, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24148761872225136\n",
      "Epoch :  37 \t loss : tensor(0.0177, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2749666915642622\n",
      "Epoch :  39 \t loss : tensor(0.0150, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3561395970459669\n",
      "Test Accuracy of the model: 95.03\n",
      "Epoch :  1 \t loss : tensor(1.7584, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7245507912660898\n",
      "Epoch :  3 \t loss : tensor(0.9937, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9491903920129016\n",
      "Epoch :  5 \t loss : tensor(0.6054, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6147686256897621\n",
      "Epoch :  7 \t loss : tensor(0.4839, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48669243132942874\n",
      "Epoch :  9 \t loss : tensor(0.4158, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4155531187119756\n",
      "Epoch :  11 \t loss : tensor(0.3750, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33375865894123957\n",
      "Epoch :  13 \t loss : tensor(0.2916, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3325479079262969\n",
      "Epoch :  15 \t loss : tensor(0.2260, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3104761042815314\n",
      "Epoch :  17 \t loss : tensor(0.1827, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32569947245625047\n",
      "Epoch :  19 \t loss : tensor(0.1498, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2097770360356884\n",
      "Epoch :  21 \t loss : tensor(0.1696, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3699089649636706\n",
      "Epoch :  23 \t loss : tensor(0.1469, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2677118928408568\n",
      "Epoch :  25 \t loss : tensor(0.1069, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26700636975365466\n",
      "Epoch :  27 \t loss : tensor(0.1147, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3170507548654566\n",
      "Epoch :  29 \t loss : tensor(0.0507, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19156599869421187\n",
      "Epoch :  31 \t loss : tensor(0.0393, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19787452208289272\n",
      "Epoch :  33 \t loss : tensor(0.0648, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.1122498126824479\n",
      "Epoch :  35 \t loss : tensor(0.0364, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24017333820141093\n",
      "Epoch :  37 \t loss : tensor(0.0577, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.18505511935936572\n",
      "Epoch :  39 \t loss : tensor(0.0300, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22485617517425371\n",
      "Test Accuracy of the model: 96.69\n",
      "Epoch :  1 \t loss : tensor(1.9086, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6946131904876849\n",
      "Epoch :  3 \t loss : tensor(0.9966, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0026877847928484\n",
      "Epoch :  5 \t loss : tensor(0.6410, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6062633700421949\n",
      "Epoch :  7 \t loss : tensor(0.4720, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5111152265177715\n",
      "Epoch :  9 \t loss : tensor(0.3437, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4721708825096443\n",
      "Epoch :  11 \t loss : tensor(0.2171, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3933183553304949\n",
      "Epoch :  13 \t loss : tensor(0.2987, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42794895604906463\n",
      "Epoch :  15 \t loss : tensor(0.1717, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37368634377125437\n",
      "Epoch :  17 \t loss : tensor(0.1745, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3468156869055246\n",
      "Epoch :  19 \t loss : tensor(0.1234, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23779730250417552\n",
      "Epoch :  21 \t loss : tensor(0.0992, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24777589204014694\n",
      "Epoch :  23 \t loss : tensor(0.0654, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26952331643021393\n",
      "Epoch :  25 \t loss : tensor(0.0698, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2940071006965627\n",
      "Epoch :  27 \t loss : tensor(0.0438, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3054612479566365\n",
      "Epoch :  29 \t loss : tensor(0.0198, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32537442111799814\n",
      "Epoch :  31 \t loss : tensor(0.0743, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44206725934194946\n",
      "Epoch :  33 \t loss : tensor(0.0206, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.422070059089615\n",
      "Epoch :  35 \t loss : tensor(0.0759, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7541854010372425\n",
      "Epoch :  37 \t loss : tensor(0.4059, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4943118761596796\n",
      "Epoch :  39 \t loss : tensor(0.0952, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23519960555322572\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.8789, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7202824486498551\n",
      "Epoch :  3 \t loss : tensor(1.0619, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.951595726175074\n",
      "Epoch :  5 \t loss : tensor(0.6193, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6093222874259426\n",
      "Epoch :  7 \t loss : tensor(0.5348, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6232176666337823\n",
      "Epoch :  9 \t loss : tensor(0.3240, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5276866968365005\n",
      "Epoch :  11 \t loss : tensor(0.3774, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.38859906939094496\n",
      "Epoch :  13 \t loss : tensor(0.2610, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37864062604491927\n",
      "Epoch :  15 \t loss : tensor(0.1471, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3493515873872057\n",
      "Epoch :  17 \t loss : tensor(0.1329, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41938292668999627\n",
      "Epoch :  19 \t loss : tensor(0.0735, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4191979121419327\n",
      "Epoch :  21 \t loss : tensor(0.1828, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3946320005657165\n",
      "Epoch :  23 \t loss : tensor(0.0448, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5354841226688721\n",
      "Epoch :  25 \t loss : tensor(0.1858, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5193614297414052\n",
      "Epoch :  27 \t loss : tensor(0.2047, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5176952916709898\n",
      "Epoch :  29 \t loss : tensor(0.0862, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4089703028696385\n",
      "Epoch :  31 \t loss : tensor(0.0609, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31792498648008816\n",
      "Epoch :  33 \t loss : tensor(0.0697, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.27190197826786605\n",
      "Epoch :  35 \t loss : tensor(0.0287, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29681476372266963\n",
      "Epoch :  37 \t loss : tensor(0.0281, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3505477299884077\n",
      "Epoch :  39 \t loss : tensor(0.0160, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37869145319210545\n",
      "Test Accuracy of the model: 93.37\n",
      "Epoch :  1 \t loss : tensor(1.7166, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7310462029276792\n",
      "Epoch :  3 \t loss : tensor(0.9177, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.871585085194856\n",
      "Epoch :  5 \t loss : tensor(0.5973, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6572439292502735\n",
      "Epoch :  7 \t loss : tensor(0.4405, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5590234757108997\n",
      "Epoch :  9 \t loss : tensor(0.3378, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5095304576239411\n",
      "Epoch :  11 \t loss : tensor(0.2269, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48191359979932197\n",
      "Epoch :  13 \t loss : tensor(0.2398, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5133477973286603\n",
      "Epoch :  15 \t loss : tensor(0.3123, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4914533613112166\n",
      "Epoch :  17 \t loss : tensor(0.2784, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4968129093943212\n",
      "Epoch :  19 \t loss : tensor(0.1341, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5576366132992278\n",
      "Epoch :  21 \t loss : tensor(0.0991, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4578759065580962\n",
      "Epoch :  23 \t loss : tensor(0.0838, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5205640784263396\n",
      "Epoch :  25 \t loss : tensor(0.1024, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49311964015563103\n",
      "Epoch :  27 \t loss : tensor(0.0795, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.550483231896155\n",
      "Epoch :  29 \t loss : tensor(0.0696, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5459047071501538\n",
      "Epoch :  31 \t loss : tensor(0.0313, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5503067115082897\n",
      "Epoch :  33 \t loss : tensor(0.0207, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.596781484166806\n",
      "Epoch :  35 \t loss : tensor(0.0396, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5809699580658084\n",
      "Epoch :  37 \t loss : tensor(0.0236, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44236817950531\n",
      "Epoch :  39 \t loss : tensor(0.0229, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5650623580824418\n",
      "Test Accuracy of the model: 92.27\n",
      "Epoch :  1 \t loss : tensor(1.7376, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.751701265135092\n",
      "Epoch :  3 \t loss : tensor(0.9345, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.0202862973037756\n",
      "Epoch :  5 \t loss : tensor(0.6917, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7094200857159741\n",
      "Epoch :  7 \t loss : tensor(0.3931, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5439101612244982\n",
      "Epoch :  9 \t loss : tensor(0.3572, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4423187119911569\n",
      "Epoch :  11 \t loss : tensor(0.2921, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3907487772362428\n",
      "Epoch :  13 \t loss : tensor(0.2643, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40220854951035273\n",
      "Epoch :  15 \t loss : tensor(0.1316, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39866975608562055\n",
      "Epoch :  17 \t loss : tensor(0.0888, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.378437022081282\n",
      "Epoch :  19 \t loss : tensor(0.1769, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36998412803359537\n",
      "Epoch :  21 \t loss : tensor(0.1627, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5031034523156858\n",
      "Epoch :  23 \t loss : tensor(0.1779, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49564644631921856\n",
      "Epoch :  25 \t loss : tensor(0.2363, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6783469128314564\n",
      "Epoch :  27 \t loss : tensor(0.2217, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49471997931502454\n",
      "Epoch :  29 \t loss : tensor(0.1224, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4347173980652251\n",
      "Epoch :  31 \t loss : tensor(0.0506, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44902386604563205\n",
      "Epoch :  33 \t loss : tensor(0.0620, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36473241763882286\n",
      "Epoch :  35 \t loss : tensor(0.0375, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36966428276695446\n",
      "Epoch :  37 \t loss : tensor(0.0241, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6148420895194162\n",
      "Epoch :  39 \t loss : tensor(0.0535, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3906642749972176\n",
      "Test Accuracy of the model: 92.27\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 3\n",
    "run_times_snapture, test_scores_snapture, pred_history_snapture, true_history_snapture, loss_list_snapture, val_losses_snapture, acc_snapture, conf_mat_snapture = cnnlstm_grit_snapture(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8839779  0.92265193 0.93922652]\n",
      " [0.89502762 0.8839779  0.92265193]\n",
      " [0.95027624 0.92265193 0.90055249]\n",
      " [0.95027624 0.96685083 0.91712707]\n",
      " [0.93370166 0.92265193 0.92265193]]\n",
      "[0.91528545 0.90055249 0.92449355 0.94475138 0.92633517]\n",
      "0.9222836095764272\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "test_scores_snapture = np.asarray(test_scores_snapture)\n",
    "print(test_scores_snapture)\n",
    "#mean test results for each trial\n",
    "mean_test_scores_per_trial_snapture = np.mean(test_scores_snapture, axis=1)\n",
    "print(mean_test_scores_per_trial_snapture)\n",
    "print(np.mean(mean_test_scores_per_trial_snapture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[172.76380328 172.06960765 171.2258079 ]\n",
      " [172.88120283 172.26791362 171.93563082]\n",
      " [172.08997154 172.35302242 173.79279244]\n",
      " [172.93996635 173.91201556 171.11345411]\n",
      " [174.6581861  171.66760939 174.53918754]]\n",
      "[172.01973961 172.36158242 172.74526213 172.65514534 173.62166101]\n",
      "172.68067810220003\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "run_times_snapture = np.asarray(run_times_snapture)\n",
    "print(run_times_snapture)\n",
    "#mean test results for each trial\n",
    "mean_run_times_per_trial_snapture = np.mean(run_times_snapture, axis=1)\n",
    "print(mean_run_times_per_trial_snapture)\n",
    "print(np.mean(mean_run_times_per_trial_snapture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1_snapture = []\n",
    "for one_trial_confs_snapture in conf_mat_snapture:\n",
    "    one_trial_f1_snapture = []\n",
    "    for one_trial_conf_snapture in one_trial_confs_snapture:\n",
    "        recall = np.diag(one_trial_conf_snapture.numpy()) / np.sum(one_trial_conf_snapture.numpy(), axis = 1)\n",
    "        precision = np.diag(one_trial_conf_snapture.numpy()) / np.sum(one_trial_conf_snapture.numpy(), axis = 0)\n",
    "        recall = np.mean(recall)\n",
    "        precision = np.mean(precision)\n",
    "        one_trial_f1_snapture.append(2 * (precision * recall) / (precision + recall))\n",
    "    all_f1_snapture.append(one_trial_f1_snapture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88668856 0.92590535 0.94111755]\n",
      " [0.89942976 0.88958818 0.92544402]\n",
      " [0.95077967 0.92568816 0.90388557]\n",
      " [0.95305043 0.96770074 0.919719  ]\n",
      " [0.93515745 0.92544934 0.92594344]]\n",
      "[0.91790382 0.90482065 0.92678447 0.94682339 0.92885007]\n",
      "0.9250364811878745\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "all_f1_snapture = np.asarray(all_f1_snapture)\n",
    "print(all_f1_snapture)\n",
    "#mean test results for each trial\n",
    "mean_f1_per_trial_snapture = np.mean(all_f1_snapture, axis=1)\n",
    "print(mean_f1_per_trial_snapture)\n",
    "print(np.mean(mean_f1_per_trial_snapture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  2., 15.,  2.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  2., 16.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  1.,  1., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  1.,  1.,  0.,  0.,  0.,  0., 18.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1., 14.,  3.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 18.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., 18.,  0.,  1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 16.,  0.,  1.,  1.,  0.,  1.,  0.],\n",
      "        [ 1.,  0.,  2., 16.,  1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 19.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 17.,  2.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  3.,  2.,  0., 15.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 18.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 18.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1., 18.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 17.,  1.,  0.,  0.,  2.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 11.,  7.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 19.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  1., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  0.,  0.,  0., 18.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[18.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 14.,  4.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 21.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  2., 16.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0., 20.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  1., 18.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 18.,  1.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1., 19.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  1.,  0., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0., 18.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 17.,  1.,  0.,  1.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., 17.,  1.,  1.,  0.,  0.,  1.],\n",
      "        [ 2.,  0.,  0.,  0., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  1., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0., 17.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 17.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  4., 16.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  1.,  0., 17.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  2.,  0.,  0., 17.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 17.,  2.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  1.,  0., 20.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., 20.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  2.,  0.,  0.,  0.,  0.,  0.,  0., 18.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 19.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 18.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0., 18.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 3.,  0., 11.,  2.,  1.,  0.,  2.,  0.,  0.],\n",
      "        [ 2.,  0.,  0., 17.,  1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., 20.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 16.,  1.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1., 17.,  2.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  1.,  0.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[17.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 16.,  2.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1., 18.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0., 15.,  4.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 18.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  5., 14.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 20.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0., 19.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., 19.]])\n"
     ]
    }
   ],
   "source": [
    "for one_trial_confs_snapture in conf_mat_snapture:\n",
    "    for one_trial_conf_snapture in one_trial_confs_snapture:\n",
    "        print(one_trial_conf_snapture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.          0.          0.33333334  0.          0.6666667   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334  1.         15.          1.6666666   0.6666667   0.33333334\n",
      "   0.          0.33333334  0.33333334]\n",
      " [ 0.6666667   0.          1.3333334  16.666666    1.3333334   0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.6666667   0.          0.          0.6666667  18.666666    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.6666667   0.6666667   0.33333334 18.666666\n",
      "   0.          0.33333334  0.33333334]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "  20.333334    0.          0.        ]\n",
      " [ 0.          0.33333334  0.33333334  0.          0.          0.\n",
      "   0.         19.          0.33333334]\n",
      " [ 0.          0.          0.          0.          0.33333334  0.\n",
      "   0.          0.33333334 19.333334  ]]\n",
      "\n",
      "\n",
      "[[18.333334    0.          0.33333334  0.          0.33333334  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.          0.33333334  0.          0.          0.6666667\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334 14.666667    4.          0.6666667   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.33333334 19.          0.6666667   0.\n",
      "   0.          0.          0.6666667 ]\n",
      " [ 0.33333334  0.          0.6666667   1.3333334  17.333334    0.33333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          1.6666666   0.6666667   0.33333334 18.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.33333334\n",
      "  19.666666    0.          0.33333334]\n",
      " [ 0.          0.          0.33333334  0.33333334  0.          0.\n",
      "   0.         18.666666    0.6666667 ]\n",
      " [ 0.          0.          0.33333334  0.33333334  0.          0.\n",
      "   0.33333334  0.6666667  18.333334  ]]\n",
      "\n",
      "\n",
      "[[18.333334    0.          0.          0.          0.33333334  0.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.33333334 19.666666    0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334 17.333334    0.6666667   0.33333334  0.33333334\n",
      "   0.          0.          0.6666667 ]\n",
      " [ 0.          0.          1.6666666  17.333334    0.6666667   0.33333334\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 1.          0.          0.33333334  0.33333334 18.333334    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  0.6666667   0.33333334  0.33333334 19.\n",
      "   0.          0.33333334  0.        ]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.33333334\n",
      "  19.666666    0.33333334  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "   0.         19.          0.6666667 ]\n",
      " [ 0.          0.          0.6666667   0.          0.          0.6666667\n",
      "   0.          0.         18.666666  ]]\n",
      "\n",
      "\n",
      "[[18.666666    0.          0.          0.          0.33333334  0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 1.          0.         15.666667    1.6666666   0.33333334  0.\n",
      "   0.6666667   0.          0.33333334]\n",
      " [ 0.6666667   0.33333334  0.33333334 18.333334    0.6666667   0.33333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334  0.          0.          0.33333334 19.333334    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  0.          0.33333334  0.         20.333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.33333334  0.          0.\n",
      "  19.666666    0.          0.6666667 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         19.666666    0.33333334]\n",
      " [ 0.          0.6666667   0.          0.          0.          0.\n",
      "   0.          0.         19.333334  ]]\n",
      "\n",
      "\n",
      "[[17.666666    0.          0.33333334  0.          1.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334 16.666666    1.          0.6666667   0.\n",
      "   0.          0.33333334  0.6666667 ]\n",
      " [ 0.          0.          2.3333333  16.333334    1.6666666   0.33333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334  0.          0.33333334  0.33333334 19.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  0.33333334  0.          0.         20.\n",
      "   0.          0.33333334  0.        ]\n",
      " [ 0.          0.          0.          0.33333334  0.          0.\n",
      "  20.          0.          0.33333334]\n",
      " [ 0.          0.          0.33333334  0.          0.          0.\n",
      "   0.         18.333334    1.3333334 ]\n",
      " [ 0.          0.          0.          0.          0.33333334  0.\n",
      "   0.          0.         19.666666  ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_conf_avg_snapture = []\n",
    "for one_trial_confs_snapture in conf_mat_snapture:\n",
    "    temp = []\n",
    "    for one_trial_conf_snapture in one_trial_confs_snapture:\n",
    "        temp.append(np.array(one_trial_conf_snapture))\n",
    "    all_conf_avg_snapture.append(np.mean(np.array(temp), axis=0))\n",
    "    print(np.mean(np.array(temp), axis=0))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def displayConfMat(confusion_matrix, save_file_name):\n",
    "    #confusion_matrix = confusion_matrix.numpy()\n",
    "    #print(confusion_matrix)\n",
    "    font = {'size'   : 5}\n",
    "    plt.rc('font', **font)\n",
    "    figure(num=None, figsize=(1080, 1080), dpi=300, facecolor='w', edgecolor='k')\n",
    "    plt_conf = ConfusionMatrixDisplay(confusion_matrix=np.array(confusion_matrix),\n",
    "                                  display_labels=np.array(classes))\n",
    "    plt_conf.plot(xticks_rotation='vertical', cmap='Blues',values_format='.5g')\n",
    "    plt.gcf().subplots_adjust(bottom=0.19)\n",
    "    plt_conf.figure_.savefig(save_file_name, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Results for Snapture - best case\n",
      "Accuracy: 0.94475\n",
      "F1: 0.94682\n",
      "===============================\n",
      "===============================\n",
      "Results for Snapture - worst case\n",
      "Accuracy: 0.90055\n",
      "F1: 0.90482\n",
      "===============================\n",
      "===============================\n",
      "Results for Snapture - avg case\n",
      "Accuracy: 0.92228\n",
      "F1: 0.92504\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD9CAYAAAALdjoMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkfUlEQVR4nO3deXxU5dnw8d81ITsEMEFEtsgmhoBAAgGpLG7VunR5a221VUv72r5a1NrW2tpH6uNSl66i1Vprqzy+Wq1WbcVSEKPsENlC2UQBERQIBEUgmOV6/jgTGDFzZpI5Z+YMub6fz3ySmXPmOneGcOU+97nPfYmqYowxQRZKdQOMMSYWS1TGmMCzRGWMCTxLVMaYwLNEZYwJvA6pbkCiJCtfJaer53FHDDzB85jpyK9rwuJTXHPEsmVv1Khqt0RiZBT0VW046LqPHtw1U1XPTeQ4saR/osrpSnbFtZ7Hnf/yjZ7HTEcNjU2+xO2QYZ15v+VmypZEY2hDHdmDv+q6T93yaUWJHieWtE9UxhgfCSCJ9X9F5AvAcKAJyABqVfW34W3jgdOAnar6aLQY9mfNGONOQu4PKBKRqojHVZFvV9XngV8Bh4DbgeyIzWNV9S7A9RTVelTGGHexe1Q1qloe/e2SAdwINLSwOa5hUEtUxhgXAqGMRIPchpNr3gVuBmpFpAswBlgkIj8GdroFsERljIlOaD69azNV/WmUTf8Kf309VgxLVMYYF5LwYLoXLFEZY9wlfuqXMEtUxhgXkvCpnxdSlqhE5KbwZclo208CKlT1qXhjjhvam8nnD+eP/1jGuKG96dwxh1seqQRgcJ9CPjd2IP/ZtIvXVmzh2i+PpuaDAzw5ezVXnHcq44b25pZHKtn03t5W/Rzzl21kyaq36XZcJ75+0dhWvTdd4y5YvpGl1Zso6tqRyy4cy+wFa6je8C4l/XvQpSD/cPxhJ/fm1cVr6dAhg+9cMjFl7T0W4vodOyoP5lF5IempUkQmiMhNQJaITBGRm0WkTER+Fv5+oojcAfQBJohIzxZiXNU8Z0Pr9x9+fX71Vqrf2smh+kaKOudx8NCRq6EXjBvEvgOHAJg0ophQSGhsUg4eauCh599g3ZaaVicpgKXVm/j+ledQU/tRq9+brnGrqjdx3eVnszsco7y0mO07asnOyqS8tJg9e/ejCiUDTqSpSak7VJ/S9h4Lcf2O7Sr2PCrfpaJP1wnnUuStwCNAJnAmzoSwrPA+LwFbgNdUddvRAVT1YVUtV9Vyycz/1AEG9DyO2x+bS31D4+HXjivIZfrMakae3IOMDGHxmm1kZWZQ2DmXos557Nx7oE0/jF9/bIIcV44K0qUgj7t+eDFbtu8mIyPE1Cmf52DdxwBM+cZZdMrLSeBYCTX1mInrd2yXo0JGhvsjCVJx6jcQ2IMznf7bQCPwCvAD4OPwPk3AbmCciMxX1a3xBC49qRsVQ3ry2MsrmfLl0WSEhIL8bEYNPpEX5q7n6i+W88FHdcxd9Q7XX1xBSITafXVced6p/GPehjb9MOWlJ/Hbx2bRrWvHNr0/HeOWlRZz3+Oz6FqQz8p171C9YRubt9VQPqSYF+csZ+1b79HrhK5ULl5H1epN5OZkxQ7qY3uPhbh+x47Kg+kJnjQj3ddMDxX0Uj9uSq61m5IBuyk5neVmyhtuM8bjESroqdmjrnHdp27OzQkfJxa76meMceHJzPSEWaIyxrgLwKmfJSpjTHRiM9ONMenAelTGmGCzMSpjTDqwUz9jTKAFZB5V2ieqEQNP8KUQQ9exN3geE6B24a99iesXm+/U3tmpnzEmHViPyhgTeDZGZYwJNGnn61EZY9KDhFKfqFLfAmNMYDnr5onrI2YMZ425p0SkXESuF5GnI7b9PPzaILcY1qMyxkQn4Ye7IhGpinj+sKo+3PxEVStFZIyqVonIdpxCpM12AwWxDmCJyhjjQgjFPvVzLUB6lP8D/LX5iapOE5EQcBNwZ7Q3WaIyxriK5/QuxvtPxVkE8zygu6ruFJGBOD2pQcAQYJ5bDEtUxhhXiSYqVV0JXBh++nL4tTfDz9+IJ0Yqq9Ccp6ovh78XDS81Gqs6TVt5XcHjc+NLGTqwJ02qZISEvfsO8tBfYxZ8jVu6VUmxuP7G9Tt2VPGNUfkuFVVoLhWRKUB5RMWZchH5LxH5bHifPBG5V0RuF5FuLcQ4XIVmV82uuI7rdQWPGa+v5oEnK/n44wZ++edZZGd6m/PTrUqKxfU3rt+xo5HwGJXbIxlSMT3hZFWdBjTXUHoJmAT8WlVnhl8rBTKAXThlsz4hsgpNt6JP5bEWeT25NhQSrv36JLKz/OmUpluVFIvrb1y/Y7sfN7HpCV5IxanfehH5Hk6ZLHAqzvwbuEFEloRfW41TnaYBiKsCTSxeV/C4+arz6JARYvvO3fzgyrPZu69t5baiSbcqKRbX37h+x3aTrGTk2oZ0r0JTVlau8xdXxd6xlWz1BJPuvKhC06Gon3a5IOqsAQB2P/Y1q0JjjEkdIXmnd24sURljXFmiMsYEX+rzlCUqY4wLIWlTENxYojLGuLJTP2NMoNlgujEm+AQkZIkqYY2qHDjU4Hlcv+Y7Dbz+BV/iLr3jPF/idszx51fEz+o2h+obfYmbnelPNZaGxiZf4nrFelTGmMCzRGWMCb7U5ylLVMaY6ETiWuHTd5aojDGu7NTPGBN4lqiMMYFn0xOMMcEmnhR3mAh8F3gKKAfmqOqc8LYvAf2Bjar692gxUj9KZowJLKcAqfuDcF2/iMdVkTFUtRJYAezHWdk3J2LzQFW9F6caTVTWozLGuBBCsU/94qrrp6qzgFki8jNgRvPL8bQiJYmqpUozIlIMjAGK/ahCs2DZm0x/YQEP3nqFp3ETqQwyekAhXzutmJeWb2No7y4sfLOGRW/WAFDe7zhGFB/His21bNr1EV89rS/v1BxgwYZdXDCyJ+NP6c73Hl3CgY+jz8JetHwjT7y4gGlTLwdg1vzVbNq6i8+UD6JDRgaz5q9mcL8enNS7G5WL1vLaknU8du93YrZ7wfKNLK3eRFHXjlx24VhmL1hD9YZ3Kenfgy4F+Yc/j2En9+bVxWvp0CGD71wysVWfTTMvKq8sXL6RqupNFB7XiUsvGMOiFW9RtXozZUP60r/P8Ux/YQHFPYs4vXwQz89expyFa/nTnZPJz8tOSXuT+fnGw8O6fv+Fk5g+iKjrt1FEfgSsd4uRqlO/DBG5RkT+LiK3isjUyI0i0kdE7hCRO6WFTymyCs3umpq4DnjayIEMGdDTo+YfkUhlkCUbd7N224ccPNRIfaOS3eHIP8eZpSdQV99IkyrnDT+RfQcbaFKlZt8h/vLa26zaUuuapADGjBhAScTP/MqC/5DZIYPMDhn8a+4qOob/I/brfTyXfX4c5UP7xdXuqupNXHf52ewO/8zlpcVs31FLdlYm5aXF7Nm7H1UoGXAiTU1K3aH6GBGj86LyStXqzUyJaO/MeavJyc4kFBJenLOCgvwcQiIcX1jAVZdMZERJnzYlKc/am8TPN6YYp33x5DBVXamqF6rqbap6u6pOU9U3VfUNVX1OVe9V1efdYqQqUSnwIM556zacnl3kb8ZYYDrwLnD8p94cUYWmsKjI/9a68OLK7fwNu7h/5nqG9Opy+LWOOZlMn7uJzww+ng4ZIV5bu4P+3TsBUNKrM2u2fdDq44RCwje/PJ5/vrqCvR/s55LzK1i57h0AXl20hkljTokrztF/O7oU5HHXDy9my/bdZGSEmDrl8xys+xiAKd84i055OS2FifNYbX5rRIxPBvlofx3fvng8lUvW09DQyBljS9iwZQcAq9ZvpXRQrwSOlVBTwzGS9/nGbAvO743bIxlSNUbVpKpNIqJAL5xKM4citi/AuUogwE4vDvifN7extHoTcxau4YyxJV6EBBKrDDL4xALKTjqOa84ZhCrsq6unb1E+HXM6MHfdTr45sT/b9hxgyVu7+dKo3jSGC3FMPKU7j8zZGDP+mo3bqFq9ifunz+JLny3n1MF9+ONfKykZcCKFXTrxp2dep6BjLuB8PueOHxZXu8tKi7nv8Vl0Lchn5bp3qN6wjc3baigfUsyLc5az9q336HVCVyoXr6Nq9SZyc7Ja/dk086LyStmQvkybPpuuBXmsXLeViRWDeejJV+l9QlfGjhjA0zOWHJ59/cqCNVx92ZmpbW8SP994JCsZuUn7KjTDR5bpnLmLPY+bl+1PDrfVExy2esIRfq2e0CknI+HqMLknDtIB33rAdZ/Vt59jVWiMManjTE9IfY/KEpUxxkXyxqHcWKIyxriyHpUxJtjinILgN0tUxpiobIzKGJMWbIzKGBN4AehQpX+iEoQMHzK+X3Nb1v7qQl/idvvi/b7Eff/Za3yJC/5VXvFrvpNf/JxTljAPlnnxQtonKmOMf8SmJxhj0kEAOlSWqIwx7uzUzxgTbDaPyhgTdM4yL6kf7LdEZYxxZT0qY0zg2RiVMSbQnJLulqiMMQEXgA7VsZeo/Ko44mdlEK9jjyvtyeRzh/LHl1YyrrQnnTvmcMuf5wFQcUoPKk7pwc69B1j11i7OGNGH+sYmZix6mwtPG0DH3EzueWrJMfcZH82LajHJjOt3bDch7wqQ/gE4DdinqveFt/0c2AvMUNUNUduQUAs8JiI/E5FvisjU8Pc3R9kvogrNrk9s86viiJ+VQbyOPX/1Nqo37eJQfSNFnfM4GLH/0vXvU1iQiyCs2bKbUEjIzerA1l37qN1XR6e82Otvp+NnfDQvqsUkM67fsd14VYBUVV8F7gIKIzbvBvJjtSFQiQpoBP6CU6XmV0CL/2s+WYWm2ye2+VVxxM/KIH7FHtCzK7f/z0LqI+5bbGpSpv5l/uE14e97bhn7Djixn5yzls3vxa5uk46f8aeP1ea3piSu37HdjpkREtcH4QKkEY+HXULeCPyx+YmqTgN+AXzZrR1BO/VTVVURaQR+AHzc2gB+VRzxszKI17FLi4uoOOVEHpu5milfHElGSCjIz2LUyT3Iz8mkpG8hW3ftY+Lw3ow6uQcHD9UzcmB3Jg3vTV5OZuz2puFnfDQvqsUkM67fsd14WID0OqAfMFZEVuAUIB0EDAHmucZI9yo0I0aWa+V876vQ+LEig5/SbfUEPz/fQK9GkES5mZJwdZjOfU/Rz/z0Mdd9Zny3wqrQGGNSR4CMAFz2s0RljIlOxCZ8GmOCLwB5yhKVMSY6IfF5VF6wRGWMcWW30BhjAi1iUmdKWaIyxriyUz8PNM+c9Vq6zcXZ9ffv+RK321k/9yVu7av/7Utc471AJyoROT7yuaru9L85xpggcQbTU90K9x7VeTj33DV73Oe2GGOCJiDzqKKe36jqYzg3BRcCNUlrkTEmUOJYPcF3sQZiOuIsw3ByEtpijAkYIa7VE3wXK1G9B5QD7yahLcaYAJLw6V+0RzLEuurXAadHFXvVM2PMMSn1I1Sxe1SdVfVWnHVjjDHtTJwL5/nObXrCj4EBInInUJSU1hhjAicIV/2iJipVvVtEegPDCUbvzxiTAgHIUzHHqK7HmZoQAl70vTUeSGYlEwh2NRPPq9ucWszki0bx4N8WMmFkP+o+buCBZxYAUFHah4rSPuys/YhVG7ZzxqiB1Dc2MmPeWi4cP4SOuVnc83hlSj6HYyGu37GjESQQM9NjjVFVAbuIo0pEIkTkJq9iJbOSCQS7monn1W1WbqZ64/uMGdqXB59dyMA+R0YElq7ZSmHnPARYs2lnuLpNJlt3fEDthwfolN+26zFB/nyTGdfv2FGJs3qC2yMZoiYqERkNvAVUAy/4cXARuVZEpgJnisipInK3iNwjIvki8k8RuUFEhrXwvsPlsmp27Tp62yee+1nJxDleQm/3Na5fn8Wzc1ZxxQXlFHQ8sn9TkzL1D/8+XBzivqfmse9AHQBPzlzB5u172vgztOltx1xcv2O7CcV4xCIiE0XkKREZJCK3isj1EdvGi8hNIjLZLYbbqV9uxPd+VYBYC4wCXgNygFdxfvZSnAT5KHAFsCryTeFyPA8DjCwr/0TbklnJBIJdzcTz6jb9u1NR2ocP99cREmHmgvUU5OcwqqQX+blZlPTrztYde5lY1p9RQ3pzsK6ekYN7MqmsP3m5bfucg/z5JjOu37GjEeIaTC8SkaqI5w9HlsxS1UoRGQOcD9wO3BCx71hVvSt88S56O1JZhUZELgYGAmcD1wGX4Xw2twLPAEuB51R1ZbQYI8vK9fUF7pV92yLdVk9oiKjd5yVbPSF9eVGFpvuAUr3s139z3ec3nz8l5nHCwzuHgPuBG1T17vDrN6rqPSLy4+bXWuI6mC4ihUAfYKuqen6/n6o+E/72zvDXwz0nEXldVe/y+pjGmPg59/N5U9cPmA3cDNSKSBdgDLAo3JtyXZ0l1lW/63AG0xuABxNqbStZkjImGBIdLw+fEV3YwqZ/hb++HitGrESVAxTjJCpjTDvTfFNyqsVKVL8BBgBvJqEtxpgACsJobaw2nIMz2H1HEtpijAmgIKxHFatHtQ6n9zcwCW0xxgSMSDBmpsdKVKXABuB3SWiLMSaAgjBTx21m+rU4A+n1OFMUjDHtTHOlZLdHMrj1qM4C5gMTcGamez+r0gOCP5Mz/ZpAmW4TSf2amNl1lD/lvQBql97vW+z2KABnfq6J6hVVtVM+Y9ozgYwAZCq3RPVA0lphjAmkwNf1U1Wb5GmMCXaiMsYYCPhSxMYY4xR3SHUrLFEZY2JIhwmfxph2LPCD6cYYAxL46QnHlEQreKRjdZtktjmR9n5uwjCGDupFkzaREQqxd98BHnqyEoCKYf2oOLUfO/fsY9W6rZwx9hTqGxqZUbmSC88YQce8bO555OWktjcVcf2OHY2zFHFSDuXK82GyaBVlJMqlg5b2F5Hfi8hkETnBq3YlWsEjHavbJLPNibR3xmureOCJV/j44wZ++ad/kZ155O/n0tWbKOySjwiseWs7IRFyszPZ+n4ttR/up1PHthXnsCo0cRLn1M/tkQx+jOf3C1eWIVxd4koR+T7wExG5XkTuPfoNItJDRO4SkTvCy5Z2A3oDl4hIRgv7H65Cs6tm19GbW5ToX4V0rG6TzDYn0t5QSLj28rPIzsr81LamJmXqtBfICxeeuG/6bPbtD1e3+ediNr/bthWyrQpN/IJ+r19bvR3xfXOS+SvO2lazgG+08J7TgQ9x7incA7wBvA/8S1Ubj945sgpN2VFVaKJJtIJHOla3SWabE2nvzd+9gA4ZIbbvqOEHk89l774DFHTMZdTQYvJzsykZcCJb39vDxNGDGTWs2KluU9KXSRWD2011G79jRxOUFT49r0IjIpcB2UAjMBZYhLM28rnhr1dGrocePvV7DKcq8xacElnXh993GnB3S8mqWVlZuc5fXBVtc5ul203J6dZeuynZf15UoSk+ZZje8vg/Xff51ui+CR8nFs97VKr6RMTTxyK+/0v46yeKNkQkrci6Xs2vVXrZNmNM6wiJjw+JyBnAMGCIqv5fEfk18A7wlKq+H0+MlFz1E5GJOGtd1anqU6logzEmDhLXhM9YBUjniMgBYGP4pd1ARyDu04CUJCpVrUzFcY0xrdO8cF4MNXGc+p0D/AJAVe8Qkc7A5cC0eNrRbuZRGWPaJtGh9PCV+xAwWkTewSntPgB4wvWNESxRGWNcJToDIXwx7OcRLz3U2hiWqIwxUYndQmOMSQe2HpUxJvBSn6YsUUWVbtVi0o2fkzK7nt7i7aYJq517V+yd2uBQfdT5zCknaVDcwRhj7NTPGBN8qU9TlqiMMTEEoENlicoYE51gY1TGmMATJAAnf5aojDGuAtChskRljInOpicYY9JCAPJU+0lU6VZ1pD1VofEr7rjh/Zj8hQoefHoeE8oGUPdxAw/8dS4Ag4uP53Onl/Cft97ntaqNXHvpBGr2fsSTLy/jiotGM254P275/Qw2bdudtPY2W7h8I1XVmyg8rhOXXjCGRSveomr1ZsqG9KV/n+OZ/sICinsWcXr5IJ6fvYw5C9fypzsnk5+X3abjxRKEMaqEp1+3tupM8zYROc8tVrS4bZVuVUfaUxUav+LOX/E21W9uZ8ywYh58Zh4D+3Y7vO2CCaXs238IgEmjBhIKCY2NysFD9Tz0zHzWbdrR6iSVaHubVa3ezJSIf7eZ81aTk51JKCS8OGcFBfk5hEQ4vrCAqy6ZyIiSPj4mqWOnCk2rqs6IyC9xCjycKiKdROReEfmNiIwBhonIVBE5K/z9+S0dMBVVaNIxbrpUofE77rOzV3LFRRUU5B/5+Y4ryGP6P5cy8pTeZGSEWFy9haysDhR2yaeoSz4725ho/Ph3+2h/Hd++eDyVS9bT0NDIGWNL2LBlBwCr1m+ldFCvxA/q4lipQtPaqjN7VPXxcI9pCE7Bh+bfoDXAncCPgFWq+lJLB0xFFZp0jJsuVWj8ilvavwcVQ4v5cH8dIRFmLlhLQcccRpX04YXKaq6+5DN88NFB5i57m+svm0AoJNR+eIArLxrNPyqrk97eZmVD+jJt+my6FuSxct1WJlYM5qEnX6X3CV0ZO2IAT89YQijk9DFeWbCGqy87s83HikcQTv0SrkLTlqozqnpXOFHdD9wK5OJUnzkrYts+4D1Vfc7t+H5VoUk36VaFxk92U7KjS16HhKvDDC4drg8/N8d1nwknFwa/Ck1bq840fxWRFcAoYJ2qVkVuM8akWBJP79wk5aqfW9UZVZ0OTE9GO4wxrZf6NJWkRGVVZ4xJT3FWoXGPIXI9zoW711W1SkS+BXQHXlPV+fHESL8BCGNMUkmMB+G6fhGPq44KsQfI4kjHqEhV7wQ+E28b2s2ET2NM28SxcJ5rXT9VfTwc52c4F9tafQXPEpUxxlWiY+nh+ZDlQIOI9AZqROQnwOvxxrBEZYxxlehgeng+ZOScyEdbG8MSlTEmKsHWTDceSseJmX7xa2KmXxNJd1Xe6UtcT4itnmCMSQMByFOWqIwxMQQgU1miMsa4aEe30Bhj0lPEpM6UskRljHEXgExlicoY48pO/YwxgZf6NGWJyhjjJiCDVO0mUQWxSorFTc+4fle3SWb1oFi8WObFCymdztyWCjZtFcQqKRY3PeP6Xd0mmdWD4hHHMi++S/V9F62qYNPMqtBY3CDE9au6TTKrB8UlAJkq1ad+ra1gA1gVGoub2rh+V7dJZvWgeBwTVWgSOngrK9i0xKrQmGRJt5uSO+VkJFwdpvTUkfrczHmu+5zcIz/4VWgS0doKNsaYFEh9hyrlp37GmABzhqFSn6ksURljohMIpT5PWaIyxsRgicoYE2yS8KmfiHwBGA5sUNX/LyK/Bt4BnlLV9+OJkep5VMaYgBNxfxCjrp+qPg/8Cugdfmk30BFoircN1qMyxkTlFHeIuZtrXT8RyQBuBO4FUNU7RKQzcDkwLZ52WKIyxrjy4KrfbTi55vsi8ihwPjAAeML1XREsUUVxqL7Rl7jZmRmxdzKB5Ft1m0m3+BLXK4nebqSqPz3qpYdaG8MSlTEmOpueYIxJD6nPVJaojDFRxTmY7jtLVMYYVwHIU5aojDHugrDCpyUqY4y71OcpS1TGGHcByFOWqIwx0YnYqV9SJVrNZOHyjVRVb6LwuE5cesEYFq14i6rVmykb0pf+fY5n+gsLKO5ZxOnlg3h+9jLmLFzLn+6cTH5edkraa3HTM+64U4uZfNEoHvzbQiaM7OdUuHlmAQAVpX2oKO3DztqPWLVhO2eMGkh9YyMz5q3lwvFD6JibxT2PV3r2sxyW+jwVzJuSg1iFpmr1ZqZEVAaZOW81OdmZhELCi3NWUJCfQ0iE4wsLuOqSiYwo6dPmJOVFey1uesadv3Iz1RvfZ8zQvjz47EIG9ik6EnvNVgo75yHAmk07CYWE3KxMtu74gNoPD9Apv+2/b24CUNshNYkqXGHmyyLyLRH5vojcICKTg1yF5ujc+dH+Or598Xgql6ynoaGRM8aWsGHLDgBWrd9K6aBeCR4vobdb3DSP++ycVVxxQTkFHY9UmGlqUqb+4d/k5WQCcN9T89h3oA6AJ2euYPP2Pd4c/ChxrJ7gu1Sd+h0EegB1QAHwLjAEqCagVWjKhvRl2vTZdC3IY+W6rUysGMxDT75K7xO6MnbEAJ6esYRQyMn7ryxYw9WXndmm43jVXoubnnFL+3enorRPRIWb9RTk5zCqpBf5uVmU9OvO1h17mVjWn1FDenOwrp6Rg3syqaw/ebneV6MRJBBjVCmpQiMiI4GvAe8D9Tjr04wFqghIFRq7Kdkki183JdfNuy3h6jAjRpbrnHmLXfc5Lr/DsVmFRlWXAcuOejlyyYe/hL9aFRpjUiwAHar2c9XPGNMGNj3BGBN0ybyy58YSlTHGXQAylSUqY4wrK0BqjAk8W+HTGBN8lqiMMUHnQQHS8cBpwE5VfVREvgT0Bzaq6t/jiZH2iWrZsjdqcjNlS5y7FwE1PjTD4vob18/Yx3LcvokebPmyN2bmZUlRjN1yRCRy1vXD4btHmo1V1btE5Mfh5wNV9e6I5zGlfaJS1W7x7isiVX7MoLW4/sb1M7bFdaeq53oRJsbzmAK5eoIx5piyKNx72iMiZcBGEfkRsD7eAGnfozLGBJuqvg68HvHSG62N0d56VA/H3sXiBjCun7EtbhpIyeoJxhjTGu2tR2WMSUOWqIwxgWeJyhgTeO0iUYlIj/DXnqluS7xEJO75Ycey5kIffhT8MOnjmB9MF5FLgPHAa8A4Vb3Ow9iFQB9gq6p6NgtZRG4HtgE9VfVnHsb9CZANHFLVX3gYdygwCXhVVas9jNsd+ArOOvqTYy1L3crY56nqyyLyBVV93qOYd+Msq42q3uNFTD/jppP2MI9qMfAesAWY4XHs64BdQAPwoMex5xCluEUCBGd55x94HPdM4M/A9TgFOrxyHtAFZw39BV4FDU8+HBZOsPu8igtsB54GmjyM6WfctHHMn/qp6macqjY9gBKPw+cAxeGHl/4ADMP75NcB+CFtuIUhhjOBG4HRInKjh3FHAUNx/u3GeBVUVe8GHg0/7eRVXCAPOBv4rIcx/YybNtpDjwqc0lve1xKCqUApHvYiwn/tFaf3cxLgZVd/i6o+5mG8ZpfgfA63q+pBr4Kq6jUiMgzntMfrG3xPU9XbPI55EPg33v8h8Ctu2mgviWocTqI6xCen8rfZUQllEt4llM0exWlJr+Z2ezzWcQtQiZO4b/IwLsDFwEfAB8BDXgQMfwaDReSnQIOHn0UtTu8d4HGPYvoZN220l0Tl+dhMeJmKs1V1loicE/sdcVuEkwCzcRKrZ1T1Di/jHR3ep7gdcIYoir0KGP63qwg/rReRPFU94EHoE4AMoBfeJpQQToHeLI/jpo32kqheBr6H94Ppo0VkHjAap2ueMFXdIiK34PzCbwUe8SKuz57AGa/7Hx9i/w74DDDX47hfAWYD5+NcDLk+0YDhsS9E5PJEYx2lC/AfnMri7VJ7SVTnApnA54DlHsadCVwDvOhhTHD+gr6L85c50MKnUWfgTP84M/zwMnbz6XU/vB2va8TpsdYBK7wIKCL/FY6504t4ERTnd6HW47hpo70kqg9weibf9jjuF4CXcHoTXhqGcynan7ryHgqfRmXj/Gea7XXso18Tka+o6tMehP9vnKvAt3hx2hdOqr2At/H4lB34EGeczssrlGnlmE9U4V+gLJwrJ43AAx6Gn4XTU8v0MCbAbFX9vccx/VQLrMU5RfFbbqIBRORcnD8GAkzEg55aS0nVQx/gTNE4wcdjBNoxPzMdDiernUCjqnoyGBn+ZS/DGd/o4NVAdbitGTg9qqb2OhM5GhGZoKqveRDn66rqx5ia50TkOpw/tEtV1cuhi7TRXhLVeJxu8wBV/Z2Hca9R1Qeav3oV1xzh420p94bj2h+DNHDMn/rB4aVQ/bBHRKbSirWfTav5cvuIqv4o8rmHY1/GB+0iUflFVZ9MdRvagebbR8DfOUQJj30Z/1iiMkGXrNtHNvsc3ySgXYxRmfQlIlcQTlJeXQgJx233S6ekE+tRmaDz67aUdr90SjqxRGUCzcfbUpI19mU8YInKBJqPt6W0+6VT0skxv3CeSV8Rt6UI3t+W0rx0SrtdjC6d2GC6aZci7gDopapXp7o9xp0lKtOuicjlXl5NNP6wMSrTLvk49mV8YGNUpt3xeezL+MBO/YwxgWc9KmNM4FmiMsYEniUqg4jcLCI/DI/dICISZb+bROQ0ESmIeK1YRL4aZf+JIjLm6O8j40V535Ui0m5XszSfZlf9DDgrn/5SRG4TkV8Cq0QkH2ct+HeBjjjLoJwEvA9sCie1d4E1wAQRmQtMwVnu+R6cWn9ZOBVqDhORS4Fy4LfAABH5HrAXZ6XUHjhrg9f7+tOatGM9KgOQISLfBZYAe8LziipwZm93BfqE13DfE94/C9imqg8CW3Aq0IzjSBGCYTi3p7RU7CEfZ9WC4UCNqt4PDMApiVVLOy5gYKKzRGXA6VE9pKr/4MhqAgtxijWsBzaHT+8Kw9s+Bk4Ukf+Hk3TGhffvjFOIYDlwGnB6C8fqj/N7FwKKwj2qt3Hq9hUCGzz/6Uzas+kJxpjAsx6VMSbwLFEZYwLPEpUxJvAsURljAs8SlTEm8CxRGWMC738BXw1yPS0a6S0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 9\n",
    "print(\"===============================\")\n",
    "print(\"Results for Snapture - best case\")\n",
    "best = np.argmax(mean_test_scores_per_trial_snapture)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial_snapture[best], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial_snapture[best], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg_snapture[best].round(3), \"3_cross_just_snapture_grit_best_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for Snapture - worst case\")\n",
    "worst = np.argmin(mean_test_scores_per_trial_snapture)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial_snapture[worst], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial_snapture[worst], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg_snapture[worst].round(3), \"3_cross_just_snapture_grit_worst_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for Snapture - avg case\")\n",
    "worst = np.mean(mean_test_scores_per_trial_snapture)\n",
    "print(\"Accuracy:\", round(np.mean(mean_test_scores_per_trial_snapture), 5))\n",
    "print(\"F1:\", round(np.mean(mean_f1_per_trial_snapture), 5))\n",
    "print(\"===============================\")\n",
    "#temp = []\n",
    "#for conf in all_confusion:\n",
    "#    temp.append(np.array(conf))\n",
    "#print(np.mean(np.array(temp), axis=0))\n",
    "whatever = np.mean(np.array(all_conf_avg_snapture), axis=0)\n",
    "displayConfMat(whatever.round(3), \"3_cross_just_snapture_grit_avg_30_70_5_9_2021_bigger.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snapture all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence level\n",
    "class CNNLSTMALL(Module):        \n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=500, \n",
    "            hidden_size=64,\n",
    "            hidden_size_snapture=200,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            num_units=64,\n",
    "            num_units_snapture=1714,\n",
    "            num_classes=9,\n",
    "            snapture=False\n",
    "    ):\n",
    "        super(CNNLSTMALL, self).__init__()\n",
    "        #cnn for static part\n",
    "        if snapture:\n",
    "            #cnn for frames\n",
    "            self.snap = Snap().to('cuda:0')\n",
    "            self.snap.double()\n",
    "    \n",
    "        #cnn for frames\n",
    "        self.cnn = CNN().to('cuda:0')\n",
    "        self.cnn.double()\n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.num_units=num_units\n",
    "        self.num_classes=num_classes\n",
    "        self.snapture=snapture\n",
    "        \n",
    "        self.rnn = LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first).to('cuda:0')\n",
    "        \n",
    "        weights_init(self.rnn)\n",
    "       \n",
    "        self.rnn.double()\n",
    "        self.linear = Linear(num_units, num_classes).to('cuda:0')\n",
    "        weights_init(self.linear)\n",
    "        self.linear.double()\n",
    "        if snapture:\n",
    "            self.act3 = Tanh()\n",
    "            self.linear2 = Linear(num_units_snapture,num_classes)\n",
    "            self.linear2.double()\n",
    "            self.linear2.to('cuda:0')\n",
    "\n",
    "    def forward(self, x, gesture_peak):\n",
    "        x = x.contiguous()\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        \n",
    "        r_out_check_activation = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        if self.snapture:\n",
    "            use_snapture=np.ones((9), dtype=bool)\n",
    "                \n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        if self.snapture:        \n",
    "            for iitem, item in enumerate(use_snapture):\n",
    "                if True:\n",
    "                    gesture_peak_maps = self.snap(gesture_peak)\n",
    "                    gesture_peak_maps = torch.cat((r_out[:, -1, :], gesture_peak_maps), dim=1)\n",
    "                    r_out2[iitem] = self.linear2(gesture_peak_maps[iitem])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMALL(\n",
      "  (snap): Snap(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=1650, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (rnn): LSTM(500, 64, num_layers=2, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=9, bias=True)\n",
      "  (act3): Tanh()\n",
      "  (linear2): Linear(in_features=1714, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = CNNLSTMALL(snapture=True)\n",
    "model.to('cuda:0')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_grit_snapture_all(num_trials, cv_split=None):\n",
    "    num_epochs = 40\n",
    "    \n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    all_confusion = []\n",
    "    all_lost = []\n",
    "    all_val_lost=[]\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        trial_scores = []\n",
    "        trial_times = []\n",
    "        trial_predictions = []\n",
    "        trial_ground_truth = []\n",
    "        trial_conf = []\n",
    "        trial_lost = []\n",
    "        trial_val_lost=[]\n",
    "        all_y = np.array([y for x, y, z in iter(gritdataset)])\n",
    "        cv_folds = StratifiedKFold(n_splits=cv_split, shuffle=True, random_state=i)\n",
    "        for cv_fold, (train_indices, test_indices) in enumerate(cv_folds.split(gritdataset, all_y)):\n",
    "            train_dataset = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "            test_dataset = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "            train_loader = DataLoader(gritdataset, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=train_dataset)\n",
    "            test_loader = DataLoader(gritdataset, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=test_dataset)\n",
    "\n",
    "            # defining the model\n",
    "            model = CNNLSTMALL(snapture=True)\n",
    "            model.to('cuda:0')\n",
    "            optimizer = Adam(model.parameters(), lr=0.001)\n",
    "            # defining the loss function\n",
    "            criterion = CrossEntropyLoss()\n",
    "            #print(model)\n",
    "            \n",
    "            start = time.process_time() \n",
    "            loss_list, val_losses = train_model(model, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "            trial_times.append(time.process_time() - start)\n",
    "            acc, preds, labels, confusion_matrix = test_model(model, test_loader, device)\n",
    "            trial_conf.append(confusion_matrix)\n",
    "            trial_predictions.append(preds)\n",
    "            trial_ground_truth.append(labels)\n",
    "            trial_scores.append(acc) #whole_sequence\n",
    "            trial_lost.append(loss_list)\n",
    "            trial_val_lost.append(val_losses)\n",
    "        all_lost.append(loss_list)\n",
    "        all_val_lost.append(val_losses)\n",
    "        test_scores.append(trial_scores)\n",
    "        run_times.append(trial_times)\n",
    "        pred_history.append(trial_predictions)\n",
    "        true_history.append(trial_ground_truth)\n",
    "        all_confusion.append(trial_conf)\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(1.6581, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6340273073908078\n",
      "Epoch :  3 \t loss : tensor(0.8325, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8680665114535807\n",
      "Epoch :  5 \t loss : tensor(0.5369, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5947992796816682\n",
      "Epoch :  7 \t loss : tensor(0.3877, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.592069314683933\n",
      "Epoch :  9 \t loss : tensor(0.4417, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6110918691021416\n",
      "Epoch :  11 \t loss : tensor(0.2449, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5423549710728589\n",
      "Epoch :  13 \t loss : tensor(0.2190, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3957500718737237\n",
      "Epoch :  15 \t loss : tensor(0.2035, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3820951452974994\n",
      "Epoch :  17 \t loss : tensor(0.1504, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4987197466745483\n",
      "Epoch :  19 \t loss : tensor(0.1609, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3171986391346797\n",
      "Epoch :  21 \t loss : tensor(0.2027, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8067092649855022\n",
      "Epoch :  23 \t loss : tensor(0.2801, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35680786448444923\n",
      "Epoch :  25 \t loss : tensor(0.1203, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39829422976963275\n",
      "Epoch :  27 \t loss : tensor(0.0599, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29329570342212863\n",
      "Epoch :  29 \t loss : tensor(0.0574, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.45227315395731793\n",
      "Epoch :  31 \t loss : tensor(0.0338, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4014890036845024\n",
      "Epoch :  33 \t loss : tensor(0.0473, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44532038809596064\n",
      "Epoch :  35 \t loss : tensor(0.0331, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43599500923110907\n",
      "Epoch :  37 \t loss : tensor(0.0629, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5277467952604602\n",
      "Epoch :  39 \t loss : tensor(0.0107, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5778111875284578\n",
      "Test Accuracy of the model: 90.61\n",
      "Epoch :  1 \t loss : tensor(1.7610, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.8052470989374079\n",
      "Epoch :  3 \t loss : tensor(1.0109, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9528102840655782\n",
      "Epoch :  5 \t loss : tensor(0.7352, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6760249321371026\n",
      "Epoch :  7 \t loss : tensor(0.4979, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4174267265237832\n",
      "Epoch :  9 \t loss : tensor(0.3293, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5089871638516887\n",
      "Epoch :  11 \t loss : tensor(0.2799, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37180272654546204\n",
      "Epoch :  13 \t loss : tensor(0.2759, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31366730554406325\n",
      "Epoch :  15 \t loss : tensor(0.1887, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.361155562354659\n",
      "Epoch :  17 \t loss : tensor(0.1916, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32372107585844484\n",
      "Epoch :  19 \t loss : tensor(0.1384, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3792159659671473\n",
      "Epoch :  21 \t loss : tensor(0.1265, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32908911713595784\n",
      "Epoch :  23 \t loss : tensor(0.1352, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33627425968363905\n",
      "Epoch :  25 \t loss : tensor(0.0600, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4289122322274268\n",
      "Epoch :  27 \t loss : tensor(0.4293, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6097133673160441\n",
      "Epoch :  29 \t loss : tensor(0.1631, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46042736257487665\n",
      "Epoch :  31 \t loss : tensor(0.0904, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.28978220665634935\n",
      "Epoch :  33 \t loss : tensor(0.1123, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32432827824256844\n",
      "Epoch :  35 \t loss : tensor(0.0489, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37694748061933475\n",
      "Epoch :  37 \t loss : tensor(0.0380, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5019209741805222\n",
      "Epoch :  39 \t loss : tensor(0.0659, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.18288562583473522\n",
      "Test Accuracy of the model: 96.13\n",
      "Epoch :  1 \t loss : tensor(1.7576, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6314151649935937\n",
      "Epoch :  3 \t loss : tensor(0.7978, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7648737808450553\n",
      "Epoch :  5 \t loss : tensor(0.5997, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5877278203979562\n",
      "Epoch :  7 \t loss : tensor(0.4926, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6063463057379129\n",
      "Epoch :  9 \t loss : tensor(0.2910, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4586815564447981\n",
      "Epoch :  11 \t loss : tensor(0.3311, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44788982200756955\n",
      "Epoch :  13 \t loss : tensor(0.2547, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4723332504680052\n",
      "Epoch :  15 \t loss : tensor(0.1677, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41445748772725005\n",
      "Epoch :  17 \t loss : tensor(0.1524, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4118609601432011\n",
      "Epoch :  19 \t loss : tensor(0.0761, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39549368037053756\n",
      "Epoch :  21 \t loss : tensor(0.0559, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5246229305415672\n",
      "Epoch :  23 \t loss : tensor(0.0439, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4059314781632375\n",
      "Epoch :  25 \t loss : tensor(0.0404, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43390406612914373\n",
      "Epoch :  27 \t loss : tensor(0.0969, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.599396403727318\n",
      "Epoch :  29 \t loss : tensor(0.2186, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35360858655239674\n",
      "Epoch :  31 \t loss : tensor(0.1687, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46121128914265447\n",
      "Epoch :  33 \t loss : tensor(0.0979, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34615049263195435\n",
      "Epoch :  35 \t loss : tensor(0.1537, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3739514728755995\n",
      "Epoch :  37 \t loss : tensor(0.1018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31197271225497647\n",
      "Epoch :  39 \t loss : tensor(0.0882, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32418089057951155\n",
      "Test Accuracy of the model: 89.50\n",
      "Epoch :  1 \t loss : tensor(1.6961, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7236929203908542\n",
      "Epoch :  3 \t loss : tensor(0.7951, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8280328438077441\n",
      "Epoch :  5 \t loss : tensor(0.7250, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6877768629174196\n",
      "Epoch :  7 \t loss : tensor(0.5148, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48043770204642683\n",
      "Epoch :  9 \t loss : tensor(0.3647, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4133224016227312\n",
      "Epoch :  11 \t loss : tensor(0.2962, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5366650834760992\n",
      "Epoch :  13 \t loss : tensor(0.2897, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4761124258686113\n",
      "Epoch :  15 \t loss : tensor(0.1962, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5122878189868608\n",
      "Epoch :  17 \t loss : tensor(0.1501, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5574521790572119\n",
      "Epoch :  19 \t loss : tensor(0.1450, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46269080313123956\n",
      "Epoch :  21 \t loss : tensor(0.0966, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5401773233267975\n",
      "Epoch :  23 \t loss : tensor(0.0853, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5720664561186576\n",
      "Epoch :  25 \t loss : tensor(0.0918, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.476883065579466\n",
      "Epoch :  27 \t loss : tensor(0.0935, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40843642772426403\n",
      "Epoch :  29 \t loss : tensor(0.1441, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6155794672330647\n",
      "Epoch :  31 \t loss : tensor(0.0932, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5346140233653528\n",
      "Epoch :  33 \t loss : tensor(0.0674, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41571719690597303\n",
      "Epoch :  35 \t loss : tensor(0.1264, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5274384674296976\n",
      "Epoch :  37 \t loss : tensor(0.0426, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3210837738099583\n",
      "Epoch :  39 \t loss : tensor(0.0449, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2630012014295466\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.7669, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6917283022893213\n",
      "Epoch :  3 \t loss : tensor(0.8562, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9650656021406906\n",
      "Epoch :  5 \t loss : tensor(0.6559, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.705732187298865\n",
      "Epoch :  7 \t loss : tensor(0.4180, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6565550453100157\n",
      "Epoch :  9 \t loss : tensor(0.4959, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6864224049823827\n",
      "Epoch :  11 \t loss : tensor(0.3830, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.580674146017992\n",
      "Epoch :  13 \t loss : tensor(0.2592, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49577791097286866\n",
      "Epoch :  15 \t loss : tensor(0.2320, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6966981770466404\n",
      "Epoch :  17 \t loss : tensor(0.1478, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7023915099548947\n",
      "Epoch :  19 \t loss : tensor(0.1542, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6545449243707799\n",
      "Epoch :  21 \t loss : tensor(0.0788, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.737406408383216\n",
      "Epoch :  23 \t loss : tensor(0.0730, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.768390684949881\n",
      "Epoch :  25 \t loss : tensor(0.0618, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6476455366581113\n",
      "Epoch :  27 \t loss : tensor(0.0446, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9795029761900258\n",
      "Epoch :  29 \t loss : tensor(0.6105, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7616542525976878\n",
      "Epoch :  31 \t loss : tensor(0.0932, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6279885992911508\n",
      "Epoch :  33 \t loss : tensor(0.1605, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4553430464392405\n",
      "Epoch :  35 \t loss : tensor(0.1080, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35330140605265276\n",
      "Epoch :  37 \t loss : tensor(0.0290, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5666697271413731\n",
      "Epoch :  39 \t loss : tensor(0.0993, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43176849923908794\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.8747, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6488954732082084\n",
      "Epoch :  3 \t loss : tensor(0.9113, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8406267876135577\n",
      "Epoch :  5 \t loss : tensor(0.7061, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5206555516949782\n",
      "Epoch :  7 \t loss : tensor(0.4735, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4613622107804782\n",
      "Epoch :  9 \t loss : tensor(0.4326, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40516491577540154\n",
      "Epoch :  11 \t loss : tensor(0.3232, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3391974563451053\n",
      "Epoch :  13 \t loss : tensor(0.2032, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2849222077045617\n",
      "Epoch :  15 \t loss : tensor(0.1876, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21956416710706192\n",
      "Epoch :  17 \t loss : tensor(0.2678, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.278920724336758\n",
      "Epoch :  19 \t loss : tensor(0.1454, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2963602365878126\n",
      "Epoch :  21 \t loss : tensor(0.1247, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26571859380604756\n",
      "Epoch :  23 \t loss : tensor(0.1349, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19483449654444693\n",
      "Epoch :  25 \t loss : tensor(0.0794, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.15295159256819038\n",
      "Epoch :  27 \t loss : tensor(0.0243, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.19701931810275355\n",
      "Epoch :  29 \t loss : tensor(0.0520, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.18671584641310268\n",
      "Epoch :  31 \t loss : tensor(0.0212, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.23709273353870086\n",
      "Epoch :  33 \t loss : tensor(0.0133, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24177768845639047\n",
      "Epoch :  35 \t loss : tensor(0.0169, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24577407725624603\n",
      "Epoch :  37 \t loss : tensor(0.0577, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29493774786485955\n",
      "Epoch :  39 \t loss : tensor(0.0210, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3633949745033826\n",
      "Test Accuracy of the model: 94.48\n",
      "Epoch :  1 \t loss : tensor(1.7243, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6059767682222326\n",
      "Epoch :  3 \t loss : tensor(0.9907, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8329559081275347\n",
      "Epoch :  5 \t loss : tensor(0.7480, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5793820511076185\n",
      "Epoch :  7 \t loss : tensor(0.5591, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5309520046612461\n",
      "Epoch :  9 \t loss : tensor(0.3517, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44683002418044543\n",
      "Epoch :  11 \t loss : tensor(0.2893, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33403581946246846\n",
      "Epoch :  13 \t loss : tensor(0.2400, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3972841760362991\n",
      "Epoch :  15 \t loss : tensor(0.1845, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40677670372125796\n",
      "Epoch :  17 \t loss : tensor(0.1266, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4530008396329552\n",
      "Epoch :  19 \t loss : tensor(0.0804, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46216159981897437\n",
      "Epoch :  21 \t loss : tensor(0.0797, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5155617007529117\n",
      "Epoch :  23 \t loss : tensor(0.1593, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4703824111698391\n",
      "Epoch :  25 \t loss : tensor(0.2346, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29238619001480304\n",
      "Epoch :  27 \t loss : tensor(0.1140, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3348860563608274\n",
      "Epoch :  29 \t loss : tensor(0.1050, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5233782191871987\n",
      "Epoch :  31 \t loss : tensor(0.1317, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3647006154713133\n",
      "Epoch :  33 \t loss : tensor(0.2307, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5397871213983333\n",
      "Epoch :  35 \t loss : tensor(0.0623, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2599445433325276\n",
      "Epoch :  37 \t loss : tensor(0.0466, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2782457094671635\n",
      "Epoch :  39 \t loss : tensor(0.0232, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.37358554580543507\n",
      "Test Accuracy of the model: 92.82\n",
      "Epoch :  1 \t loss : tensor(1.6161, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7877223454581312\n",
      "Epoch :  3 \t loss : tensor(0.8943, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8705674417236494\n",
      "Epoch :  5 \t loss : tensor(0.5506, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5503027256286012\n",
      "Epoch :  7 \t loss : tensor(0.4809, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4300781351929664\n",
      "Epoch :  9 \t loss : tensor(0.3624, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.376024191347965\n",
      "Epoch :  11 \t loss : tensor(0.2968, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33948708811778316\n",
      "Epoch :  13 \t loss : tensor(0.1830, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.382505960655076\n",
      "Epoch :  15 \t loss : tensor(0.1461, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.39894217941184085\n",
      "Epoch :  17 \t loss : tensor(0.1082, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6517467600646574\n",
      "Epoch :  19 \t loss : tensor(0.5487, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5924271389806804\n",
      "Epoch :  21 \t loss : tensor(0.1300, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4200268509401477\n",
      "Epoch :  23 \t loss : tensor(0.1170, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36961397469358226\n",
      "Epoch :  25 \t loss : tensor(0.1014, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4686045541856865\n",
      "Epoch :  27 \t loss : tensor(0.1379, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3915823784823726\n",
      "Epoch :  29 \t loss : tensor(0.0993, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3395800619772176\n",
      "Epoch :  31 \t loss : tensor(0.0765, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.27604620292446486\n",
      "Epoch :  33 \t loss : tensor(0.0375, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32979833834497074\n",
      "Epoch :  35 \t loss : tensor(0.0319, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4510465323177139\n",
      "Epoch :  37 \t loss : tensor(0.0208, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46201078108626836\n",
      "Epoch :  39 \t loss : tensor(0.0090, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34868164078863545\n",
      "Test Accuracy of the model: 93.37\n",
      "Epoch :  1 \t loss : tensor(1.7546, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6633452596484906\n",
      "Epoch :  3 \t loss : tensor(0.8844, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8411023619115335\n",
      "Epoch :  5 \t loss : tensor(0.5548, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6769099839858441\n",
      "Epoch :  7 \t loss : tensor(0.5028, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4953905479600104\n",
      "Epoch :  9 \t loss : tensor(0.3883, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4453635372352835\n",
      "Epoch :  11 \t loss : tensor(0.2993, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41751678960150446\n",
      "Epoch :  13 \t loss : tensor(0.1992, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36320529856995165\n",
      "Epoch :  15 \t loss : tensor(0.1485, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36282856752320497\n",
      "Epoch :  17 \t loss : tensor(0.1547, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5673826873970306\n",
      "Epoch :  19 \t loss : tensor(0.1981, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47557837114101736\n",
      "Epoch :  21 \t loss : tensor(0.1841, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4773902589464412\n",
      "Epoch :  23 \t loss : tensor(0.1817, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2556789704199114\n",
      "Epoch :  25 \t loss : tensor(0.0825, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4686977686440145\n",
      "Epoch :  27 \t loss : tensor(0.1076, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35418190747792183\n",
      "Epoch :  29 \t loss : tensor(0.0654, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3506032691754375\n",
      "Epoch :  31 \t loss : tensor(0.0652, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3733694529151919\n",
      "Epoch :  33 \t loss : tensor(0.1412, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5180624028256814\n",
      "Epoch :  35 \t loss : tensor(0.0317, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4991764566752808\n",
      "Epoch :  37 \t loss : tensor(0.0756, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4129470099473516\n",
      "Epoch :  39 \t loss : tensor(0.0283, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24792826503837925\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.7656, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.5902099596763009\n",
      "Epoch :  3 \t loss : tensor(0.8774, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8998103244128494\n",
      "Epoch :  5 \t loss : tensor(0.6638, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.589735958909832\n",
      "Epoch :  7 \t loss : tensor(0.4458, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6883002755938138\n",
      "Epoch :  9 \t loss : tensor(0.4287, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4921147077544347\n",
      "Epoch :  11 \t loss : tensor(0.3024, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4620356692451537\n",
      "Epoch :  13 \t loss : tensor(0.2586, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3588092858202289\n",
      "Epoch :  15 \t loss : tensor(0.2130, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2923326421255629\n",
      "Epoch :  17 \t loss : tensor(0.1922, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31701238382949676\n",
      "Epoch :  19 \t loss : tensor(0.1124, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.267764344360716\n",
      "Epoch :  21 \t loss : tensor(0.0787, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.24182430624764548\n",
      "Epoch :  23 \t loss : tensor(0.0963, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.247617793087091\n",
      "Epoch :  25 \t loss : tensor(0.0351, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32678387589626645\n",
      "Epoch :  27 \t loss : tensor(0.1071, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31029736198338126\n",
      "Epoch :  29 \t loss : tensor(0.0245, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.344157874117286\n",
      "Epoch :  31 \t loss : tensor(0.0220, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3458016496308824\n",
      "Epoch :  33 \t loss : tensor(0.2170, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7100896897740582\n",
      "Epoch :  35 \t loss : tensor(0.1956, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5630902929990501\n",
      "Epoch :  37 \t loss : tensor(0.0856, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41048327069569285\n",
      "Epoch :  39 \t loss : tensor(0.0545, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42964629453105685\n",
      "Test Accuracy of the model: 91.16\n",
      "Epoch :  1 \t loss : tensor(1.6024, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.7026358148168008\n",
      "Epoch :  3 \t loss : tensor(0.7868, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9973939966293246\n",
      "Epoch :  5 \t loss : tensor(0.6583, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.63729988802479\n",
      "Epoch :  7 \t loss : tensor(0.4300, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5055993545147781\n",
      "Epoch :  9 \t loss : tensor(0.3906, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3841733693706875\n",
      "Epoch :  11 \t loss : tensor(0.2955, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3794501174520979\n",
      "Epoch :  13 \t loss : tensor(0.2665, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3775216653587305\n",
      "Epoch :  15 \t loss : tensor(0.2114, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.43755770223146245\n",
      "Epoch :  17 \t loss : tensor(0.1464, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36750014583830865\n",
      "Epoch :  19 \t loss : tensor(0.1884, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42523223888082734\n",
      "Epoch :  21 \t loss : tensor(0.1632, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3621721018565005\n",
      "Epoch :  23 \t loss : tensor(0.1041, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32652927932812764\n",
      "Epoch :  25 \t loss : tensor(0.0624, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2878696395608544\n",
      "Epoch :  27 \t loss : tensor(0.1452, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.31267221572545534\n",
      "Epoch :  29 \t loss : tensor(0.1187, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.25466227933118785\n",
      "Epoch :  31 \t loss : tensor(0.3788, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.400291501899074\n",
      "Epoch :  33 \t loss : tensor(0.0892, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.359435153512725\n",
      "Epoch :  35 \t loss : tensor(0.0959, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3966742649891186\n",
      "Epoch :  37 \t loss : tensor(0.0604, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4293424584346972\n",
      "Epoch :  39 \t loss : tensor(0.0393, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3837872974094307\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.6535, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.5509999651786028\n",
      "Epoch :  3 \t loss : tensor(0.8211, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8116803991411943\n",
      "Epoch :  5 \t loss : tensor(0.4754, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5013989460280943\n",
      "Epoch :  7 \t loss : tensor(0.3341, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.42228698508946144\n",
      "Epoch :  9 \t loss : tensor(0.2798, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41497860370909584\n",
      "Epoch :  11 \t loss : tensor(0.2106, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3935803576314862\n",
      "Epoch :  13 \t loss : tensor(0.2927, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3701530862780836\n",
      "Epoch :  15 \t loss : tensor(0.3624, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4322337966311644\n",
      "Epoch :  17 \t loss : tensor(0.2337, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6560426965359337\n",
      "Epoch :  19 \t loss : tensor(0.2105, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.26224619130434856\n",
      "Epoch :  21 \t loss : tensor(0.1289, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3215853640320007\n",
      "Epoch :  23 \t loss : tensor(0.0680, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.22841887055004392\n",
      "Epoch :  25 \t loss : tensor(0.0504, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29829133407137975\n",
      "Epoch :  27 \t loss : tensor(0.0595, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3637046575275609\n",
      "Epoch :  29 \t loss : tensor(0.0426, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2853035508954633\n",
      "Epoch :  31 \t loss : tensor(0.0916, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.29983730888350774\n",
      "Epoch :  33 \t loss : tensor(0.0550, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40691465727929765\n",
      "Epoch :  35 \t loss : tensor(0.0269, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3991725044021222\n",
      "Epoch :  37 \t loss : tensor(0.0297, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.21799459513457378\n",
      "Epoch :  39 \t loss : tensor(0.2088, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3998111201963294\n",
      "Test Accuracy of the model: 90.06\n",
      "Epoch :  1 \t loss : tensor(1.6266, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.5182203893963664\n",
      "Epoch :  3 \t loss : tensor(0.7694, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8354331478838205\n",
      "Epoch :  5 \t loss : tensor(0.4461, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6402348382818772\n",
      "Epoch :  7 \t loss : tensor(0.3743, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.46762731881362357\n",
      "Epoch :  9 \t loss : tensor(0.2762, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3622375364074511\n",
      "Epoch :  11 \t loss : tensor(0.2977, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.44985445645902306\n",
      "Epoch :  13 \t loss : tensor(0.1805, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.36713524789236507\n",
      "Epoch :  15 \t loss : tensor(0.1726, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.48637388605005666\n",
      "Epoch :  17 \t loss : tensor(0.1915, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4511494474204285\n",
      "Epoch :  19 \t loss : tensor(0.0698, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.35146281893916004\n",
      "Epoch :  21 \t loss : tensor(0.0643, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41663863462286765\n",
      "Epoch :  23 \t loss : tensor(0.0794, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4624370089518457\n",
      "Epoch :  25 \t loss : tensor(0.1243, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4410152184616258\n",
      "Epoch :  27 \t loss : tensor(0.1268, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3287082242798133\n",
      "Epoch :  29 \t loss : tensor(0.1021, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3418154737192655\n",
      "Epoch :  31 \t loss : tensor(0.0989, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4158490741929956\n",
      "Epoch :  33 \t loss : tensor(0.0342, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3964370118576655\n",
      "Epoch :  35 \t loss : tensor(0.1542, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.40188765425572054\n",
      "Epoch :  37 \t loss : tensor(0.1134, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4662656699102569\n",
      "Epoch :  39 \t loss : tensor(0.0825, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3358823407622893\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.5867, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.5576035380506894\n",
      "Epoch :  3 \t loss : tensor(0.9051, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8883005702617103\n",
      "Epoch :  5 \t loss : tensor(0.6453, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7015190217207667\n",
      "Epoch :  7 \t loss : tensor(0.4922, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6831431984032\n",
      "Epoch :  9 \t loss : tensor(0.4054, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6150984849089348\n",
      "Epoch :  11 \t loss : tensor(0.2273, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.49811356034612336\n",
      "Epoch :  13 \t loss : tensor(0.2410, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6170192398920474\n",
      "Epoch :  15 \t loss : tensor(0.2367, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.6233560372391965\n",
      "Epoch :  17 \t loss : tensor(0.1392, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5239135243788168\n",
      "Epoch :  19 \t loss : tensor(0.0954, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5966041914694352\n",
      "Epoch :  21 \t loss : tensor(0.3434, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.7099853845609232\n",
      "Epoch :  23 \t loss : tensor(0.1981, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5294026246030024\n",
      "Epoch :  25 \t loss : tensor(0.0765, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5518720207865929\n",
      "Epoch :  27 \t loss : tensor(0.0509, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4986701531443916\n",
      "Epoch :  29 \t loss : tensor(0.0308, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5792160321919665\n",
      "Epoch :  31 \t loss : tensor(0.0577, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4972576279859619\n",
      "Epoch :  33 \t loss : tensor(0.1830, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.8423383112712265\n",
      "Epoch :  35 \t loss : tensor(0.0978, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4359607771936685\n",
      "Epoch :  37 \t loss : tensor(0.0508, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4875468043207518\n",
      "Epoch :  39 \t loss : tensor(0.0979, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4384092849174143\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.7834, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 1.6733853070574511\n",
      "Epoch :  3 \t loss : tensor(0.9167, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.9475095584925954\n",
      "Epoch :  5 \t loss : tensor(0.5477, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5887161121307535\n",
      "Epoch :  7 \t loss : tensor(0.3825, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.47697051951767855\n",
      "Epoch :  9 \t loss : tensor(0.3432, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.34044223934631934\n",
      "Epoch :  11 \t loss : tensor(0.2795, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.41575801022158765\n",
      "Epoch :  13 \t loss : tensor(0.1369, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33253713130273876\n",
      "Epoch :  15 \t loss : tensor(0.1475, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.33734958452306846\n",
      "Epoch :  17 \t loss : tensor(0.1327, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3925806191369859\n",
      "Epoch :  19 \t loss : tensor(0.2674, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.5908182671145884\n",
      "Epoch :  21 \t loss : tensor(0.2039, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32784359408207187\n",
      "Epoch :  23 \t loss : tensor(0.0833, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3142150189158392\n",
      "Epoch :  25 \t loss : tensor(0.0834, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.2883514803080961\n",
      "Epoch :  27 \t loss : tensor(0.0830, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3520580535250722\n",
      "Epoch :  29 \t loss : tensor(0.0554, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.32390808853690095\n",
      "Epoch :  31 \t loss : tensor(0.0950, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.25168493810996584\n",
      "Epoch :  33 \t loss : tensor(0.0437, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.1832562480305527\n",
      "Epoch :  35 \t loss : tensor(0.0306, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.4561522749853697\n",
      "Epoch :  37 \t loss : tensor(0.0789, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.25554650261016304\n",
      "Epoch :  39 \t loss : tensor(0.0257, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0.3886134183886382\n",
      "Test Accuracy of the model: 92.82\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 3\n",
    "run_times_snapture_all, test_scores_snapture_all, pred_history_snapture_all, true_history_snapture_all, loss_list_snapture_all, val_losses_snapture_all, acc_snapture_all, conf_mat_snapture_all = cnnlstm_grit_snapture_all(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90607735 0.96132597 0.89502762]\n",
      " [0.93922652 0.91712707 0.94475138]\n",
      " [0.9281768  0.93370166 0.91712707]\n",
      " [0.91160221 0.93922652 0.90055249]\n",
      " [0.91712707 0.91712707 0.9281768 ]]\n",
      "[0.92081031 0.93370166 0.92633517 0.91712707 0.92081031]\n",
      "0.9237569060773481\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "test_scores_snapture_all = np.asarray(test_scores_snapture_all)\n",
    "print(test_scores_snapture_all)\n",
    "#mean test results for each trial\n",
    "mean_test_scores_per_trial_snapture_all = np.mean(test_scores_snapture_all, axis=1)\n",
    "print(mean_test_scores_per_trial_snapture_all)\n",
    "print(np.mean(mean_test_scores_per_trial_snapture_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169.30920296 170.90191075 172.71278692]\n",
      " [170.25981565 172.87135509 170.59197668]\n",
      " [169.18108894 169.48801424 169.34528114]\n",
      " [170.89027843 170.22560685 168.995147  ]\n",
      " [170.383844   169.06244089 165.95567311]]\n",
      "[170.97463354 171.24104914 169.33812811 170.03701076 168.46731934]\n",
      "170.01162817793337\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "run_times_snapture_all = np.asarray(run_times_snapture_all)\n",
    "print(run_times_snapture_all)\n",
    "#mean test results for each trial\n",
    "mean_run_times_per_trial_snapture_all = np.mean(run_times_snapture_all, axis=1)\n",
    "print(mean_run_times_per_trial_snapture_all)\n",
    "print(np.mean(mean_run_times_per_trial_snapture_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1_snapture_all = []\n",
    "for one_trial_confs_snapture_all in conf_mat_snapture_all:\n",
    "    one_trial_f1_snapture_all = []\n",
    "    for one_trial_conf_snapture_all in one_trial_confs_snapture_all:\n",
    "        recall = np.diag(one_trial_conf_snapture_all.numpy()) / np.sum(one_trial_conf_snapture_all.numpy(), axis = 1)\n",
    "        precision = np.diag(one_trial_conf_snapture_all.numpy()) / np.sum(one_trial_conf_snapture_all.numpy(), axis = 0)\n",
    "        recall = np.mean(recall)\n",
    "        precision = np.mean(precision)\n",
    "        one_trial_f1_snapture_all.append(2 * (precision * recall) / (precision + recall))\n",
    "    all_f1_snapture_all.append(one_trial_f1_snapture_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.91027636 0.9627349  0.90082734]\n",
      " [0.94143471 0.91953683 0.94637736]\n",
      " [0.92870976 0.93672542 0.91742068]\n",
      " [0.91957617 0.94077872 0.90316891]\n",
      " [0.91940736 0.92014403 0.93287834]]\n",
      "[0.92461287 0.93578297 0.92761862 0.9211746  0.92414325]\n",
      "0.9266664607485675\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "all_f1_snapture_all = np.asarray(all_f1_snapture_all)\n",
    "print(all_f1_snapture_all)\n",
    "#mean test results for each trial\n",
    "mean_f1_per_trial_snapture_all = np.mean(all_f1_snapture_all, axis=1)\n",
    "print(mean_f1_per_trial_snapture_all)\n",
    "print(np.mean(mean_f1_per_trial_snapture_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 16.,  3.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 19.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 4.,  0.,  0.,  2., 14.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  0.,  0.,  0., 18.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1., 18.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  2., 15.,  0.,  0.,  2.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 19.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 16.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  4., 16.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  2., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  0.,  0.,  0., 18.,  0.,  0.,  1.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 17.,  3.],\n",
      "        [ 0.,  2.,  0.,  0.,  0.,  0.,  0.,  0., 18.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 18.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 19.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  0., 18.,  0.,  1.,  0.],\n",
      "        [ 0.,  2.,  0.,  0.,  0.,  0., 18.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 14.,  2.,  1.,  1.,  0.,  0.,  1.],\n",
      "        [ 2.,  0.,  0., 17.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  2., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 1.,  0.,  1.,  0.,  0.,  0.,  0., 18.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., 19.]])\n",
      "tensor([[15.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1., 15.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 20.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 20.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 16.,  1.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 19.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., 19.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 18.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 19.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 16.,  2.,  1.,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  1., 17.,  0.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 18.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[17.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., 17.,  0.,  0.,  0.,  1.,  1.,  0.,  0.],\n",
      "        [ 0.,  1., 15.,  2.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., 19.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  2., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 20.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 17.,  0.,  0.,  0.,  3.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 14.,  3.,  0.,  1.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., 20.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  3., 17.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  2.,  0., 18.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 18.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 18.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  1.,  0., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 18.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0., 19.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 16.,  1.,  1.,  0.,  1.,  0.,  0.],\n",
      "        [ 2.,  0.,  3., 15.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  1., 17.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  4.,  0.,  0.,  0., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 19.]])\n",
      "tensor([[16.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 19.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 15.,  4.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., 16.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 15.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 19.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  0.,  2., 16.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 20.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 17.,  3.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 18.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  5., 14.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.,  0., 20.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  1., 19.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 19.]])\n"
     ]
    }
   ],
   "source": [
    "for one_trial_confs_snapture_all in conf_mat_snapture_all:\n",
    "    for one_trial_conf_snapture_all in one_trial_confs_snapture_all:\n",
    "        print(one_trial_conf_snapture_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334  1.         15.666667    1.6666666   0.          0.6666667\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.          0.          1.6666666  18.          0.33333334  0.\n",
      "   0.          0.          0.6666667 ]\n",
      " [ 1.6666666   0.          0.          1.3333334  17.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          1.3333334   0.          0.          0.         19.\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.\n",
      "  20.333334    0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         19.          1.        ]\n",
      " [ 0.          1.          0.          0.          0.          0.\n",
      "   0.          0.33333334 18.666666  ]]\n",
      "\n",
      "\n",
      "[[17.666666    0.          0.          0.          1.3333334   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.6666667   0.33333334 15.666667    1.6666666   0.6666667   0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.6666667   0.          0.         18.666666    0.6666667   0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.33333334  0.          0.          0.33333334 19.333334    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.33333334  0.33333334  0.6666667  19.\n",
      "   0.          0.33333334  0.33333334]\n",
      " [ 0.          0.6666667   0.          0.          0.          0.\n",
      "  20.          0.          0.        ]\n",
      " [ 0.33333334  0.          0.33333334  0.          0.          0.\n",
      "   0.         19.          0.33333334]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "   0.          0.         19.666666  ]]\n",
      "\n",
      "\n",
      "[[18.333334    0.          0.          0.          0.6666667   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334 19.          0.          0.          0.          0.33333334\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.          0.6666667  16.666666    1.3333334   1.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          1.         18.          0.6666667   0.33333334\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 0.33333334  0.          0.33333334  1.         18.333334    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.6666667   0.          0.          0.33333334 18.666666\n",
      "   0.33333334  0.33333334  0.6666667 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  20.          0.          0.6666667 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         19.333334    0.6666667 ]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "   0.          0.33333334 19.333334  ]]\n",
      "\n",
      "\n",
      "[[19.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.          0.          0.          0.          1.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.33333334  0.33333334 16.          1.3333334   0.6666667   0.33333334\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 0.6666667   0.          1.3333334  17.666666    0.33333334  0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.6666667   0.          0.33333334  1.3333334  17.333334    0.\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.          1.3333334   0.          0.6666667   0.         18.666666\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "  19.666666    0.          0.6666667 ]\n",
      " [ 0.          0.          0.          0.          0.33333334  0.\n",
      "   0.         19.333334    0.33333334]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.\n",
      "   0.33333334  0.         19.333334  ]]\n",
      "\n",
      "\n",
      "[[17.666666    0.          0.          0.          1.3333334   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.666666    0.          0.33333334  0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.6666667   0.         16.          2.3333333   0.33333334  0.33333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          2.6666667  16.333334    1.          0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.6666667   0.          0.33333334  0.6666667  18.          0.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.          0.33333334  0.          0.         20.333334\n",
      "   0.          0.33333334  0.        ]\n",
      " [ 0.33333334  0.          0.          0.          0.          0.33333334\n",
      "  20.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         19.          1.        ]\n",
      " [ 0.33333334  0.          0.          0.          0.          0.\n",
      "   0.          0.         19.666666  ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_conf_avg_snapture_all = []\n",
    "for one_trial_confs_snapture_all in conf_mat_snapture_all:\n",
    "    temp = []\n",
    "    for one_trial_conf_snapture_all in one_trial_confs_snapture_all:\n",
    "        temp.append(np.array(one_trial_conf_snapture_all))\n",
    "    all_conf_avg_snapture_all.append(np.mean(np.array(temp), axis=0))\n",
    "    print(np.mean(np.array(temp), axis=0))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def displayConfMat(confusion_matrix, save_file_name):\n",
    "    #confusion_matrix = confusion_matrix.numpy()\n",
    "    #print(confusion_matrix)\n",
    "    font = {'size'   : 5}\n",
    "    plt.rc('font', **font)\n",
    "    figure(num=None, figsize=(1080, 1080), dpi=300, facecolor='w', edgecolor='k')\n",
    "    plt_conf = ConfusionMatrixDisplay(confusion_matrix=np.array(confusion_matrix),\n",
    "                                  display_labels=np.array(classes))\n",
    "    plt_conf.plot(xticks_rotation='vertical', cmap='Blues',values_format='.5g')\n",
    "    plt.gcf().subplots_adjust(bottom=0.19)\n",
    "    plt_conf.figure_.savefig(save_file_name, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Results for Snapture - best case\n",
      "Accuracy: 0.9337\n",
      "F1: 0.93578\n",
      "===============================\n",
      "===============================\n",
      "Results for Snapture - worst case\n",
      "Accuracy: 0.91713\n",
      "F1: 0.92117\n",
      "===============================\n",
      "===============================\n",
      "Results for Snapture - avg case\n",
      "Accuracy: 0.92376\n",
      "F1: 0.92667\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD9CAYAAAALdjoMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAg1UlEQVR4nO3de5xVdb3/8ddnhvtlAEEFQRluSoAoMAiIF7yVltrFyspSD/Xr19G8dD2WpXlSj2ZZaaZZeVLzeOmUl9QyFREBRUdAQVREEAVUQBARAbl8zh9rDWyRvfae2WvtvRb7/Xw89gP2zJ7PXuwZPvNd3/1d37e5OyIiaVZT6QMQESlEjUpEUk+NSkRST41KRFJPjUpEUq9VpQ+gVNa6g1vbLrHXHbFf79hripTTzJlPr3T33UupUVvX133z+sjH+PoVD7j7saU8TyHZb1Rtu9B2/9NirzvtsctirylSTu1b2+JSa/jmDbQd/IXIx2yYdXWPUp+nkMw3KhFJkAFmlT4KNSoRKcAqP5WtRiUi0TSiEpF0M6iprfRBqFGJSARDp34iknamUz8RyQCd+olIull1n/qZ2XnunndVpZn1A8a4+22lPM/4A/sz8VNjuPaOqRw+aiAb3t/MNbc/VkrJD5g2cwFPPruQ3XfrzJdPHKe6qptI3aRr55WSdVRlb5VmdriZnQe0MbOzzOx8MxtlZj8K/z7BzC4B9gEON7MPXctiZl83s0Yza/RN70U+37TZC5nz0jLGDq/n2r9MZVDfkq4o+JCn5iziW6d/lJWr31Vd1U2sbtK1I1lN9K0MKjGm6wwsBy4C/gC0Bo4CfgG0CR9zH7AYeNTdl+5YwN2vd/cGd2+w1h2KetK/PvQMp504hrqO7WL4J2yX1C8b1VXdctaOeFaorY2+lUElTv0GAauArcDXgC3Aw8B3gPfDx2wF3gLGm9k0d3+tpU82bEAvxuxfzzvrNlBjxgPTny/t6HfQMKwfv7rxQXbv1kl1VTexuknXzislyxMs63um13Tq5UlclLxaFyVLxrVvbU+7e0MpNWrqenvb0WdGPmbDpPNLfp5C9K6fiETQynQRyYIUnPqpUYlIfqaV6SKSBRpRiUi6aY5KRLIgBad+lR/TiUh6Na2jKmFleni1yW1m1mBm55rZHTmf+0n4sX2jamR+RDViv96JBDF0O/S82GuC1mc1Wbdxc2K1O7bN/I91ihR16tfDzBpz7l/v7tc33XH3yWY21t0bzWwZsDHnsW8BdYWeQN9REYlWeNS0shkLPk8Cbm+64+5Xm1kNcB5wab4vUqMSkWglzlGZ2QEEl8MdB+zp7svNbBDBSGpfYCgwNaqGGpWI5Gel70fl7s8AJ4R3/xF+7KXw/tPF1FCjEpFIVlP599zUqEQkr2DfvMovT1CjEpH8LLxVmBqViEQwanTqJyJpp1M/EUm9qm5UZnacu/8j/Lt5uNVooXSalkoiwSPJhJuspaQkVXf6zJf4893T+e1F8e7imrXXIenaeaVkjqoSKTRfMrOzgIacxJkGM/uxmX0sfEwHM7vCzC42sw/FxuSm0KxYuaKo500iwSPJhJuspaQkVffgkYMYMvBDQUQly9rrkHTtfCyco4q6lUMlZsn2c/ergU3h/fuAI4Ar3f2B8GPDgFpgBUFs1gfkptDs3qO45pDk6DWJhJuspaSk4OygWbL4OlTqNTazyFs5VOLU70Uz+yZBTBYEiTP/Ar5tZk+GH5tLkE6zGWhxAk2uJBI8kky4yVpKSlJ15720lMY5i5j0+DyOHDcktrpZex2Srh0lDXNUmU+hGTWqwafNaCz8wGbS7gnJ0u4JyYsjhaZVj/7e9fi81woD8NaNX1QKjYhUjlG+07soalQiEkmNSkTSr/J9So1KRCIYuoRGRNJPp34ikmqaTBeR9DOwGjWqkjmwecvW2OuumBy9dqSl+vy/2xKpO++qkxKp27ZVMvMTSdWFZH4eAFrVJnPMSR1vXDSiEpHUU6MSkfQrsU+Z2QTgG8BtQAMwyd0nhZ/7DDAAWODud+arUfn3HUUktcyK2j2hR9NuJuHt67k13H0yMBtYR7AZQe7V+4Pc/QqC2Ky8NKISkUhFnPoVFUDq7g8CD5rZj4D7mz5czDGoUYlIpFLnqHICSH9M0JjW5ASQLjCz7wEvRtVQoxKRSKUuT9ghgHRHCiAVkRKZ3vUTkZQLAkgrfRRqVCISyaip1pXpO0uaMbN6YCxQX0oKzfRZC3hqziJ6dOvEKSeM46Hp85gzfwlDBvSia13HbSkew/fbm0dmPE+rVrX8/5MnVKzuzozbdw++fPgAfnL7LE45bACLV7zLnTMWN7vO47MWcMvd0/nNT04F4O8Pz+L5hcs49tD9eevtdTz30hIG9+9Fl7oOPD1nET26deazxx1UVN3GOYvovltnvnT8WJ6Y/TKNc19h1NC+DNhnD26+ezr1vXtwaMO+3PXQTCY9/jx/vHQiHTu0jay7K3zv4kiKKefxFiMNp36VWkdVa2ZnmtmdZnaRmV2Y+0kz28fMLjGzS20nr1JuCs3KFR9MoWmcs4hzTj2Gt8KkjoZh9Sx7czVt27SmYVg9q95ehzsMGbgXW7c6GzZu2rH8TiVVd2cen7+cea+9zfENe7N2/Sa2tnC76HEjBjJk4F7b7o8Y2pc3V6yhdetWjBzal9eXv03bNq0ZOaSeVWvWFfc+MdA49xXOynktHpg6l3ZtW1NTY9wzaTZ1HdtRY8Ye3ev4+skTGDFkn4JNCnaN710cSTHlPN6CLDj1i7qVQ6UalQPXEiwCW0owssv9SR4H3AwsAfb40BfnpND02P2DKTQ79rWudR247LufY/Gyt6itreHCsz7J+g3vA3DWV46mc4fikmOSqhulVW0Nk+a+zqCedSXXAujTczfOP+NEXl78Jl06d+A/zz2JV8Pj/+G/n7jt+AvZ8bV4d90Gvva5w5j85Its3ryFI8cNYf7iNwF49sXXGLZvnxbVzeL3Lo7/uJX4Wct7LEBNjUXeyqFSc1Rb3X2rmTnQhyBpZmPO56cTLLk3YHlzCo8aVs9VNz1It7qOPPPCq8yZv5RXlq6kYWg990yaxfMvv06fnt2YPOMFGucuon27NhWtuzMf6dOVhoE9mDLvDU4aW8/WrS0bUc1bsJTGuYu4+qYHOeljDdz/6DMse3M1Jxw1ktvufYJXl73FiCF9uW/ybF5c+Dq999ytqLqjhvbl6psfoltdB5554TUmjBnMdbc+wt49uzFuxEDuuP/JbZutPTx9HmecclRxdXeB710cSTHlPN5ipGGOKvMpNCNHNfiU6U8WfmBK1H/jjkTqZm33hNoU/PA3V9Z2T+jcrrbkdJj2e+3rA796TeRj5l78UaXQiEjlBMsTKv9LRY1KRCJU8fIEEckOjahEJN3KuAQhihqViOSlOSoRyQTNUYlI6qVgQJX9RuUOW1q4IDJK29a1sdeE5NY79f3CdYnUXfG3MxOpm6Sk1jtVJW3zIiJpZ1qeICJZkIIBlRqViETTqZ+IpJvWUYlI2gXbvFT+zQk1KhGJVOqIKicp+XfAwcBad78q/NxPgLeB+919fr4alW+VIpJqZhZ5o8ikZHd/BLgM6J7z6beAjoWOQSMqEckriHSPJyk59H3g90133P1qM6sBzgMuzfdFalQiEimGU7+mpORzgP7AODObTZCUvC8wFJgaVWOXa1RJJaTkE0fqSNxpMeOH9WbiscO4+q5ZjP1ILw4a3IuJV/wTgHFD9mL0fj1pfPENFix7m9M+NpRX3ljDo88s4TOHDuLoUX057bJ/sG5D/sAApcUkWzdtKTQ1JXaqOJKSUzVHZWY/MrN/M7MLw7+fn+dx21Jo3lq5QwpNQgkp+cSROhJ3Wsy0uUuZs2glsxcs565pC5g6Z8m2zx13UD/Wv7+Zre58cvxA3ln3Plu3Osvffo/r/v4MM+e/GdmkQGkxSddNVQoN1Z1Ck88W4E8EKTW/AHa6a31uCk33HtEpNHElpOSTxDcqrrQYgBPGDeCex1/edr9z+zb8/r5nOWLEPrSureGhmYvZd+8g1GF4/915duHKgjWVFpNs3VSl0Fiwv33UrRzSdurn7u5mtgX4DlD8/8hQUgkp+cSROhJ3Wsyw+h6MGdyLo0f2ZY+uHVi5Zj39e3Whc4c2PDzrVc448UBeW76WaXOX8sUjB2+7qPuYUX25+s6ZBY9XaTHJ1k1bCk0aVqZnPoVmxMgGnzxtRux1k9o94Z31yQzTtXvCdlnbPSHNKTRd+n7ED/nhjZGPuf8bY5RCIyKVY0BtCkZUalQikt/2RZ0VpUYlIpFS0KfUqEQkP6P0dVRxUKMSkUja4VNEUq2cizqjqFGJSCSd+lWhDm2SWZ+1+LZvJFJ395P/kEjd1f/79cIPaqGk1iUltT4r7eu+Ut2ozGyP3Pvuvjz5wxGRNAkm0yt9FNEjquPgA9e/3pTwsYhI2qRkHVXeMae730hwUXB3oPCVqiKyS8rC7gmdCLYK3a8MxyIiKWOkY/eEQo3qdaABWFLgcSKyiypiz/TEFXrXrxXBiKrlO8uJSKZVfoaq8Iiqi7tfRLC3sYhUmdRvnGdm/wEMNLNLgR5lORoRSZ00vOuXt1G5++VmtjdwIOkY/YlIBcQYQHoBcAqw2t1/FX7uMIJQ0uXufkO+GoXmqM4lWJpQA9xT2uGWRxZTaJJKHYk93WZoLyZ+dAhX3/MMYwf35KD99mTilQ8DcPSIvRnerzvzFq9i9bsbGTO4J8vffo9nF63kyAP3ZtPmrfzu/rnNfm3S/PomdbyVqJ2PYcWsTO9hZo0596939+ub7rj7ZDMbC3wCuBj4ds5jx7n7ZeEZXF6F5qgagRUUkWRaCjM7L65aWUyhSSp1JPZ0m+deD9JtXl7JXdMXMnXusm2fe2r+m/Tu3okNm7bw1EvL6d65HWbGvFdXU2NG+xZeOpTm1zep461E7bws2D0h6kYYQJpzu75Q2RxF7YWet1GZ2UHAy8Ac4O5mPHHRzOxsM7sQOMrMDjCzy83sZ2bW0czuNbNvm9nwnXxd3risLKbQlCt1JNZ0m7H9uOeJRdvur1n3Pt/74zTq96xj61bnwj/PoEPbYMB+1d3PsLaFe8Vn6fUNnqukQ61Y7Sg1BW6FNAWQAluB84ENZtbVzI4FnghHU5GX6EWd+rXP+XtSCRDPA6OBR4F2wCME//ZhBA3yBuA04NncLwo79vUQhDvkfi6LKTRJpY7Enm7TdzfGDO7J0SNWsUeX9qx8ZwP9e9bRuUMb9q/vTr+edTTOX86JY/sxZJ/deG3lu0wY3pvR++7J+vc3t+i1SfPrm9TxVqJ2Pkbpk+kRAaT/DP+cUvA4KplCY2afAwYBxwDnEEy0GXAR8BfgKeBv4T90p7KWQpPUlf3vvb8lkbp9v/LfidTV7gnJa9/aSk6H2XPgMD/lyv+NfMwvP/mRyqbQmFl3YB/gNXeP/Xo/d/9L+NdLwz+3jZzMbIq7Xxb3c4pI8YLr+Sr/pn+hd/3OIZhM3wxcm/zhbKcmJZIOad/mBYJ5o3qCRiUiVabpouRKK9SofgkMBF4qw7GISAqlYWau0DF8lGCy+5IyHIuIpFAa9qMqNKJ6gWD0N6gMxyIiKWNW1Mr0xBVqVMOA+cCvy3AsIpJCaViVEbUy/WyCifRNBEsURKTKNCUlR93KIWpEdTQwDTicYGX6k2U5omaqsWQWZ77Twks+Cqlr3zqRum1bJbNwN6mFmd2OvDCRugArHkyudjVKwZlfZKN62N11yidSzQxqU9CpohrVNWU7ChFJpdTn+rm7FnmKSLoblYgIZONaPxGpYkG4Q6WPQo1KRArIwoJPEaliqZ9MFxEBS/3yhF1KHAkecae6JHm8WUrjGX9APRNPaOD3dz3J+APq6dK5HRdc9y8ABtfvzscPHsxzC9/k0ZkLOfsLh7Dy7XXc+sBsTjt+FOMPqOeC6/7FomWr8tZXCk3LBVsRl+WpIsU+TZYvUcbyvHWws8eb2W/NbKKZ9YzruOJI8Ig71SXJ481SGs+0Z15hzstvsHHTZnp07cj6DduvCjj+kCGsfW8jAEc0DKDGjC1btrJ+4yau++sTvPDK8sgmBUqhKYkFp35Rt3JIYj6/f5gsg5mdZ2anm9m3gB+Y2blmdsWOX2BmvczsMjO7JEys2B3YGzjZzD50fUxuCs2KHVJo8knit0KcqS47iiH08QP3s5DGM7BPdy7+48Ns2rx9z/PdurTn5vtnMnJwb2pra5jx3Ku0adOK7l060KNrR5avXlfEsSmFphSlXutnZkeG//d/H96/Mrxf9EAkiVO/hTl/b2oytxPsbfUg8JWdfM2hwDsE1xSuAp4G3gD+6e4fSi3ITaEZNaqhqIFLHAkecae6JHm8WUrjGdZ/T8YM24cb723krJPHU1tr1HVsx+ghfbh78nOc8dlxrHl3A4/NWsS5XzyEGjNWr13P6cc38Pcp8wrWVwpNyxW5w2ehANJJZvYesCD80FtAJ4L4rOKOI+4UGjM7BWgLbAHGAU8QxOIcG/55eu5+6OGp340EqcyLCSKyzg2/7mDg8p01qyajRjX4tBmN+T7dYlm7KHnjpmRSaJJK48niRcnVmEJT/5HhfsFN90Y+5qsH9S34PGZ2AfBf7r4pvN8FONXdry7mOGIfUbn7LTl3b8z5+5/CPz8Q2pDTtHIjnZs+NjnOYxOR5jFKnx8Kp29qgIPM7FWCaPeBwC2RX5ijIu/6mdkEgr2uNrj7bZU4BhEpgpW+4DM8I/pJzoeua26NijQqd59ciecVkeZp2jiv0qpmHZWItEzl25QalYgUkIIBlRqViORnuoRGRLJA+1GJSOpVvk2pUeWV1MLMpCS1MDMpSSbF7D727ETqrn7qN4nUTWqxbhwsA+EOIiI69ROR9Kt8m1KjEpECUjCgUqMSkfwMzVGJSOoZloKTPzUqEYmUggGVGpWI5KflCSKSCSnoU9XTqJJK8FDd+OrGnRYzfuQgJp50CNfe+giHj96PDe9v4ppbJgEwuH9PPn7YcJ5bsIxHn3qRs79yNCtXr+XWe2dw2mfGM37EIC646k4WLVlZ9teh3AlChaRhjqrkvVWbmzrT9DkzOy6qVr66LZVUgofqxlc37rSYaTNfYs78JYw9cADX3voIg+r33Pa54484kLXrNgBwxJjB1NQYW7Z6kG5z62ReWPh6s5sUxPQ6lDlBKEpTAOmukELTrNQZM/s5QcDDAWbW2cyuMLNfmtlYYLiZXWhmR4d//8TOnjAtKTSqG2/dpNJi/vpAI6d9ejx1Hdtv+9huXTpy8z2PM3Jo3yDd5pmFtGndiu5dO9GjWyeWr1rbwn9Di75shxrlTRAqpNQUmjjEcerX3NSZVe5+UzhiGkoQ+ND0EzcPuBT4HvCsu9+3syesVAqN6iZbN+60mGGDejPmgP688+56amqMBx6bQ12n9ozev567H5rFGV88gjVr1/NY43zOPe2YIN3mnXWc/unx/H3S7Bb9G2J5HcqcIFRIGk79Sk6haUnqjLtfFjaq3wAXAe0J0meOzvncWuB1d/9b1PMnlUIjydq8peikpGbTRcmBrh1alZxCM3jYgX793yZFPubw/bqX/DyFlDyiamnqTNOfZjYbGA284O6NuZ8TkQqL4fTOzM4lmGaa4u6NZvZVYE/gUXefVkyNsgSVmdmEcO7qCzt+zt1vdvez3b2MOdUiUiwrcCMMIM25fX2HEquANmwfGPVw90uBQ4o9hrIsT1DqjEg2FZlCszLq1M/dbwIwsx8RTA01e76patZRiUjLlDqVHr573wBsNrO9gZVm9gNgSrE11KhEJFKpG+eF797nvoN/Q3NrqFGJSCRdQiMiqZeCPqVGJSL5GdozXTIgqYWZrWqTWxmT1MLMbuO+nUjdFVN/nkjdWJhO/UQkA1LQp9SoRKSAFHQqNSoRiVC+HRKiqFGJSF45l8lUlBqViERLQadSoxKRSDr1E5HUq3ybUqMSkSgpmaSqmkaV5vSVrNWNOy0m6eONu+74EQOY+OmDufb2KRzeMChIt7n1UQDGDK9nzP79WL56Lc++uJQjx+zHps1buH/KXE6YMJxOHdrysxv+FVm/nK9vIUVu85K4smycl09LEmxaKs3pK1mrG3daTNLHG3fdabNeZs5LSxl7QD+uvX0Kg/puT7d5au5iunftiGHMe/l1asxo37Y1r72xmtXvvEfnjoUDKcr5+hajiI3zElfRRkUzE2yaKIWmsnWTSovZ+XOVdKiJ1v3rg7M47VNjqeu0/d+3datz4TX30qFdawCuuuWRbbFct97/FK8sLRzBVc7Xtygp6FSVPvVrboINoBSaSteNOy0m6eONu+6wgXsxZng/3nl3AzVmPDD1Oeo6tWP0sHo6tm/DkAG9eO2N1UwYvS+j969n/Yb3GTlkH44YvS8d2hd+Lcr5+hZjl0ihKenJm5lgszNKoUlWFi9KTkrWLkru3K625HSYYQeM9L89MDXyMfv16pj+FJpSNDfBRkQqoPIDqoqf+olIigXTUJXvVGpUIpKfQU3l+5QalYgUoEYlIulmJZ/6mdmngAOB+e7+P2Z2JfAqcJu7v1FMjey99SIiZWUWfaNAUrK73wX8Atg7/NBbQCeg6LeUNaISkbyCcIeCD4tMSjazWuD7wBUA7n6JmXUBTgWuLuY41KhEJFIM7/r9lKDXfMvMbgA+AQwEbon8qhxqVHlooWMga8cLyX3vVj9+ZSJ1k1pIGpdSLzdy9x/u8KHrmltDjUpE8tPyBBHJhsp3KjUqEcmryMn0xKlRiUikFPQpNSoRiZaGHT7VqEQkWuX7lBqViERLQZ9SoxKR/Mx06ldWpaaZlDsZJI3pK1mtm6XUnKQTblqk8n0qnRclpzGFptzJIGlMX8lq3Syl5iSdcNMSKch2qEyjChNmPmtmXzWzb5nZt81sYppTaMqdDJLm9JWs1c1iak5SCTctUcTuCYmr1KnfeqAXsAGoA5YAQ4E5pDSFptzJIGlMX8lq3Syl5iSdcNNchqVijqoiKTRmNhL4IvAGsIlgf5pxQCMpSaHRRcnZlbXvXVIXJW9o/GXJ6TAjRjb4pKkzIh+zW8dWu2YKjbvPBGbu8OHcLR/+FP6pFBqRCkvBgKp63vUTkRbQ8gQRSbtyvrMXRY1KRKKloFOpUYlIJAWQikjqaYdPEUk/NSoRSbsYAkgPAw4Glrv7DWb2GWAAsMDd7yymRuYb1cyZT69s39oWF/nwHkAS1xmobrJ1k6y9K9ftW+qTzZr59AMd2liPAg9rZ2a5q66vD68eaTLO3S8zs/8I7w9y98tz7heU+Ubl7rsX+1gza0xiBa3qJls3ydqqG83dj42jTIH7Bel6DhFJ2hPh6GmVmY0CFpjZ94AXiy2Q+RGViKSbu08BpuR86Onm1qi2EdX1hR+iuimsm2Rt1c2AiuyeICLSHNU2ohKRDFKjEpHUU6MSkdSrikZlZr3CP3tX+liKZWZFrw/blTUFfSQR+CHZsctPppvZycBhwKPAeHc/J8ba3YF9gNfcPbZVyGZ2MbAU6O3uP4qx7g+AtsBGd/+vGOvuDxwBPOLuc2KsuyfweYJ99CcW2pa6mbWPc/d/mNmn3P2umGpeTrCtNu7+szhqJlk3S6phHdUM4HVgMXB/zLXPAVYAm4FrY649iTzhFiUwgu2dvxNz3aOA/wbOJQjoiMtxQFeCPfSnx1U0XHw4PGywa+OqCywD7gDi3rQ9qbqZscuf+rn7KwSpNr2AITGXbwfUh7c4/Q4YTvzNrxXwXVpwCUMBRwHfBw4ys+/HWHc0sD/B925sXEXd/XLghvBu57jqAh2AY4CPxVgzybqZUQ0jKgiit+LPEoILgWHEOIoIf9s7weinHxDnUH+xu98YY70mJxO8Dhe7+/q4irr7mWY2nOC0J+4LfA9295/GXHM98C/i/0WQVN3MqJZGNZ6gUW3kg0v5W2yHhnIE8TWUV2KqszN9mo475rmOC4DJBI37vBjrAnwOeBdYA1wXR8HwNRhsZj8ENsf4WqwmGL0D3BRTzSTrZka1NKrY52bCbSqOcfcHzeyjhb+iaE8QNMC2BI01Nu5+SZz1diyfUN1WBFMU9XEVDL93Y8K7m8ysg7u/F0PpnkAt0Id4G0oNQUBvm5jrZka1NKp/AN8k/sn0g8xsKnAQwdC8ZO6+2MwuIPiBfw34Qxx1E3YLwXzdnxOo/WvgEOCxmOt+HngI+ATBmyHnllownPvCzE4ttdYOugLPESSLV6VqaVTHAq2BjwOzYqz7AHAmcE+MNSH4DbqE4DdzqoWnUUcSLP84KrzFWbvp9Lo/8c7XbSEYsW4AZsdR0Mx+HNZcHke9HE7ws7A65rqZUS2Nag3ByORrMdf9FHAfwWgiTsMJ3oreEnPd2IWnUW0J/jM9FHftHT9mZp939ztiKP+fBO8CXxDHaV/YVPsAC4n5lB14h2CeLs53KDNll29U4Q9QG4J3TrYA18RY/kGCkVrrGGsCPOTuv425ZpJWA88TnKIkrX2pBczsWIJfBgZMIIaR2s6aaozWECzR6Jngc6TaLr8yHbY1q+XAFnePZTIy/GEfRTC/0SquierwWGsJRlRbq3Ulcj5mdri7PxpDnS+7exJzarEzs3MIftE+5e5xTl1kRrU0qsMIhs0D3f3XMdY9092vafozrrqyXYKXpVwR1tUvgwzY5U/9YNtWqElYZWYX0oy9n6XZErl8xN2/l3s/xrkvSUBVNKqkuPutlT6GKtB0+Qgku4ao5LkvSY4alaRduS4feSXh+lKCqpijkuwys9MIm1Rcb4SEdat+65Qs0YhK0i6py1KqfuuULFGjklRL8LKUcs19SQzUqCTVErwspeq3TsmSXX7jPMmunMtSjPgvS2naOqVqN6PLEk2mS1XKuQKgj7ufUenjkWhqVFLVzOzUON9NlGRojkqqUoJzX5IAzVFJ1Ul47ksSoFM/EUk9jahEJPXUqEQk9dSoBDM738y+G87dYGaW53HnmdnBZlaX87F6M/tCnsdPMLOxO/49t16erzvdzKp2N0v5ML3rJxDsfPpzM/upmf0ceNbMOhLsBb8E6ESwDUo/4A1gUdjUlgDzgMPN7DHgLILtnn9GkPXXhiChZhsz+xLQAPwKGGhm3wTeJtgptRfB3uCbEv3XSuZoRCUAtWb2DeBJYFW4rmgMwertbsA+4R7uq8LHtwGWuvu1wGKCBJrxbA8hGE5wecrOwh46EuxacCCw0t1/AwwkiMRaTRUHGEh+alQCwYjqOnf/O9t3E3icIKzhReCV8PSue/i594G9zOzfCZrO+PDxXQiCCGYBBwOH7uS5BhD83NUAPcIR1UKC3L7uwPzY/3WSeVqeICKppxGViKSeGpWIpJ4alYiknhqViKSeGpWIpJ4alYik3v8BboNftDQj2o4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 9\n",
    "print(\"===============================\")\n",
    "print(\"Results for Snapture - best case\")\n",
    "best = np.argmax(mean_test_scores_per_trial_snapture_all)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial_snapture_all[best], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial_snapture_all[best], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg_snapture_all[best].round(3), \"3_cross_just_snapture_all_grit_best_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for Snapture - worst case\")\n",
    "worst = np.argmin(mean_test_scores_per_trial_snapture_all)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial_snapture_all[worst], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial_snapture_all[worst], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg_snapture_all[worst].round(3), \"3_cross_just_snapture_all_grit_worst_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for Snapture - avg case\")\n",
    "worst = np.mean(mean_test_scores_per_trial_snapture_all)\n",
    "print(\"Accuracy:\", round(np.mean(mean_test_scores_per_trial_snapture_all), 5))\n",
    "print(\"F1:\", round(np.mean(mean_f1_per_trial_snapture_all), 5))\n",
    "print(\"===============================\")\n",
    "#temp = []\n",
    "#for conf in all_confusion:\n",
    "#    temp.append(np.array(conf))\n",
    "#print(np.mean(np.array(temp), axis=0))\n",
    "whatever = np.mean(np.array(all_conf_avg_snapture_all), axis=0)\n",
    "displayConfMat(whatever.round(3), \"3_cross_just_snapture_all_grit_avg_30_70_5_9_2021_bigger.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pause detection\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(img1, img2):\n",
    "    err = np.sum((img1.astype(\"float\") - img2.astype(\"float\")) ** 2)\n",
    "    err /= float(img1.shape[0] * img2.shape[1])\n",
    "    return err\n",
    "\n",
    "def diffImg(t0, t1):\n",
    "    return cv2.absdiff(t0, t1)\n",
    "\n",
    "def getdiffDir(imgs):\n",
    "    #print(np.shape(imgs))\n",
    "    diff = []\n",
    "    all_ssim = []\n",
    "    all_mse = []\n",
    "    t = diffImg(imgs[0], imgs[1])\n",
    "    for i, img in enumerate(imgs[2:-1]):\n",
    "        im = diffImg(imgs[i-1], img)\n",
    "        t += im\n",
    "        all_ssim.append(1 - ssim(img.reshape(64,48), imgs[0].reshape(64,48)))\n",
    "        all_mse.append(mse(img.reshape(64,48), imgs[0].reshape(64,48)))\n",
    "        #t += im\n",
    "        #imshow(t)\n",
    "        #plt.show()\n",
    "    diff.append(t)\n",
    "    return all_ssim, all_mse, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on sequence level\n",
    "class GRITDatasetWPause(Dataset):\n",
    "    \n",
    "    def __init__(self, path='/path/to/grit_preprocessed_frames',\n",
    "                kendon_path='/path/to/grit_kendon',\n",
    "                participants=[1,2,3,4,5,6]\n",
    "                ):\n",
    "        self.x_samples = []\n",
    "        self.y_labels = []\n",
    "        self.gesture_peak = []\n",
    "        self.gesture_peak_original = []\n",
    "        self.x_lengths = []\n",
    "        self.file_names = []\n",
    "        self.use_snapture = []\n",
    "        # load all images in a directory\n",
    "        for folder in listdir(path): #abort, circle, etc.\n",
    "            if folder in classes:\n",
    "                print(folder)\n",
    "                for subfolder in listdir(os.path.join(path,folder)): #1_1, 1_2, etc.\n",
    "                    if int(subfolder[0]) in participants:\n",
    "                        print(subfolder)\n",
    "                        loaded_images = [] #has all images in folder\n",
    "                        loaded_labels = []\n",
    "                        for filename in sorted(listdir(os.path.join(path,folder,subfolder)) ,key=lambda x: int(os.path.splitext(x)[0])): #1, 2, etc.\n",
    "                            # load image\n",
    "                            img_file = Image.open(os.path.join(path, folder, subfolder, filename))\n",
    "                            img_data = np.asarray(img_file)\n",
    "                            # store loaded image  \n",
    "                            if (len(loaded_images) < 100): #100 or any large number, to put everything inside array\n",
    "                                loaded_images.append(np.transpose(img_data / 255.0))\n",
    "                                loaded_labels.append(folder)\n",
    "                            #print('> loaded %s %s' % (filename, img_data.shape))\n",
    "                        #print(np.shape(loaded_images))\n",
    "                        if (len(loaded_images) > 22): #22 here is the wanted sequence length\n",
    "                            #print(\"#frames: \", len(loaded_images)) \n",
    "                            #take first frames again\n",
    "                            loaded_images = loaded_images[:22]\n",
    "                            self.x_lengths.append(22)\n",
    "                        else:\n",
    "                            self.x_lengths.append(len(loaded_images))\n",
    "                            loaded_images = np.pad(loaded_images, ((22-len(loaded_images), 0),(0,0),(0,0)), 'constant', constant_values=[0])\n",
    "\n",
    "                        self.y_labels.append(folder)\n",
    "                        self.file_names.append(folder+subfolder)\n",
    "                        img_file_kendon = Image.open(os.path.join(kendon_path, folder, subfolder, \"3.jpg\")).convert('L')\n",
    "                        self.gesture_peak_original.append(img_file_kendon)\n",
    "                        img_data_kendon = np.asarray(img_file_kendon)\n",
    "                        self.gesture_peak.append(np.reshape(np.transpose(img_data_kendon / 255.0), (1, 64, 48)))\n",
    "                        loaded_images = np.array(loaded_images)\n",
    "                        #loaded_images = np.append(loaded_images, np.transpose(img_data_kendon / 255.0))\n",
    "                        print(np.shape(loaded_images))\n",
    "                        loaded_images = np.reshape(loaded_images, (-1, 1, 64, 48))\n",
    "                        print(np.shape(loaded_images))\n",
    "                        self.x_samples.append(loaded_images)\n",
    "                        \n",
    "\n",
    "                        kendon_stroke_index = round(len(loaded_images) / 2)\n",
    "                        seq = loaded_images[kendon_stroke_index-2:kendon_stroke_index+2]\n",
    "                        #print(len(seq))\n",
    "                        all_error = []\n",
    "\n",
    "                        all_ssim, all_mse, diff = getdiffDir(seq)\n",
    "                        if (np.mean(all_ssim) < 0.048 ):\n",
    "                            self.use_snapture.append(True)\n",
    "                        else:\n",
    "                            self.use_snapture.append(False)\n",
    "                        \n",
    "        self.y_labels = l_e.fit_transform(self.y_labels)\n",
    "    def __len__(self):\n",
    "        return len(self.x_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #return self.x_samples[idx], self.y_labels[idx]\n",
    "        return self.x_samples[idx], self.y_labels[idx], self.gesture_peak[idx], self.use_snapture[idx]#, self.gesture_peak_original[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circle\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "hello\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "warn\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "no\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "turn_right\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "stop\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "turn\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_12\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "abort\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "turn_left\n",
      "3_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_9\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_10\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "2_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_4\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_8\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_1\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_6\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "4_5\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "1_11\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_0\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "6_7\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "5_3\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n",
      "3_2\n",
      "(22, 64, 48)\n",
      "(22, 1, 64, 48)\n"
     ]
    }
   ],
   "source": [
    "gritdatasetwpause = GRITDatasetWPause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 306, True: 237})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(gritdatasetwpause.use_snapture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 22, 1, 64, 48)\n",
      "(543,)\n",
      "(543,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(gritdatasetwpause.x_samples))\n",
    "print(np.shape(gritdatasetwpause.y_labels))\n",
    "print(np.shape(gritdatasetwpause.use_snapture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_w_pause(model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels, peak, use_snapture) in enumerate(train_loader):\n",
    "            images, labels, peak, use_snapture = images.to('cuda:0'), labels.to('cuda:0'), peak.to('cuda:0'), use_snapture.to('cuda:0')\n",
    "            outputs = model(images.double(), peak.double(), use_snapture.double()).to('cuda:0') #statless\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            val_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%2 == 0:\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', loss, '\\t', 'val_loss :', val_loss)\n",
    "    return loss_list, val_losses\n",
    "    \n",
    "def test_model_w_pause(model, test_loader, device):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = torch.zeros(9, 9)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels, peak, use_snapture in test_loader:\n",
    "            data, labels, peak, use_snapture = data.to('cuda:0'), labels.to('cuda:0'), peak.to('cuda:0'), use_snapture.to('cuda:0')\n",
    "            predictions = model(data.double(), peak.double(), use_snapture.double()).to('cuda:0') #statelss\n",
    "            #print(data.size())\n",
    "            #print(np.shape(predictions))\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc_on_test = float(num_correct)/float(total)\n",
    "    print(f\"Test Accuracy of the model: {acc_on_test*100:.2f}\")\n",
    "    return acc_on_test, [], [], confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence level\n",
    "class CNNLSTMPause(Module):        \n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=500, \n",
    "            hidden_size=64,\n",
    "            hidden_size_snapture=200,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            num_units=64,\n",
    "            num_units_snapture=1714,\n",
    "            num_classes=9,\n",
    "            snapture=False\n",
    "    ):\n",
    "        super(CNNLSTMPause, self).__init__()\n",
    "        #cnn for static part\n",
    "        if snapture:\n",
    "            #cnn for frames\n",
    "            self.snap = Snap().to('cuda:0')\n",
    "            self.snap.double()\n",
    "    \n",
    "        #cnn for frames\n",
    "        self.cnn = CNN().to('cuda:0')\n",
    "        self.cnn.double()\n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.num_units=num_units\n",
    "        self.num_classes=num_classes\n",
    "        self.snapture=snapture\n",
    "        \n",
    "        self.rnn = LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first).to('cuda:0')\n",
    "        \n",
    "        weights_init(self.rnn)\n",
    "       \n",
    "        self.rnn.double()\n",
    "        self.linear = Linear(num_units, num_classes).to('cuda:0')\n",
    "        weights_init(self.linear)\n",
    "        self.linear.double()\n",
    "        if snapture:\n",
    "            self.act3 = Tanh()\n",
    "            self.linear2 = Linear(num_units_snapture,num_classes)\n",
    "            self.linear2.double()\n",
    "            self.linear2.to('cuda:0')\n",
    "\n",
    "    def forward(self, x, gesture_peak, use_snapture):\n",
    "        x = x.contiguous()\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        \n",
    "        r_out_check_activation = self.linear(r_out[:, -1, :])\n",
    "                        \n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        if self.snapture:        \n",
    "            for iitem, item in enumerate(use_snapture):\n",
    "                if item:\n",
    "                    #print(item)\n",
    "                    gesture_peak_maps = self.snap(gesture_peak)\n",
    "                    gesture_peak_maps = torch.cat((r_out[:, -1, :], gesture_peak_maps), dim=1)\n",
    "                    r_out2[iitem] = self.linear2(gesture_peak_maps[iitem])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMPause(\n",
      "  (snap): Snap(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=1650, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (rnn): LSTM(500, 64, num_layers=2, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=9, bias=True)\n",
      "  (act3): Tanh()\n",
      "  (linear2): Linear(in_features=1714, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = CNNLSTMPause(snapture=True)\n",
    "model.to('cuda:0')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_grit_w_pause(num_trials, cv_split=None):\n",
    "    num_epochs = 40\n",
    "    \n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    all_confusion = []\n",
    "    all_lost = []\n",
    "    all_val_lost=[]\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        trial_scores = []\n",
    "        trial_times = []\n",
    "        trial_predictions = []\n",
    "        trial_ground_truth = []\n",
    "        trial_conf = []\n",
    "        trial_lost = []\n",
    "        trial_val_lost=[]\n",
    "        all_y = np.array([y for x, y, z, q in iter(gritdatasetwpause)])\n",
    "        cv_folds = StratifiedKFold(n_splits=cv_split, shuffle=True, random_state=i)\n",
    "        for cv_fold, (train_indices, test_indices) in enumerate(cv_folds.split(gritdatasetwpause, all_y)):\n",
    "            train_dataset = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "            test_dataset = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "            train_loader = DataLoader(gritdatasetwpause, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=train_dataset)\n",
    "            test_loader = DataLoader(gritdatasetwpause, batch_size=64, shuffle=False, num_workers=0,  drop_last=False, sampler=test_dataset)\n",
    "\n",
    "            # defining the model\n",
    "            model = CNNLSTMPause(snapture=True)\n",
    "            model.to('cuda:0')\n",
    "            optimizer = Adam(model.parameters(), lr=0.001)\n",
    "            # defining the loss function\n",
    "            criterion = CrossEntropyLoss()\n",
    "            #print(model)\n",
    "            \n",
    "            start = time.process_time() \n",
    "            loss_list, val_losses = train_model_w_pause(model, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "            trial_times.append(time.process_time() - start)\n",
    "            acc, preds, labels, confusion_matrix = test_model_w_pause(model, test_loader, device)\n",
    "            trial_conf.append(confusion_matrix)\n",
    "            trial_predictions.append(preds)\n",
    "            trial_ground_truth.append(labels)\n",
    "            trial_scores.append(acc) #whole_sequence\n",
    "            trial_lost.append(loss_list)\n",
    "            trial_val_lost.append(val_losses)\n",
    "        all_lost.append(loss_list)\n",
    "        all_val_lost.append(val_losses)\n",
    "        test_scores.append(trial_scores)\n",
    "        run_times.append(trial_times)\n",
    "        pred_history.append(trial_predictions)\n",
    "        true_history.append(trial_ground_truth)\n",
    "        all_confusion.append(trial_conf)\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(1.7302, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.9027, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.7298, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4501, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4245, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.2710, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2288, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2169, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1430, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0419, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0666, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0187, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0198, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0410, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0163, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0187, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0156, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0053, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0053, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0052, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 90.61\n",
      "Epoch :  1 \t loss : tensor(1.7172, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.9185, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.7621, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4820, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.2543, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.1390, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.1580, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1920, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.0431, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1304, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0971, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0488, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0514, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0671, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0183, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0248, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0108, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0130, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0067, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0036, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 91.16\n",
      "Epoch :  1 \t loss : tensor(1.9573, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.0709, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.9441, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5388, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3281, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.2739, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2420, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2473, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.0588, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1053, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0453, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0476, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0299, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0195, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0128, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0255, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0148, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0092, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0057, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0081, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 91.16\n",
      "Epoch :  1 \t loss : tensor(1.6070, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.8506, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.8370, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4927, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3202, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.1720, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.4198, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1375, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1393, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0930, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0992, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0774, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0555, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0376, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0076, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0087, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0122, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0171, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0113, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0068, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.9161, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.1681, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.7590, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5674, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.5101, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3783, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2334, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2337, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.2855, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1125, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.1298, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0935, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0264, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0423, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0262, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0098, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0160, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0145, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 93.37\n",
      "Epoch :  1 \t loss : tensor(1.6793, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.1336, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4941, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4669, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3033, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2744, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1930, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.0455, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0746, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0404, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0135, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0432, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0561, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0315, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0129, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0074, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0066, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0074, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 93.37\n",
      "Epoch :  1 \t loss : tensor(1.9442, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.3057, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.5474, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4833, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.5431, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.5955, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2594, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1318, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.3107, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1063, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0753, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0608, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0357, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0243, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0255, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0136, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0159, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0084, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0055, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0112, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 92.82\n",
      "Epoch :  1 \t loss : tensor(1.8087, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.0222, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.7001, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5596, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3283, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3908, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.4174, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2368, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1265, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1300, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0990, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0529, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0675, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0500, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0308, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0265, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0240, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0148, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0051, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0114, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.7315, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.8649, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.7422, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4281, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4557, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.2517, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.1668, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.3576, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1819, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0980, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0400, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.2468, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.3673, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0844, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0848, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0362, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0416, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0102, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0166, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0061, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.7559, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.9768, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.6223, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.6336, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4161, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.5039, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2863, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1750, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1664, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0882, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0770, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0432, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0473, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0367, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0873, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.4459, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0740, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0300, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0256, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0131, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 91.71\n",
      "Epoch :  1 \t loss : tensor(1.8657, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.1348, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(1.0056, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5066, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4327, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3812, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.4281, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1989, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1674, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.2033, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.1268, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0530, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0527, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0195, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0421, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0225, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0164, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0094, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0087, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0042, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 93.92\n",
      "Epoch :  1 \t loss : tensor(1.8005, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.1007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.5711, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5564, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4420, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3358, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2050, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2416, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.3018, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1956, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0872, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.1186, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0472, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0752, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0370, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0390, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0428, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0162, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0136, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 90.06\n",
      "Epoch :  1 \t loss : tensor(1.8007, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.7602, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.6061, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4966, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.5658, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3828, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2673, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2309, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1925, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1212, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0822, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0803, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0589, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0821, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0561, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0785, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0441, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0269, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0265, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0069, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 91.16\n",
      "Epoch :  1 \t loss : tensor(1.7133, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.0122, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.6395, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5852, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3330, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.2572, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.1701, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1027, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.2307, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0767, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0468, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0613, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0225, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0267, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0142, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0126, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0099, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0044, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0079, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0082, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 92.27\n",
      "Epoch :  1 \t loss : tensor(1.7719, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.1909, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5288, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4195, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3104, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.1539, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2784, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.2338, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0800, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0612, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0544, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0313, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0170, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0142, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0120, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0128, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0095, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0078, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 93.92\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 3\n",
    "run_times_snapture_pause, test_scores_snapture_pause, pred_history_snapture_pause, true_history_snapture_pause, loss_list_snapture_pause, val_losses_snapture_pause, acc_snapture_pause, conf_mat_snapture_pause = cnnlstm_grit_w_pause(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90607735 0.91160221 0.91160221]\n",
      " [0.93922652 0.93370166 0.93370166]\n",
      " [0.9281768  0.93922652 0.91712707]\n",
      " [0.91712707 0.93922652 0.90055249]\n",
      " [0.91160221 0.92265193 0.93922652]]\n",
      "[0.90976059 0.93554328 0.9281768  0.91896869 0.92449355]\n",
      "0.9233885819521179\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "test_scores_snapture_pause = np.asarray(test_scores_snapture_pause)\n",
    "print(test_scores_snapture_pause)\n",
    "#mean test results for each trial\n",
    "mean_test_scores_per_trial_snapture_pause = np.mean(test_scores_snapture_pause, axis=1)\n",
    "print(mean_test_scores_per_trial_snapture_pause)\n",
    "print(np.mean(mean_test_scores_per_trial_snapture_pause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[125.84696346 127.06677533 125.69979103]\n",
      " [123.97868099 127.01946075 127.87316768]\n",
      " [126.53289178 124.68708516 125.74525604]\n",
      " [123.02255605 125.41449665 123.23174001]\n",
      " [123.83128014 120.66034764 126.73447311]]\n",
      "[126.20450994 126.29043647 125.65507766 123.88959757 123.74203363]\n",
      "125.15633105446656\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "run_times_snapture_pause = np.asarray(run_times_snapture_pause)\n",
    "print(run_times_snapture_pause)\n",
    "#mean test results for each trial\n",
    "mean_run_times_per_trial_snapture_pause = np.mean(run_times_snapture_pause, axis=1)\n",
    "print(mean_run_times_per_trial_snapture_pause)\n",
    "print(np.mean(mean_run_times_per_trial_snapture_pause))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1_snapture_pause = []\n",
    "for one_trial_confs_snapture_pause in conf_mat_snapture_pause:\n",
    "    one_trial_f1_snapture_pause = []\n",
    "    for one_trial_conf_snapture_pause in one_trial_confs_snapture_pause:\n",
    "        recall = np.diag(one_trial_conf_snapture_pause.numpy()) / np.sum(one_trial_conf_snapture_pause.numpy(), axis = 1)\n",
    "        precision = np.diag(one_trial_conf_snapture_pause.numpy()) / np.sum(one_trial_conf_snapture_pause.numpy(), axis = 0)\n",
    "        recall = np.mean(recall)\n",
    "        precision = np.mean(precision)\n",
    "        one_trial_f1_snapture_pause.append(2 * (precision * recall) / (precision + recall))\n",
    "    all_f1_snapture_pause.append(one_trial_f1_snapture_pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90970521 0.91373067 0.91613151]\n",
      " [0.94257292 0.93417494 0.93575637]\n",
      " [0.92976579 0.94227251 0.91942879]\n",
      " [0.92061072 0.94070177 0.90512371]\n",
      " [0.91400958 0.92476437 0.94076586]]\n",
      "[0.91318913 0.93750141 0.93048903 0.9221454  0.92651327]\n",
      "0.9259676480722746\n"
     ]
    }
   ],
   "source": [
    "#test_scores: all cv splits of all trials in format (acc_frame_by_frame, acc_whole_sequence)\n",
    "all_f1_snapture_pause = np.asarray(all_f1_snapture_pause)\n",
    "print(all_f1_snapture_pause)\n",
    "#mean test results for each trial\n",
    "mean_f1_per_trial_snapture_pause = np.mean(all_f1_snapture_pause, axis=1)\n",
    "print(mean_f1_per_trial_snapture_pause)\n",
    "print(np.mean(mean_f1_per_trial_snapture_pause))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 17.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., 18.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 3.,  0.,  1.,  0., 16.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  2.,  1.,  0., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0., 19.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  1., 13.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., 17.,  0.,  0.,  1.,  0.,  2.],\n",
      "        [ 3.,  0.,  0.,  2., 15.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[17.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 15.,  1.,  0.,  3.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., 16.,  0.,  2.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  0.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  0.,  0., 19.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 18.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 19.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  0.,  2.,  0., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 19.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[18.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 19.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 17.,  1.,  0.,  1.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1., 19.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  2.,  0., 17.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., 19.]])\n",
      "tensor([[17.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 18.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., 16.,  0.,  0.,  1.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  2., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  0.,  0., 19.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[17.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  0., 17.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 20.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  1., 18.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  3.,  0.,  0., 18.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 19.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 17.,  1.,  1.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1., 15.,  3.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  1., 18.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[16.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 19.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1., 17.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 19.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  1.,  0., 18.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 19.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[17.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 15.,  3.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 18.,  0.,  0.,  0.,  0.,  3.],\n",
      "        [ 0.,  0.,  0.,  1., 18.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., 19.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 19.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[17.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 18.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 19.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 20.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0., 17.,  0.,  3.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., 19.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 18.,  0.,  0.,  0.,  0.,  2.,  0.,  0.],\n",
      "        [ 2.,  0., 13.,  1.,  1.,  0.,  2.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., 18.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  3.,  2., 16.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[17.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., 18.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., 19.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  0.,  1., 17.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  2.,  0.,  2., 17.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 18.,  0.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 19.]])\n",
      "tensor([[17.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 17.,  1.,  0.,  0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., 19.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  2.,  0.,  0., 18.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 16.,  4.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.]])\n",
      "tensor([[19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0., 18.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  2., 15.,  1.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., 20.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., 20.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  1., 18.]])\n"
     ]
    }
   ],
   "source": [
    "for one_trial_confs_snapture_pause in conf_mat_snapture_pause:\n",
    "    for one_trial_conf_snapture_pause in one_trial_confs_snapture_pause:\n",
    "        print(one_trial_conf_snapture_pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.333334    0.          0.6666667   0.          1.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.6666667   0.6666667  15.          1.3333334   0.33333334  1.\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 0.          0.          1.6666666  17.          0.33333334  0.6666667\n",
      "   0.33333334  0.          0.6666667 ]\n",
      " [ 2.          0.          0.33333334  0.6666667  17.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  1.3333334   0.33333334  0.         19.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.33333334  0.33333334  0.          0.\n",
      "  20.          0.          0.        ]\n",
      " [ 0.          0.          0.33333334  0.          0.          0.\n",
      "   0.         19.666666    0.        ]\n",
      " [ 0.          0.          0.33333334  0.          0.          0.\n",
      "   0.          0.         19.666666  ]]\n",
      "\n",
      "\n",
      "[[18.          0.          0.33333334  0.          0.6666667   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.666666    0.          0.          0.          0.\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.33333334  0.33333334 17.666666    0.6666667   0.          0.33333334\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.          1.3333334  18.          0.          0.\n",
      "   0.33333334  0.          1.        ]\n",
      " [ 0.33333334  0.          0.6666667   1.         18.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.6666667   1.          1.          0.         18.333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  20.333334    0.          0.33333334]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         19.666666    0.33333334]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.33333334 19.666666  ]]\n",
      "\n",
      "\n",
      "[[17.333334    0.33333334  0.33333334  0.33333334  0.6666667   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.666666    0.          0.          0.          0.\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 0.6666667   0.33333334 17.          0.6666667   0.6666667   0.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.          0.          1.         18.          1.          0.\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 0.33333334  0.          0.          0.6666667  19.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  1.3333334   0.33333334  0.33333334 18.\n",
      "   0.33333334  0.          0.33333334]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  19.666666    0.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         20.          0.        ]\n",
      " [ 0.          0.          0.6666667   0.          0.          0.\n",
      "   0.          0.         19.333334  ]]\n",
      "\n",
      "\n",
      "[[17.666666    0.          0.6666667   0.          0.6666667   0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         19.333334    0.          0.          0.          0.\n",
      "   0.6666667   0.          0.        ]\n",
      " [ 1.          0.33333334 15.333333    1.6666666   0.6666667   0.\n",
      "   0.6666667   0.          0.        ]\n",
      " [ 0.          0.          0.33333334 18.333334    0.6666667   0.\n",
      "   0.33333334  0.          1.        ]\n",
      " [ 0.33333334  0.          0.          0.33333334 19.          0.33333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          1.3333334   0.6666667  18.333334\n",
      "   0.          0.33333334  0.33333334]\n",
      " [ 0.          0.33333334  0.          0.          0.          0.\n",
      "  19.          0.          1.3333334 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         20.          0.        ]\n",
      " [ 0.          0.33333334  0.          0.          0.33333334  0.\n",
      "   0.          0.         19.333334  ]]\n",
      "\n",
      "\n",
      "[[17.666666    0.          0.          0.33333334  1.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.6666667   0.33333334 17.666666    0.33333334  0.33333334  0.\n",
      "   0.          0.          0.33333334]\n",
      " [ 0.6666667   0.          1.3333334  17.666666    0.33333334  0.33333334\n",
      "   0.33333334  0.          0.        ]\n",
      " [ 1.          0.          0.          0.33333334 18.666666    0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.33333334  1.3333334   0.33333334  0.6666667  18.333334\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.33333334\n",
      "  19.666666    0.          0.6666667 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         18.666666    1.3333334 ]\n",
      " [ 0.          0.          0.          0.33333334  0.33333334  0.\n",
      "   0.          0.33333334 19.        ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_conf_avg_snapture_pause = []\n",
    "for one_trial_confs_snapture_pause in conf_mat_snapture_pause:\n",
    "    temp = []\n",
    "    for one_trial_conf_snapture_pause in one_trial_confs_snapture_pause:\n",
    "        temp.append(np.array(one_trial_conf_snapture_pause))\n",
    "    all_conf_avg_snapture_pause.append(np.mean(np.array(temp), axis=0))\n",
    "    print(np.mean(np.array(temp), axis=0))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def displayConfMat(confusion_matrix, save_file_name):\n",
    "    #confusion_matrix = confusion_matrix.numpy()\n",
    "    #print(confusion_matrix)\n",
    "    font = {'size'   : 5}\n",
    "    plt.rc('font', **font)\n",
    "    figure(num=None, figsize=(1080, 1080), dpi=300, facecolor='w', edgecolor='k')\n",
    "    plt_conf = ConfusionMatrixDisplay(confusion_matrix=np.array(confusion_matrix),\n",
    "                                  display_labels=np.array(classes))\n",
    "    plt_conf.plot(xticks_rotation='vertical', cmap='Blues',values_format='.5g')\n",
    "    plt.gcf().subplots_adjust(bottom=0.19)\n",
    "    plt_conf.figure_.savefig(save_file_name, dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Results for Pause - best case\n",
      "Accuracy: 0.93554\n",
      "F1: 0.9375\n",
      "===============================\n",
      "===============================\n",
      "Results for Pause - worst case\n",
      "Accuracy: 0.90976\n",
      "F1: 0.91319\n",
      "===============================\n",
      "===============================\n",
      "Results for Pause - avg case\n",
      "Accuracy: 0.92339\n",
      "F1: 0.92597\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 9\n",
    "print(\"===============================\")\n",
    "print(\"Results for Pause - best case\")\n",
    "best = np.argmax(mean_test_scores_per_trial_snapture_pause)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial_snapture_pause[best], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial_snapture_pause[best], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg_snapture_pause[best].round(3), \"3_cross_pause_grit_best_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for Pause - worst case\")\n",
    "worst = np.argmin(mean_test_scores_per_trial_snapture_pause)\n",
    "print(\"Accuracy:\", round(mean_test_scores_per_trial_snapture_pause[worst], 5))\n",
    "print(\"F1:\", round(mean_f1_per_trial_snapture_pause[worst], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_conf_avg_snapture_pause[worst].round(3), \"3_cross_pause_grit_worst_30_70_5_9_2021_bigger.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for Pause - avg case\")\n",
    "worst = np.mean(mean_test_scores_per_trial_snapture_pause)\n",
    "print(\"Accuracy:\", round(np.mean(mean_test_scores_per_trial_snapture_pause), 5))\n",
    "print(\"F1:\", round(np.mean(mean_f1_per_trial_snapture_pause), 5))\n",
    "print(\"===============================\")\n",
    "#temp = []\n",
    "#for conf in all_confusion:\n",
    "#    temp.append(np.array(conf))\n",
    "#print(np.mean(np.array(temp), axis=0))\n",
    "whatever = np.mean(np.array(all_conf_avg_snapture_pause), axis=0)\n",
    "displayConfMat(whatever.round(3), \"3_cross_pause_grit_avg_30_70_5_9_2021_bigger.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
