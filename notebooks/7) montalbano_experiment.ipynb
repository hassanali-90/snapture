{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pytorch\n",
    "#cnnlstm on sequence level (not frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # order devices by bus id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # only make device 0 visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from platform import python_version\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import itertools\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import image as matplotimage\n",
    "from os import listdir\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import time\n",
    "import re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "from torch.nn import Module\n",
    "from torch.nn import LSTM\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import orthogonal_\n",
    "from torch.nn.init import normal_\n",
    "from torch.nn.init import calculate_gain\n",
    "from torch.nn.init import zeros_\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, ChainDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.helper import SliceDataset\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The currently selected GPU is number: 0 , it's a  NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print(\"The currently selected GPU is number:\", torch.cuda.current_device(),\n",
    "      \", it's a \", torch.cuda.get_device_name(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  1\n",
      "Device 0 : NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPUs available: \", torch.cuda.device_count())\n",
    "for device in range(torch.cuda.device_count()):\n",
    "    print(\"Device\",device, \":\", torch.cuda.get_device_name(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['vattene', 'vieniqui', 'perfetto', 'furbo', 'fheduepalle', 'chevuoi', 'daccordo', 'seipazzo', 'combinato', 'freganiente', 'ok', 'cosatifarei', 'basta', 'prendere', 'noncenepiu', 'fame', 'tantotempo', 'buonissimo', 'messidaccordo', 'sonostufo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2 14 20 6 7 3 1 13 18 5 12 16 15 4 9 10 8 17 19 11\n",
      "2, \"sample missing,  just making the doc consistent\"\n",
      "3, 12 3 18 14 16 20 5 2 4 1 10 6 9 19 15 17 11 13 8 7\n",
      "4, 13 1 8 18 7 17 16 9 5 10 11 4 20 3 19 2 14 6 15 12\n",
      "5, 10 4 7 13 19 15 9 11 17 1 8 5 18 3 12 16 14 2 20 6\n",
      "6, 14 15 10 16 11 2 20 8 7 9 1 19 17 18 6 4 13 3 5 12\n",
      "7, 19 11 12 9 6 3 15 16 14 4 2 10 20 1 13 8 5 7 18 17\n",
      "8, 17 7 8 2 9 15 11 3 12 1 6 20 16 4 14 13 10 5 18 19\n",
      "9, 11 3 7 15 8 17 12 16 20 4 5 13 10 1 18 6 19 2 14 9\n",
      "10, 3 11 5 9 12 15 4 2 20 19 13 16 17 18 1 7 8 14 6 10\n",
      "11, 12 1 5 9 10 18 13 7 2 20 14 19 15 6 16 4 3 11 17 8\n",
      "12, 8 13 10 11 12 15 17 2 16 9 20 14 4 7 5 1 19 6 3 18\n",
      "13, 15 11 19 7 5 14 4 8 6 17 2 20 13 1 10 9 12 16 18 3\n",
      "14, 17 20 1 15 3 18 8 16 11 14 2 12 9 19 6 5 10 13 4 7\n",
      "15, 3 7 16 9 12 10 4 19 15 2 14 20 6 17 1 18 13 8 11 5\n",
      "16, 9 12 19 2 1 13 11 16 17 5 20 7 14 3 18 15 4 6 8 10\n",
      "17, 10 16 6 20 18 2 12 19 9 17 13 4 1 7 5 8 3 11 14 15\n",
      "18, 14 2 8 4 12 18 11 7 16 20 1 15 17 9 10 13 3 19 6 5\n",
      "19, 17 6 12 2 5 18 13 4 14 8 10 7 3 20 9 19 15 16 1 11\n",
      "20, 9 5 10 1 16 6 2 12 18 19 14 20 17 8 7 4 3 15 13 11\n",
      "21, 19 5 14 18 13 12 9 6 11 4 16 10 8 17 2 7 3 1 20 15\n",
      "22, 20 11 9 2 3 14 13 1 4 8 15 18 10 16 12 17 7 5 19 6\n",
      "23, 9 2 12 20 14 19 16 15 18 1 5 13 6 3 8 10 4 7 17 11\n",
      "24, 14 3 5 7 15 11 13 12 2 16 8 10 20 18 1 19 6 17 4 9\n",
      "25, 19 10 5 9 18 8 3 14 1 7 17 15 13 11 2 20 12 4 6 16\n",
      "26, 18 10 16 2 11 13 6 9 8 20 1 3 4 7 15 17 5 19 14 12\n",
      "27, 10 2 1 9 6 18 8 4 15 3 14 17 11 19 5 13 16 20 12 7\n",
      "28, 17 8 20 12 3 18 6 14 9 13 16 5 19 7 2 15 11 4 10 1\n",
      "29, 4 2 14 18 11 17 7 16 19 10 6 1 3 15 20 9 5 13 8 12\n",
      "30, 10 1 11 16 14 3 6 20 9 2 5 15 19 13 18 8 7 12 17 4\n",
      "31, 14 16 13 10 9 12 8 17 19 4 15 3 7 6 18 20 2 1 11 5\n",
      "32, 1 2 10 18 7 5 6 9 15 8 16 4 14 11 12 13 17 3 19 20\n",
      "33, 10 5 1 14 11 18 9 3 17 20 6 19 13 4 8 7 2 12 15 16\n",
      "34, 19 17 3 18 10 1 15 2 4 6 9 16 14 12 8 5 13 7 20 11\n",
      "35, 13 9 1 4 6 12 17 7 5 10 18 3 16 15 19 2 8 14 11 20\n",
      "36, 14 18 6 11 5 9 12 4 1 16 17 3 8 20 13 15 10 7 19 2\n",
      "37, 13 12 17 14 6 10 18 4 1 20 19 11 7 3 15 5 16 2 8 9\n",
      "38, 13 15 19 9 7 11 3 20 14 4 17 16 18 10 1 8 6 5 12 2\n",
      "39, 18 9 12 17 7 1 10 11 5 6 14 19 15 2 8 20 3 13 16 4\n",
      "40, 8 12 16 17 2 14 13 15 11 20 19 7 5 6 1 18 3 10 4 9\n",
      "41, 8 6 14 1 15 10 17 18 20 11 9 3 7 4 19 13 5 2 12 16\n",
      "42, 10 18 6 20 2 19 9 15 4 8 13 1 5 17 16 12 11 7 3 14\n",
      "43, 16 2 17 14 12 9 11 20 7 3 6 18 13 10 15 1 4 5 19 8\n",
      "44, 11 12 15 4 1 13 6 18 8 14 16 3 19 10 5 7 17 2 9 20\n",
      "45, 6 7 8 17 4 12 14 1 19 16 11 9 5 3 2 13 18 15 10 20\n",
      "46, 9 11 16 2 17 14 5 10 6 12 8 3 19 15 18 1 4 20 7 13\n",
      "47, 11 14 6 5 16 19 18 4 3 15 12 1 2 13 10 8 17 7 9 20\n",
      "48, 8 4 3 13 18 11 14 15 5 17 19 10 2 12 20 16 1 7 9 6\n",
      "49, 3 7 10 20 19 15 4 16 18 5 9 1 6 14 13 8 11 17 12 2\n",
      "50, 20 4 3 19 8 17 16 11 12 2 6 5 14 18 7 1 10 15 9 13\n",
      "51, 4 15 19 18 1 16 10 7 9 6 11 14 2 20 12 17 3 5 8 13\n",
      "52, 15 10 5 20 19 13 1 6 4 14 17 9 8 12 16 3 7 2 18 11\n",
      "53, 20 1 2 18 8 5 10 17 12 15 14 16 19 3 7 11 6 13 9 4\n",
      "54, 5 4 9 11 16 12 19 6 7 17 18 8 1 10 2 15 20 3 14 13\n",
      "55, 20 9 7 5 17 11 4 3 1 15 10 8 6 12 14 16 2 19 18 13\n",
      "56, 17 19 5 14 15 6 1 11 18 9 16 4 20 12 3 10 8 2 13 7\n",
      "57, 14 18 6 8 17 10 9 12 20 15 1 11 16 3 13 7 4 5 2 19\n",
      "58, 3 10 20 7 5 16 14 6 19 15 1 4 11 9 2 8 18 17 12 13\n",
      "59, 17 15 8 5 7 12 19 4 10 20 2 16 6 13 11 3 9 1 14 18\n",
      "60, 1 15 19 3 5 17 7 8 13 18 4 14 9 12 6 11 20 10 16 2\n",
      "61, 11 20 4 17 15 14 10 18 3 1 8 5 7 14 9 13 2 6 16 12\n",
      "62, 8 3 6 12 15 14 7 17 10 13 1 4 11 2 20 5 20 9 16 18\n",
      "63, 4 10 3 17 15 9 1 11 16 13 7 2 19 8 5 6 14 18 20 12\n",
      "64, 13 1 17 8 16 12 11 9 15 19 20 3 4 14 5 6 18 2 10 7\n",
      "65, 7 4 16 13 2 15 10 5 20 3 17 19 9 11 14 18 8 1 6 12\n",
      "66, 1 19 11 14 12 2 10 16 13 6 7 3 18 20 5 17 8 4 15 9\n",
      "67, 18 10 1 4 5 6 3 12 14 13 19 11 9 15 16 2 8 20 7 17\n",
      "68, 11 19 12 20 1 15 6 3 9 13 17 7 8 4 16 18 14 2 5 10\n",
      "69, 17 20 5 18 13 19 8 11 15 14 6 16 10 7 1 12 3 2 4 9\n",
      "70, 7 19 10 8 5 14 20 16 12 2 9 1 3 15 6 13 18 4 11 17\n",
      "71, 12 3 18 20 2 11 7 6 4 16 15 9 13 17 1 14 19 5 10 8\n",
      "72, 15 2 3 12 9 20 5 4 6 19 7 10 11 14 16 13 18 1 8 17\n",
      "73, 7 17 15 10 19 6 2 12 3 14 1 13 20 16 8 5 11 9 4 18\n",
      "74, 15 5 9 12 10 18 8 6 14 17 16 11 2 19 3 13 4 20 1 7\n",
      "75, 5 7 19 3 13 16 8 14 9 4 18 6 12 20 10 15 2 1 11 17\n",
      "76, 13 9 20 2 16 6 11 7 8 15 3 1 10 14 18 17 19 12 4 5\n",
      "77, 12 8 19 1 7 20 2 11 4 14 16 15 17 10 13 3 18 9 6 5\n",
      "78, 16 5 17 8 11 19 14 1 15 2 6 13 10 4 7 20 12 18 9 3\n",
      "79, 4 5 18 8 12 14 1 19 9 17 13 15 11 16 6 7 3 10 20 2\n",
      "80, 7 19 10 13 1 3 9 20 11 14 15 6 16 12 4 8 18 17 2 5\n",
      "81, 6 1 11 10 5 17 12 3 14 16 15 8 7 9 2 20 4 19 13 18\n",
      "82, 8 5 11 13 17 19 16 2 7 14 4 20 10 1 18 15 3 9 6 12\n",
      "83, 15 18 11 12 10 16 8 9 1 5 13 17 2 19 4 7 3 20 14 6\n",
      "84, 11 19 4 18 5 3 10 9 6 13 16 20 15 14 17 1 8 7 2 12\n",
      "85, 18 4 7 20 2 11 1 8 14 16 10 19 17 3 9 5 6 13 15 12\n",
      "86, 5 6 19 1 13 4 16 2 14 20 12 8 3 9 11 15 18 10 17 7\n",
      "87, 12 14 15 4 20 2 7 3 9 19 18 16 11 6 1 13 5 17 8 10\n",
      "88, 16 6 11 1 4 7 9 15 5 18 2 12 17 19 10 14 8 20 3 13\n",
      "89, 1 15 19 16 13 17 5 18 10 9 11 8 20 7 12 2 6 14 4 3\n",
      "90, 16 7 18 4 6 20 19 3 12 5 1 2 11 13 15 14 10 9 8 17\n",
      "91, 15 11 5 14 10 7 12 6 8 16 17 18 19 9 20 3 1 13 2 4\n",
      "92, 11 14 16 4 12 2 6 17 5 20 8 7 19 1 15 9 18 13 10 3\n",
      "93, 18 13 16 2 6 7 12 15 3 14 5 8 10 1 19 11 17 20 9 4\n",
      "94, 16 5 12 2 17 15 14 11 19 1 6 9 8 7 18 3 10 4 13 20\n",
      "95, 11 13 6 15 20 17 1 18 5 19 10 8 3 14 2 4 9 7 12 16\n",
      "96, 15 4 18 12 17 11 13 2 20 10 6 8 3 1 19 7 16 5 9 14\n",
      "97, 18 16 3 7 17 8 13 5 4 19 14 11 9 15 1 12 20 10 2 6\n",
      "98, 14 8 4 7 1 16 5 15 2 13 6 19 18 3 9 12 11 20 10 17\n",
      "99, 6 1 11 20 15 13 12 2 19 3 9 8 17 4 14 18 16 10 5 7\n",
      "100, \"sample missing,  just making the doc consistent\"\n",
      "101, 14 2 17 11 5 8 20 16 10 4 6 18 7 19 9 1 3 15 12 13\n",
      "102, 7 2 5 17 20 6 16 19 11 3 9 12 18 1 10 14 4 13 8\n",
      "103, 2 15 10 6 17 16 7 20 3 8 1 13 9 12 4 14 19 5 18 11\n",
      "104, 8 9 13 12 18 20 16 10 14 6 15 3 19 5 2 1 11 17 7 4\n",
      "105, 11 3 17 13 8 20 9 16 5 15 10 1 2 6 14 12 4 19 7 18\n",
      "106, 4 18 12 7 15 10 16 11 3 14 6 1 5 8 2 20 17 19 9 13\n",
      "107, 9 4 10 6 11 18 2 20 19 5 1 17 7 8 14 16 13 15 3 12\n",
      "108, 18 14 17 11 9 1 2 10 3 7 5 16 20 15 12 8 19 4 13 6\n",
      "109, 7 8 10 17 18 4 15 12 9 16 5 13 14 20 1 6 2 19 3 11\n",
      "110, 17 20 7 16 6 13 11 18 3 14 15 8 10 9 4 5 1 19 12 2\n",
      "111, 9 3 5 8 10 1 11 15 4 14 19 16 18 12 6 17 2 7 20 13\n",
      "112, 3 2 17 11 8 16 12 7 20 14 6 19 5 1 15 9 18 10 13 4\n",
      "113, 7 2 5 9 8 1 16 15 14 4 12 13 11 18 6 19 3 10 17 20\n",
      "114, 1 15 3 8 2 20 10 11 12 13 9 17 16 4 7 6 18 14 19 5\n",
      "115, 3 7 13 5 17 11 12 15 1 9 10 18 14 19 20 4 6 8 2 16\n",
      "116, 16 10 20 17 13 14 12 5 2 8 4 3 11 6 7 15 19 18 9 1\n",
      "117, 19 14 17 6 8 12 3 16 9 2 5 11 1 4 13 15 7 20 18 10\n",
      "118, 19 10 8 17 11 12 2 1 20 18 7 6 5 3 14 13 16 15 4 9\n",
      "119, 9 17 15 6 20 7 10 5 3 13 18 19 2 12 1 8 11 16 14 4\n",
      "120, 6 1 15 10 14 8 7 13 4 11 20 16 17 2 18 5 3 9 19 12\n",
      "121, 9 10 7 3 4 2 1 18 15 20 14 17 19 16 11 12 6 5 8 13\n",
      "122, 9 19 11 8 10 15 17 16 3 12 1 4 5 14 13 20 7 2 6 18\n",
      "123, 17 18 4 7 11 9 20 15 13 6 16 8 19 5 10 2 14 12 3 1\n",
      "124, 20 14 2 12 11 10 6 15 3 16 17 1 8 5 4 7 18 19 9 13\n",
      "125, 10 5 11 14 1 18 13 9 7 20 2 19 15 3 8 4 17 6 16 12\n",
      "126, 17 3 11 12 6 8 15 20 4 16 7 1 19 13 18 9 10 5 14 2\n",
      "127, 12 5 20 9 7 8 19 13 2 11 1 10 17 18 4 6 14 15 16 3\n",
      "128, 20 17 16 6 13 3 11 8 18 19 12 2 7 15 4 14 1 5 10 9\n",
      "129, 9 16 7 11 13 10 8 17 20 2 14 15 6 19 18 5 12 4 1 3\n",
      "130, 9 19 2 16 3 12 15 10 11 14 4 6 7 20 18 17 13 8 5 1\n",
      "131, 18 9 6 17 10 11 16 15 8 1 2 19 3 14 4 12 13 5 7 20\n",
      "132, 8 1 19 7 17 11 6 5 16 15 9 10 20 13 14 3 2 18 12\n",
      "133, 16 17 19 6 11 3 18 7 15 12 4 20 9 1 10 5 13 2 8 14\n",
      "134, 19 14 1 11 13 16 17 9 5 4 18 15 7 2 10 12 6 8 3 20\n",
      "135, 14 10 5 2 9 12 8 11 15 16 4 20 6 17 1 19 18 7 13 3\n",
      "136, 8 11 9 16 7 20 15 3 13 4 12 6 10 5 17 14 2 18 1 19\n",
      "137, 13 11 18 3 17 6 12 10 4 19 1 20 7 15 2 14 8 5 9 16\n",
      "138, 16 13 11 10 8 19 12 7 20 17 4 5 18 15 14 2 6 3 1 9\n",
      "139, 18 3 4 12 13 7 1 11 16 9 6 19 10 8 20 5 14 15 17 2\n",
      "140, 5 7 19 13 12 9 2 17 15 18 6 3 16 8 20 11 10 1 4 14\n",
      "141, 18 15 5 7 16 10 1 3 6 2 4 19 20 11 14 9 17 13 12\n",
      "142, 16 2 4 1 10 14 9 20 11 8 18 13 15 3 6 5 17 7 12 19\n",
      "143, 16 12 15 18 19 1 11 5 9 4 17 20 14 2 8 10 13 6 7 3\n",
      "144, 11 18 13 16 4 2 10 17 20 5 15 3 1 9 6 7 19 12 8 14\n",
      "145, 2 18 15 9 5 7 20 11 8 14 13 12 6 16 3 10 17 4 19 1\n",
      "146, 4 18 19 11 10 12 2 16 6 15 3 9 20 1 7 5 14 17 8 13\n",
      "147, 13 12 3 1 18 10 8 14 2 11 4 7 15 9 20 17 6 5 19 16\n",
      "148, 3 17 4 19 16 18 5 13 12 14 6 7 10 2 11 9 8 20 15 1\n",
      "149, 2 18 15 1 8 9 19 17 7 16 20 11 5 13 6 3 12 10 14 4\n",
      "150, 17 18 15 12 4 9 6 5 8 11 19 3 1 20 16 2 14 13 7 10\n",
      "151, 3 14 17 13 4 5 9 1 11 10 19 7 18 16 8 15 2 12 20 6\n",
      "152, 14 18 20 8 7 19 2 13 4 3 17 12 5 16 11 9 15 1 6 10\n",
      "153, 5 10 11 6 7 8 1 17 3 15 2 13 4 19 12 16 20 18 9 14\n",
      "154, 1 12 2 10 16 14 13 15 9 17 6 4 19 3 7 8 5 11 18\n",
      "155, 18 10 8 9 19 5 1 16 2 3 6 12 20 14 4 13 15 11 17 7\n",
      "156, 7 11 19 18 16 2 4 15 17 14 6 12 5 3 1 20 10 9 13 8\n",
      "157, 19 4 9 11 18 5 6 16 17 12 1 15 14 10 2 8 20 13 7 3\n",
      "158, 2 3 8 6 7 12 20 11 16 18 1 15 5 10 14 19 4 17 13 9\n",
      "159, 11 2 14 12 18 9 15 16 10 5 13 1 4 8 20 6 17 3 19 7\n",
      "160, 17 18 6 16 14 10 9 2 1 19 3 20 7 4 11 5 15 12 13 8\n",
      "161, 7 10 11 13 15 5 3 9 6 17 1 2 19 18 16 14 4 20 12 8\n",
      "162, 16 11 12 13 10 7 15 2 20 18 8 6 9 1 19 4 17 5 14\n",
      "163, 15 20 18 2 7 17 13 1 19 9 6 5 4 16 8 10 3 11 12\n",
      "164, 2 10 11 8 20 1 19 4 15 16 17 6 7 9 13 18 5 3 14 12\n",
      "165, 4 2 14 19 20 3 7 1 16 11 15 17 12 10 5 6 9 18 13 8\n",
      "166, 5 7 4 13 19 18 6 12 9 20 17 3 1 2 10 16 11 8 15 14\n",
      "167, 13 10 6 9 1 16 2 15 3 17 20 12 18 14 5 11 4 8 19 7\n",
      "168, 15 3 1 13 6 12 14 5 16 2 20 8 10 17 18 7 11 4 9 19\n",
      "169, 16 3 2 15 8 5 6 11 18 9 10 1 19 13 14 7 17 20 4 12\n",
      "170, 10 12 16 11 6 8 5 14 9 7 17 15 2 4 18 20 3 13 1 19\n",
      "171, 2 7 8 12 14 9 19 17 5 3 13 16 10 11 20 15 6 4 1 18\n",
      "172, 15 6 4 9 3 2 12 17 7 14 18 1 20 13 16 11 8 19 5 10\n",
      "173, 5 6 17 14 12 1 10 3 2 15 11 9 20 13 18 16 8 7 4 19\n",
      "174, 16 17 9 13 1 11 8 7 2 3 4 5 19 10 12 15 18 14 20 6\n",
      "175, 12 8 18 6 16 17 4 3 9 5 7 14 19 15 13 20 10 1 2 11\n",
      "176, 9 4 12 7 14 20 5 16 15 11 3 18 19 17 13 8 10 1 2 6\n",
      "177, 2 1 20 11 18 10 4 16 9 5 12 17 15 14 19 7 8 13 3 6\n",
      "178, 9 4 7 1 5 3 16 13 6 12 18 17 2 8 15 20 14 19 10 11\n",
      "179, 10 14 4 3 19 5 7 20 9 11 2 8 1 18 16 17 6 12 15 13\n",
      "180, 9 15 13 19 17 3 5 2 18 11 10 14 12 20 7 16 4 8 6 1\n",
      "181, 12 15 17 20 4 7 1 13 18 11 14 3 8 10 5 2 9 16 6 19\n",
      "182, 2 4 1 17 11 10 12 6 3 5 7 13 9 14 20 15 18 19 16 8\n",
      "183, 1 4 10 14 20 15 6 11 18 3 13 16 17 5 8 12 2 9 19\n",
      "184, 11 18 12 4 16 19 13 10 5 2 6 20 9 7 8 17 1 3 14 15\n",
      "185, 3 5 19 2 11 7 8 16 20 10 15 17 1 14 18 12 6 4 9 13\n",
      "186, 14 2 12 9 20 11 13 1 10 17 15 4 16 7 19 3 18 6 5 8\n",
      "187, 1 15 9 18 10 17 13 5 20 8 19 4 3 11 6 7 16 12 14 2\n",
      "188, 1 17 14 8 13 10 18 4 2 5 12 11 20 15 16 6 7 19 9 3\n",
      "189, 17 7 1 3 18 12 11 10 8 2 6 13 5 14 15 16 4 20 9 19\n",
      "190, 1 19 15 12 9 13 20 2 18 17 6 3 14 7 10 11 4 5 16 8\n",
      "191, 5 17 18 13 3 6 7 9 2 16 20 1 11 10 8 12 15 14 4 19\n",
      "192, 8 6 12 14 13 19 18 16 1 10 5 3 11 7 20 2 9 15 4 17\n",
      "193, 17 1 15 9 4 19 3 2 7 12 18 5 10 20 13 6 16 8 14 11\n",
      "194, 11 6 8 14 19 17 10 12 1 15 20 3 5 9 4 18 16 13 2 7\n",
      "195, 9 13 5 19 12 10 16 14 8 4 2 3 17 7 1 20 15 11 6 18\n",
      "196, 12 1 18 4 3 15 9 14 17 13 10 8 16 20 7 2 5 6 19 11\n",
      "197, 2 6 15 13 9 5 18 11 20 19 16 4 8 7 1 10 14 3 12 17\n",
      "198, 18 2 1 7 17 20 13 15 14 19 12 4 8 5 10 6 3 16 9 11\n",
      "199, 8 9 5 14 11 12 15 17 7 10 19 1 18 2 4 6 3 13 20 16\n",
      "200, 19 1 8 6 16 13 4 17 7 12 2 18 20 11 3 14 5 15 9 10\n",
      "201, 18 20 11 12 17 16 13 10 2 19 9 1 7 3 6 14 5 4 8\n",
      "202, 19 5 14 18 20 10 2 8 12 15 16 11 9 13 17 3 1 7 4\n",
      "203, 3 7 2 4 1 19 15 8 16 6 10 5 17 11 20 9 18 13 14 12\n",
      "204, 8 20 17 7 10 13 9 5 4 19 14 6 18 2 11 12 15 3 1 16\n",
      "205, 6 5 7 3 14 11 13 20 17 12 16 8 10 4 19 2 15 9 18\n",
      "206, 5 6 12 7 2 15 10 4 3 8 16 9 20 11 1 14 18 17 13 19\n",
      "207, 2 5 7 3 11 6 16 19 15 9 14 8 4 12 1 17 10 18 13 20\n",
      "208, 20 8 9 16 3 6 7 5 1 11 19 18 13 14 2 17 12 10 15 4\n",
      "209, 18 9 12 5 3 10 20 2 11 4 17 14 6 8 16 13 1 7 15 19\n",
      "210, 19 14 9 13 10 16 15 8 3 4 1 20 5 11 18 2 12 17 6 7\n",
      "211, 9 20 14 8 18 3 12 16 10 6 1 19 2 7 13 5 15 17 11 4\n",
      "212, 16 6 15 9 12 2 11 8 19 10 14 4 13 17 20 1 3 5\n",
      "213, 17 20 10 5 13 6 15 8 2 7 16 19 1 3 9 4 11 14 12 18\n",
      "214, 15 3 1 14 12 5 19 16 8 13 17 20 2 4 10 6 7 11 18 9\n",
      "215, 1 5 10 20 8 17 15 12 13 11 2 3 7 4 6 14 16 19 18 9\n",
      "216, 8 12 11 3 5 10 16 17 7 9 20 15 14 13 4 6 2 1 19 18\n",
      "217, 13 12 14 19 9 15 16 3 11 17 1 6 20 18 10 4 7 2 5 8\n",
      "218, 13 10 17 18 6 4 19 14 1 20 12 8 7 2 9 5 16 11 15 3\n",
      "219, 9 10 2 11 4 5 13 18 6 3 15 20 1 14 16 19 7 12 8 17\n",
      "220, 5 16 18 11 6 20 13 1 4 14 2 19 15 7 8 10 12 9 3 17\n",
      "221, 1 11 12 7 5 4 10 14 19 16 2 8 13 15 20 3 9 18 17\n",
      "222, 7 19 12 16 15 8 11 14 13 9 2 18 5 1 4 6 20 3 17 10\n",
      "223, 5 17 3 19 12 10 13 18 8 15 4 11 7 14 16 9 1 6 2 20\n",
      "224, 7 14 19 16 1 2 6 15 4 11 8 3 20 10 9 18 12 17 5 13\n",
      "225, 7 12 5 14 19 6 17 1 13 2 15 16 11 10 3 9 18 4 8 20\n",
      "226, 2 18 1 20 5 9 19 10 3 11 8 6 12 13 4 17 15 14 16 7\n",
      "227, 15 2 11 4 19 5 14 12 9 17 18 3 7 1 6 20 16 8 10 13\n",
      "228, 13 18 12 5 8 15 3 17 7 4 14 9 10 2 20 1 6 11 16 19\n",
      "229, 6 14 16 17 20 12 5 1 8 2 11 9 3 13 15 4 7 18 10 19\n",
      "230, 20 15 8 14 4 9 3 1 6 2 10 5 19 18 11 7 16 12 13 17\n",
      "231, 9 14 2 18 15 10 19 6 20 17 16 12 8 7 5 4 11 1 13 3\n",
      "232, 15 16 11 17 8 6 9 14 19 12 10 2 4 18 20 13 1 7 5 3\n",
      "233, 18 19 2 5 8 13 12 1 7 20 15 11 4 6 9 17 3 16 14 10\n",
      "234, 5 4 8 18 9 7 10 16 6 17 19 20 2 11 15 14 12 1 13 3\n",
      "235, 12 17 13 9 8 3 20 11 18 19 15 16 7 10 2 14 1 6 5 4\n",
      "236, 17 4 14 16 5 12 11 18 9 7 1 10 2 13 3 8 19 15 6 20\n",
      "237, 8 10 17 13 5 19 4 7 2 6 16 3 20 1 11 14 9 18 15 12\n",
      "238, 16 8 2 10 14 3 7 1 15 17 9 11 19 12 6 5 13 4 18 20\n",
      "239, 1 3 2 14 11 10 16 9 12 20 13 19 17 4 15 8 18 7 5 6\n",
      "240, 18 15 3 19 11 14 10 2 9 7 8 13 16 1 6 17 5 4 12 20\n",
      "241, 3 8 7 13 7 14 12 4 18 15 16 1 11 5 2 6 9 10 20 17\n",
      "242, 19 20 14 17 16 11 1 15 9 3 12 8 6 13 18 5 10 2 4 7\n",
      "243, 20 3 1 8 19 16 9 7 17 2 11 4 13 14 5 18 13 10 12 6\n",
      "244, 6 18 4 12 13 5 9 20 17 3 1 5 16 2 10 14 11 19 8 7\n",
      "245, 16 17 10 5 19 11 8 2 12 6 14 7 20 18 1 13 4 9 3\n",
      "246, 10 12 6 18 13 14 2 11 16 4 5 1 17 9 3 15 8 20 19 7\n",
      "247, 3 4 15 16 20 6 13 19 7 10 2 12 11 17 1 18 5 14 8 9\n",
      "248, 16 1 11 6 20 10 7 8 13 15 9 12 14 4 17 18 19 5 2\n",
      "249, 2 1 15 20 7 11 14 13 17 18 9 8 6 10 16 3 19 4 5 12\n",
      "250, 16 4 7 1 5 6 8 15 10 14 12 9 17 18 19 3 2 20 13 11\n",
      "251, 14 7 8 15 17 12 19 9 1 18 4 2 6 3 20 5 11 13 16 10\n",
      "252, 19 4 16 14 6 2 15 9 10 7 12 3 11 20 1 5 18 17 8 13\n",
      "253, 19 9 4 3 7 8 20 14 2 13 5 12 17 18 11 15 6 1 16 10\n",
      "254, 3 8 4 2 19 1 9 18 14 12 10 5 20 17 15 7 6 11 13 16\n",
      "255, 3 12 2 5 19 13 11 16 10 8 4 6 7 15 1 9 17 18 20 14\n",
      "256, 14 16 7 8 19 2 5 20 3 11 10 12 18 6 17 15 4 13 1 9\n",
      "257, 12 14 2 4 10 18 19 8 6 1 3 9 16 15 11 20 5 7 13\n",
      "258, 18 15 4 13 3 12 1 2 14 17 10 20 11 16 7 19 5 9 6 8\n",
      "259, 4 19 15 1 20 7 2 8 17 13 12 14 9 18 3 16 10 6 5 11\n",
      "260, 18 4 11 17 6 8 13 12 15 3 1 16 14 2 10 19 5 20\n",
      "261, 5 18 15 3 10 8 9 16 20 6 19 1 13 7 11 2 4 12 14\n",
      "262, 10 5 15 7 3 18 12 2 20 6 9 14 19 1 8 4 13 16 17 11\n",
      "263, 14 1 11 6 17 8 4 12 15 18 3 16 10 7 13 9 19 20 5 2\n",
      "264, 9 14 7 6 11 17 3 15 20 18 10 2 4 8 13 19 5 16 1 12\n",
      "265, 3 10 6 18 8 16 5 19 13 4 1 15 11 7 14 17 20 12 2\n",
      "266, 16 2 1 19 3 15 5 9 7 18 10 11 20 13 6 4 14 12 8 17\n",
      "267, 12 13 5 14 4 15 6 3 7 19 9 11 18 16 2 1 10 20 17 8\n",
      "268, 9 13 15 20 2 8 19 7 17 1 18 5 12 6 4 16 14 3 11 10\n",
      "269, 10 19 16 7 11 12 20 2 13 8 18 6 17 5 9 3 4 14 1 15\n",
      "270, 15 17 18 4 7 19 2 8 11 3 16 10 14 6 1 13 12 5 9 20\n",
      "271, 20 16 8 7 9 11 19 5 4 6 2 10 18 17 1 15 3 14\n",
      "272, 7 13 11 5 20 6 8 18 10 2 16 15 19 3 12 4 17 14 1\n",
      "273, 2 17 11 10 16 6 13 1 9 14 3 15 5 7 8 20 18 19 12\n",
      "274, 11 1 18 6 15 10 4 5 20 7 9 8 2 19 3 13 16 14 17\n",
      "275, 11 8 19 6 10 15 9 7 13 3 18 2 20 1 16 12 14 4 17\n",
      "276, 15 10 19 8 12 16 14 13 7 18 9 5 2 4 6 11 17 3 20\n",
      "277, 19 15 20 12 13 18 3 2 10 7 4 16 9 5 6 14 11 1 17\n",
      "278, 5 15 9 19 11 3 10 17 1 12 14 8 2 20 4 18 13 7 16\n",
      "279, 7 5 18 3 2 1 17 12 4 8 11 19 13 20 15 16 9 14 6\n",
      "280, 10 5 14 16 6 13 1 7 12 3 9 11 2 8 18 4 20 15 17 19\n",
      "281, 20 5 6 15 4 16 8 9 2 14 10 18 3 1 19 12 7 17 11\n",
      "282, 18 20 15 8 11 4 7 19 5 14 17 6 3 12 2 16 1 9 13\n",
      "283, 16 3 11 15 19 7 8 18 6 20 9 14 1 2 5 10 13 4 12 17\n",
      "284, 7 15 10 5 8 3 11 6 9 12 18 2 14 20 4 19 13 16 1\n",
      "285, 15 17 8 10 6 18 12 3 1 9 2 19 11 13 20 16 4 7 5\n",
      "286, 6 15 19 12 8 11 16 4 7 3 9 17 10 1 20 2 5 18 14 13\n",
      "287, 6 11 19 14 18 8 17 20 13 5 10 9 4 12 1 15 7 3 16\n",
      "288, 19 10 11 4 18 20 15 14 9 1 16 8 13 6 17 12 5 3 7 2\n",
      "289, 2 20 10 1 5 18 9 16 6 19 4 15 12 11 17 8 13 7 14 3\n",
      "290, 20 2 3 8 4 16 1 5 15 19 17 13 18 14 11 9 10 7 12 6\n",
      "291, 9 10 8 4 19 14 17 16 9 11 1 3 12 18 6 20 5 13 2\n",
      "292, 9 10 8 4 19 14 17 16 9 11 1 3 12 18 6 20 5 13 2\n",
      "293, 2 4 5 1 11 19 20 6 16 13 10 9 8 7 18 15 17 14 3 12\n",
      "294, 12 3 11 5 16 13 9 4 6 19 2 17 20 8 14 15 10 1 18 9\n",
      "295, 11 6 8 4 13 9 20 5 18 19 12 3 17 10 1 2 14 15 16 9\n",
      "296, 17 12 18 2 8 9 16 9 11 1 4 15 5 20 3 6 13 10 14 19\n",
      "297, 17 8 13 14 20 11 9 1 2 3 15 19 5 10 18 6 12 16 4 9\n",
      "298, 10 13 15 1 9 17 9 4 3 20 11 16 18 14 2 8 5 19 12\n",
      "299, 9 14 20 6 13 5 19 12 8 15 10 4 18 1 7 11 17 16 3 2\n",
      "300, 5 9 1 2 18 3 8 4 20 13 12 15 14 11 6 16 19 10 17\n",
      "301, 10 12 1 5 4 20 6 2 11 15 13 19 9 9 8 18 14 3 16 17\n",
      "302, 1 17 16 12 5 9 19 13 20 18 11 3 4 6 15 8 14 10 9 2\n",
      "303, 18 13 4 12 10 15 5 19 20 17 1 11 16 8 9 3 6 2 14 9\n",
      "304, 8 1 9 12 14 18 13 9 2 11 3 20 19 5 10 6 15 17 16 4\n",
      "305, 7 19 15 8 3 2 12 11 18 17 9 13 14 1 4 20 6 5 16 10\n",
      "306, 9 19 15 9 1 8 16 4 11 3 2 5 17 14 10 13 6 12 18 20\n",
      "307, 2 10 19 8 9 3 6 20 16 12 18 15 4 1 11 9 5 13 17 14\n",
      "308, 11 12 19 20 4 6 18 17 9 8 1 10 15 2 14 9 3 5 16 13\n",
      "309, 9 18 2 6 9 16 10 4 1 20 19 5 3 11 12 15 14 13 17 8\n",
      "310, 2 3 12 4 9 6 15 17 14 8 10 19 18 5 20 1 11 16 9 13\n",
      "311, 9 19 2 17 7 12 20 13 6 1 14 16 5 3 8 11 10 4 15 18\n",
      "312, 16 19 20 3 13 7 15 9 8 17 6 11 18 10 1 2 5 14 12\n",
      "313, 4 16 15 2 7 3 1 18 10 9 17 5 12 11 6 14 19 13 8 20\n",
      "314, 7 14 16 18 15 6 2 4 17 5 20 9 1 11 3 10 19 8 12 13\n",
      "315, 16 11 18 9 12 17 14 10 15 20 1 3 13 6 5 2 7 4 8 19\n",
      "316, 14 16 6 8 9 15 18 3 17 1 10 7 11 4 5 2 13 19 20\n",
      "317, 16 14 2 6 3 13 17 18 9 20 11 12 1 4 19 7 8 15 10 5\n",
      "318, 5 17 14 19 2 12 6 1 4 8 10 13 15 16 9 7 11 18 20\n",
      "319, 3 2 12 4 16 8 17 6 13 15 18 20 19 9 11 1 5 7 14\n",
      "320, 16 4 13 20 15 18 5 7 2 11 9 17 12 19 14 10 6 1 8 3\n",
      "321, 1 14 13 12 9 5 16 15 2 8 19 20 4 3 17 10 18 7 6 11\n",
      "322, 20 3 11 6 9 15 5 12 8 7 13 17 16 19 14 4 1 18 10\n",
      "323, 10 9 17 15 4 2 3 8 1 14 18 20 19 13 7 5 16 12 6 11\n",
      "324, 15 8 1 17 13 7 18 9 2 14 4 12 20 11 16 3 6 19 5\n",
      "325, 18 6 4 3 15 10 17 16 11 8 1 2 19 14 9 13 5 7 20\n",
      "326, 18 5 16 9 1 7 19 6 2 11 3 4 12 14 17 10 13 8 10 15 20\n",
      "327, 11 6 14 10 9 4 17 2 13 20 1 19 16 8 7 15 3 12 18 5\n",
      "328, 10 7 14 4 8 20 18 2 6 16 15 5 19 12 9 3 13 1 17\n",
      "329, 2 20 8 16 12 10 3 17 15 1 18 4 11 13 5 14 7 19 9 6\n",
      "330, 17 1 4 16 2 18 11 19 12 20 8 10 5 7 3 9 15 6 13 14\n",
      "331, \"sample missing,  just making the doc consistent\"\n",
      "332, 16 18 5 17 7 8 3 14 15 4 13 11 19 9 6 12 10 20 2\n",
      "333, 7 11 5 3 14 18 6 1 13 4 9 8 2 20 16 12 10 17 19 15\n",
      "334, 1 12 20 4 10 17 14 5 3 16 19 13 9 15 18 2 7 6 11 8\n",
      "335, 12 19 18 7 16 9 13 8 14 20 15 1 5 3 11 4 2 6 17 10\n",
      "336, 16 3 19 8 9 5 1 12 18 14 10 17 13 20 15 2 6 11 7 4\n",
      "337, 12 14 5 19 1 2 13 4 10 9 15 20 18 16 7 17 6 8 11 3\n",
      "338, 14 4 10 19 2 3 17 5 18 7 16 11 15 1 8 6 12 13 9 20\n",
      "339, 6 12 1 10 11 4 14 2 7 17 20 15 19 18 3 16 5 9 13\n",
      "340, 13 1 4 5 10 11 17 8 15 14 3 19 2 9 20 18 6 16 12 7\n",
      "341, 14 13 6 16 3 7 8 19 9 11 5 12 10 2 18 1 20 15 17\n",
      "342, 20 18 16 9 19 4 8 7 6 17 15 2 14 12 13 11 3 5 10\n",
      "343, 19 2 17 13 12 5 4 6 18 3 11 9 16 10 1 14 7 8 15\n",
      "344, 8 16 19 12 6 20 1 15 18 14 7 9 2 17 13 3 4 11 10 5\n",
      "345, 6 19 16 1 7 17 18 12 11 8 20 5 14 2 10 4 15 3 13\n",
      "346, 10 13 12 8 2 18 7 9 20 15 19 1 14 5 4 6 11 16 17\n",
      "347, 8 9 16 13 12 17 1 11 3 19 2 5 7 4 18 15 10 14 6\n",
      "348, 7 5 16 6 2 1 15 14 9 18 3 19 13 10 12 4 11 20 17\n",
      "349, \"sample missing,  just making the doc consistent\"\n",
      "350, \"sample missing,  just making the doc consistent\"\n",
      "351, 16 6 9 1 10 2 18 14 19 3 11 8 5 17 7 13 15 20 12 4\n",
      "352, 6 10 19 12 14 4 7 16 18 2 5 15 3 20 9 17 1 8 13 11\n",
      "353, 16 14 10 11 8 12 3 6 1 7 17 20 13 18 9 4 2 15 5 19\n",
      "354, 11 7 3 20 18 9 10 8 14 19 12 15 4 16 13 6 5 17 1 2\n",
      "355, 5 3 11 8 6 15 19 4 2 10 14 18 16 1 13 9 12 7 17 20\n",
      "356, 20 6 18 10 1 4 2 3 16 15 19 7 12 9 11 14 13 5 17 8\n",
      "357, 3 17 2 19 11 7 5 16 6 8 4 12 15 9 18 1 14 20 13 10\n",
      "358, 2 10 8 7 13 19 15 20 18 17 11 6 1 14 4 12 3 5 9 16\n",
      "359, 9 8 20 10 2 4 15 12 7 16 18 6 17 3 1 5 14 13 19 11\n",
      "360, 1 18 9 12 11 7 6 8 19 10 17 2 20 15 16 5 3 4 13 14\n",
      "361, 9 13 6 16 11 5 2 20 12 18 10 3 14 15 7 8 4 1 17 19\n",
      "362, 18 1 6 2 7 9 8 15 3 14 4 10 12 11 20 16 5 17 13 19\n",
      "363, 10 1 5 14 15 11 19 12 18 6 2 7 4 16 13 9 3 20 8 17\n",
      "364, 15 13 19 9 12 16 6 10 1 20 11 3 14 17 18 5 8 2 4 7\n",
      "365, 4 2 8 5 15 16 7 11 10 17 13 18 3 1 19 20 9 14 6 12\n",
      "366, 19 5 17 7 8 9 13 14 6 10 4 20 2 18 3 11 15 12 1 16\n",
      "367, 1 11 12 10 8 9 7 13 15 6 19 17 5 20 4 2 16 3 14 18\n",
      "368, 15 1 16 4 13 12 8 14 7 11 9 2 3 20 17 18 5 19 6 10\n",
      "369, 1 13 14 8 15 16 6 20 17 5 11 19 10 4 7 18 2 3 12 9\n",
      "370, 14 9 4 3 12 7 17 20 8 15 10 1 16 11 19 18 2 13 5 6\n",
      "371, 20 12 1 17 16 10 4 8 3 13 9 19 2 7 18 15 5 11 6 14\n",
      "372, 12 5 8 14 13 7 1 6 16 10 15 19 3 9 20 18 11 2 4 17\n",
      "373, 11 2 4 12 1 19 7 8 13 3 5 20 10 9 6 15 14 18 16 17\n",
      "374, 1 13 19 6 2 15 9 14 8 4 5 20 16 11 7 12 17 3 10 18\n",
      "375, 20 12 14 1 5 9 16 8 6 13 17 7 15 4 3 19 2 11 10 18\n",
      "376, 14 8 11 4 10 5 17 3 6 2 15 18 20 9 16 1 19 12 7 13\n",
      "377, 8 5 15 3 18 11 10 7 13 17 19 4 6 1 9 12 16 20 14 2\n",
      "378, 14 5 6 1 19 20 4 9 11 7 17 3 8 12 15 2 16 13 18 10\n",
      "379, 3 19 8 6 14 17 1 11 9 5 2 4 15 10 16 18 7 12 13 20\n",
      "380, 14 6 12 3 10 19 13 7 15 17 2 8 18 20 1 4 9 11 5 16\n",
      "381, 13 11 20 18 12 8 1 14 17 2 19 3 6 7 15 5 4 16 9 10\n",
      "382, \"sample missing,  just making the doc consistent\"\n",
      "383, 13 6 17 14 16 1 11 15 4 7 3 9 18 5 20 8 2 10 12 19\n",
      "384, 18 17 1 11 6 8 2 15 3 4 9 13 10 5 12 19 20 14 7 16\n",
      "385, 8 1 6 18 20 12 16 5 13 9 4 2 11 19 3 10 7 17 15 14\n",
      "386, 8 15 16 2 11 3 5 19 12 1 20 10 17 4 7 18 9 14 6\n",
      "387, \"sample missing,  just making the doc consistent\"\n",
      "388, \"sample missing,  just making the doc consistent\"\n",
      "389, 15 9 2 8 18 1 12 5 16 11 6 3 4 13 10 7 17 19 14\n",
      "390, 3 16 14 8 1 13 12 19 4 10 6 9 20 11 18 7 2 5 17 15\n",
      "391, 14 15 8 6 9 12 5 13 17 1 18 4 19 16 3 10 20 2 7 11\n",
      "392, 8 2 14 3 9 6 13 20 10 18 19 12 1 16 4 11 5 17 7 15\n",
      "393, 19 16 5 12 3 2 8 10 18 14 7 20 4 1 11 6 17 9 15 13\n",
      "394, 1 12 5 2 10 20 19 14 4 7 17 9 15 8 18 11 3 16 13\n",
      "395, 15 11 16 3 13 12 19 7 18 20 2 4 5 17 9 10 6 14 8 1\n",
      "396, 20 14 7 16 5 12 19 9 17 4 11 2 18 1 10 15 13 8 3 6\n",
      "397, 11 16 18 12 3 19 8 9 10 5 1 6 17 14 2 13 7 4 20 15\n",
      "398, \"sample missing,  just making the doc consistent\"\n",
      "399, \"sample missing,  just making the doc consistent\"\n",
      "400, \"sample missing,  just making the doc consistent\"\n",
      "401, 19 7 5 6 15 18 4 2 8 14 17 10 16 20 13 1 3 9 12 11\n",
      "402, 1 5 13 10 8 3 18 12 17 11 9 20 14 6 19 2 4 7 16 15\n",
      "403, 2 3 12 4 9 6 15 17 14 8 10 19 18 5 20 1 11 16 9 13\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "sampling_index = []\n",
    "all_labels_training = []\n",
    "with open('/path/to/training_labels_edited.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        sampling_index.append(row[0])\n",
    "        all_labels_training.append(str.split(row[1]))\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800, 16 11 14 12 7 13 8 15 3 9\n",
      "801, 5 6 17 7 20 11 2 15 3 12 10\n",
      "802, 17 13 1 11 4 5 9 19 3 6\n",
      "803, 16 6 1 2 7 14 19 17 11 8\n",
      "804, 6 2 8 1 13 5 11 16 14\n",
      "805, 17 15 13 18 19 4 3 11 12 5\n",
      "806, 13 3 10 4 14 5 7 11 18 15\n",
      "807, 13 18 15 17 9 12 2 16 14\n",
      "808, 4 12 7 11 20 2 17 19 8 6 5\n",
      "809, 3 15 13 11 7 6 19 1 14 12\n",
      "810, 10 4 13 3 17 9 1 2 18\n",
      "811, 19 15 7 10 17 13 5 8 16 12 1\n",
      "812, 9 18 3 5 11 8 17 13\n",
      "813, 10 14 20 17 2 12 7 15 1 5 19\n",
      "814, 14 18 11 2 17 15 5 6 13 1\n",
      "815, 10 11 20 4 18 2 8 14 5 16\n",
      "816, 5 17 10 8 4 2 20 15 16 19\n",
      "817, 13 7 20 4 1 14 16 6 3\n",
      "818, 17 13 2 12 6 10 9 15 20 3\n",
      "819, 9 17 16 20 13 15 6 8 1 7\n",
      "820, 2 11 9 12 19 17 16 4 10 7\n",
      "821, 11 2 8 7 6 3 9 16 13 10\n",
      "822, 16 4 13 11 8 9 2 14 19 1\n",
      "823, 7 10 6 14 2 11 9 20 5 8\n",
      "824, 20 14 7 6 13 2 4 19 12 5\n",
      "825, 2 8 6 1 13 19 10 3 16\n",
      "826, 10 11 9 18 3 5 15 8 12 4\n",
      "827, 7 17 9 13 18 3 12 2 4\n",
      "828, 1 17 14 13 15 16 5 18 20 4 19\n",
      "829, 14 5 3 13 4 18 9 20 11 10 19\n",
      "830, 4 5 10 9 3 2 8 16 1 18 15\n",
      "831, 12 20 11 9 8 4 16 6 18 15\n",
      "832, 8 5 7 6 4 10 18 3 13 11\n",
      "833, 5 15 2 6 19 20 13 11 7 9\n",
      "834, 2 16 15 4 9 19 6 12 10\n",
      "835, 3 16 19 8 12 5 6 15 7\n",
      "836, 13 11 2 20 18 16 5 15 10 4\n",
      "837, 20 17 15 18 4 19 8 12 14 11\n",
      "838, 20 17 15 18 4 19 8 12 14 11\n",
      "839, 15 10 8 13 1 19 5 16 20\n",
      "840, 15 10 8 13 1 19 5 16 20\n",
      "841, 3 15 17 5 1 19 11 6 8 9\n",
      "842, 3 15 17 5 1 19 11 6 8 9\n",
      "843, 17 8 5 4 2 12 13 10 15 16\n",
      "844, 17 8 5 4 2 12 13 10 15 16\n",
      "845, 3 5 6 8 16 14 4 18 17\n",
      "846, 13 17 2 6 8 5 20 7 11 9 10 15\n",
      "847, 2 17 19 13 10 20 6 3 1 11\n",
      "848, 13 5 20 14 1 17 11 18 7\n",
      "849, 19 10 20 1 8 5 3 9 4 6\n",
      "850, 4 9 19 8 14 7 17 13 15 16 20\n",
      "851, 2 19 7 11 16 8 12 5 6 13\n",
      "852, 12 8 15 18 19 17 20 14 1\n",
      "853, 2 3 18 8 7 19 6 10 9 15\n",
      "854, 7 12 18 16 19 6 4 1 20 11 17\n",
      "855, 13 3 17 19 7 15 16 1 6 12 14\n",
      "856, 8 17 20 7 14 2 11 6 16 10 1\n",
      "857, 20 8 16 18 2 9 15 5 14 3\n",
      "858, 20 7 9 1 16 2 4 10 8 13\n",
      "859, 17 5 2 14 11 8 3 16 13 18\n",
      "860, 6 17 19 12 16 2 20 15 7 4\n",
      "861, 6 19 20 13 5 8 15 1 11 4 9\n",
      "862, 13 6 4 14 11 1 9 12 17 16\n",
      "863, 13 17 4 11 7 18 15 12 14 8\n",
      "864, 1 14 2 7 6 17 8 15 19\n",
      "865, 20 3 4 15 6 8 16 1 14 19 18\n",
      "866, 4 8 10 18 9 13 11 17 1 19\n",
      "867, 9 4 2 1 14 18 6 8 13 5\n",
      "868, 19 15 2 9 4 12 5 7 3 6 11\n",
      "869, 4 6 8 20 16 5 12 17 14\n",
      "870, 16 8 5 12 1 13 15 3 14 9\n",
      "871, 14 4 5 9 8 10 19 2 18 3 11\n",
      "872, 11 12 2 13 16 8 14 9 18 17\n",
      "873, 12 8 19 2 7 16 13 1 6 3\n",
      "874, 4 18 2 9 11 7 8 14 6 15\n",
      "875, 10 13 6 4 8 5 18 9 11\n",
      "876, 13 1 16 10 18 12 15 6 5 11\n",
      "877, 12 16 1 9 13 2 3 15 17 14\n",
      "878, 10 9 11 3 16 15 7 20 4\n",
      "879, 12 10 17 6 7 14 8 5 2 19\n",
      "880, 10 7 1 20 17 2 15 11 4 8\n",
      "881, 9 19 7 5 14 3 18 17 10 11\n",
      "882, 1 18 20 19 11 14 15 7 8\n",
      "883, 17 1 9 14 8 20 15 19 3 6\n",
      "884, 13 9 15 17 10 7 6 3 1 16\n",
      "885, 13 19 1 2 10 3 18 14 6 17\n",
      "886, 1 20 19 11 3 9 2 10 16 14\n",
      "887, 8 2 20 4 18 17 16 14 13 1\n",
      "888, 13 17 1 14 6 10 8 4 20 16\n",
      "889, 1 4 5 16 19 3 15 9 12 8\n",
      "890, 14 18 13 11 20 8 16 2 4\n",
      "891, 6 10 20 4 11 2 19 14 3\n",
      "892, 6 3 12 18 13 1 19 8 5 15\n",
      "893, 1 9 16 17 8 7 3 19 14\n",
      "894, 7 4 9 18 20 12 13 14 11 17\n",
      "895, 15 20 18 17 7 8 6 12 14 11\n",
      "896, 13 20 12 17 4 7 8 10 5 1\n",
      "897, 14 12 18 7 8 17 9 13 15 20\n",
      "898, 10 5 11 4 7 19 6 8 14 17\n",
      "899, 18 4 20 14 19 2 10 12 1\n",
      "900, 4 18 12 3 1 6 11 16 8 14 13\n",
      "901, 10 8 17 12 2 15 19 16 5 20\n",
      "902, 17 18 14 5 1 16 20 4 3 8\n",
      "903, 8 2 7 20 15 10 11 19 6 14 3\n",
      "904, 19 4 13 17 3 8 20 1 11 7 5\n",
      "905, 15 16 13 9 19 5 14 6 12 18\n",
      "906, 19 2 20 13 12 5 3 9 14\n",
      "907, 4 14 9 19 12 3 17 5 20 16\n",
      "908, 4 8 9 17 5 2 13 19 18\n",
      "909, 18 7 13 1 19 9 6 12 8 10 5\n",
      "910, 13 18 12 17 7 3 1 11 16 2\n",
      "911, 6 18 13 8 1 16 4 9 7 20\n",
      "912, 8 16 3 13 15 10 12 4 17 11\n",
      "913, 4 14 19 17 3 6 8 15 10 5\n",
      "914, 17 9 11 14 13 20 8 7 19 1 10\n",
      "915, 12 20 1 10 4 8 11 3 18 2\n",
      "916, 8 11 17 9 4 10 14 18 20 3\n",
      "917, 4 16 17 19 8 12 6 7 15 2\n",
      "918, 18 15 8 12 5 3 9 16 20 1\n",
      "919, 8 14 12 15 4 17 10 6 20 18 9\n",
      "920, 20 12 13 7 6 14 4 16 19 11 1 10\n",
      "921, 3 17 2 11 1 9 5 4 16 20 6\n",
      "922, 12 8 11 19 6 3 2 13 18\n",
      "923, 9 16 18 12 13 11 10 20 2 1 6\n",
      "924, 6 5 19 2 13 16 17 3 8 14\n",
      "925, 1 3 1 1 2 14 7 19 15 10 13 12\n",
      "926, 18 2 20 5 12 13 11 15 17\n",
      "927, 4 16 13 18 5 8 1 10 7 11\n",
      "928, 3 6 7 16 1 10 9 2 20 12\n",
      "929, 7 11 13 3 8 16 12 15 1 9\n",
      "930, 14 11 20 5 16 13 3 12 19 9\n",
      "931, full video is missing\n",
      "932, 14 17 12 1 18 8 19 13 10 6\n",
      "933, 9 15 18 14 8 13 20 7 1 5\n",
      "934, 5 16 14 17 8 7 1 12 15\n",
      "935, 16 18 1 6 8 11 10 7 14 19 12\n",
      "936, 19 14 3 20 10 13 4 5 16 7 18\n",
      "937, 12 4 19 1 8 17 3 14\n",
      "938, 17 16 20 6 2 19 7 10 3 8\n",
      "939, 12 14 20 2 16 10 18 17 13 5\n",
      "940, 19 7 2 17 15 9 1 4 3 18 13\n",
      "941, 1 14 13 4 10 8 9 11 17\n",
      "942, 4 14 6 15 7 19 18 9 5 1\n",
      "943, 12 13 4 20 2 7 14 8 1 11 10\n",
      "944, 18 12 20 14 17 6 5 11 13\n",
      "945, 2 13 7 6 8 1 3 17 12\n",
      "946, 1 15 20 13 9 8 17 11 18 2\n",
      "947, 1 19 4 16 12 18 17 6 20 10 7\n",
      "948, 10 12 9 5 18 11 7 14 6 15\n",
      "949, 15 3 16 19 12 2 5 13 1 9 14\n",
      "950, 5 9 17 19 7 16 4 8 20 18\n",
      "951, 5 8 12 14 17 15 16 6 13 20\n",
      "952, 5 13 3 2 12 20 17 8 19\n",
      "953, 10 11 2 8 16 18 19 3 15 17\n",
      "954, 14 19 9 2 1 17 11 5 8 6\n",
      "955, 4 5 15 18 8 11 13 14 2 9 7\n",
      "956, 15 3 8 16 9 6 13 18 14\n",
      "957, 1 11 6 18 15 14 16 10 5 20\n",
      "958, 5 8 9 11 19 7 17 12 18\n",
      "959, 19 7 8 16 18 5 11 12 10 4 17\n",
      "960, 12 5 17 8 19 9 20 16 15 7\n",
      "961, 6 8 16 7 14 4 15 11 19 10\n",
      "962, 18 20 9 4 12 2 16 8 15 17\n",
      "963, 20 16 1 7 18 12 3 19 6 14\n",
      "964, 4 13 7 14 5 15 3 16 10 9\n",
      "965, 1 13 14 6 5 12 18 16 20 8\n",
      "966, 7 8 16 5 17 1 12 6 10 3\n",
      "967, 2 5 11 4 16 12 17 15 13 9\n",
      "968, 12 8 3 16 13 20 5 1 4\n",
      "969, 17 2 9 19 20 6 11 18 16 5\n",
      "970, 4 2 3 8 7 6 13 9 5 14\n",
      "971, 20 12 18 15 14 5 16 11 19 8\n",
      "972, 6 20 1 12 9 16 17 11 2 3\n",
      "973, 1 14 19 15 12 10 8 2 16 6\n",
      "974, 11 7 14 2 18 15 9 5 1 13 3\n",
      "975, 4 12 7 15 19 6 18 1 9\n",
      "976, 12 13 11 10 7 8 6 2 16\n",
      "977, 12 2 6 9 5 17 3 14 19 20\n",
      "978, 9 6 16 17 20 19 3 14 10 4\n",
      "979, 11 15 6 3 12 9 14 16 1 4 19\n",
      "980, 5 16 7 8 9 6 15 17 10 14 2\n",
      "981, 9 10 19 17 1 2 16 12 3 6 13\n",
      "982, 1 3 9 7 20 19 14 15 11\n",
      "983, 10 18 14 1 17 3 11 13 5\n",
      "984, 14 4 10 2 11 20 17 19 13 6\n",
      "985, 4 20 10 9 17 11 12 8 16 6\n",
      "986, 17 14 10 8 9 5 12 13 18 16\n",
      "987, 20 8 14 13 18 3 11 15 20 6\n",
      "988, 10 11 4 12 3 7 5 13 16 2\n",
      "989, 3 15 14 7 2 10 20 1 17 4\n",
      "990, 16 6 19 17 20 1 14 10 3 2\n",
      "991, 17 16 11 14 1 2 4 3 15 19\n",
      "992, 10 19 4 17 18 20 12 11 7\n",
      "993, 6 2 17 11 18 14 16 12 3 20 1\n",
      "994, 12 13 6 3 8 10 15 9 17\n",
      "995, 12 4 3 10 8 5 20 13 7 11\n",
      "996, 3 9 2 6 4 15 14 18 13 10\n",
      "997, 11 19 18 13 9 7 8 14 16 17\n",
      "998, 16 15 5 19 4 18 6 8 7 20\n",
      "999, 12 15 8 16 7 4 11 6 5 10\n",
      "1000, 5 13 9 10 20 16 4 11 14 8\n",
      "1001, 12 20 15 14 8 16 6 5 11 17\n",
      "1002, 19 10 17 7 12 11 13 8 3 6\n",
      "1003, 2 20 3 9 7 16 17 10 5 14\n",
      "1004, 13 11 14 18 2 8 17 5 12 1 7\n",
      "1005, 5 11 20 7 17 19 3 13 14\n",
      "1006, 9 2 7 14 6 20 11 15 4 16\n",
      "1007, 15 8 19 13 12 3 10 17 2\n",
      "1008, 11 1 5 17 8 4 15 12 13 10 6\n",
      "1009, 8 6 10 15 20 11 13 17 5\n",
      "1010, 13 17 16 1 15 14 7 19 18 20\n",
      "1011, 18 10 12 20 19 7 17 6 1 11\n",
      "1012, 4 8 20 10 9 3 1 7 11 6\n",
      "1013, 9 2 19 20 5 3 12 15 4 14\n",
      "1014, 15 9 6 14 5 4 10 8 17 7\n",
      "1015, 2 7 1 19 12 9 13 8 17\n",
      "1016, 5 2 17 3 12 6 1 10 11 7 8\n",
      "1017, 7 10 17 12 6 9 11 8 2 15\n",
      "1018, 11 18 14 5 13 6 10 19 12 3 17\n",
      "1019, 15 17 5 14 16 18 7 3 8 9\n",
      "1020, 1 20 7 12 11 16 3 10 6\n",
      "1021, 8 18 4 2 6 7 20 13 3 14\n",
      "1022, 8 9 16 19 10 11 3 18 13 2\n",
      "1023, 1 4 6 10 9 13 8 19 15 3\n",
      "1024, 4 12 16 20 3 19 1 8 13 18\n",
      "1025, 15 20 13 18 14 10 19 6 16\n",
      "1026, 5 2 11 18 10 8 15 1 9 12\n",
      "1027, 14 2 5 7 6 20 11 8 16\n",
      "1028, 10 12 15 18 1 11 3 17 19 7\n",
      "1029, 5 20 17 18 7 4 3 14 10 19\n",
      "1030, 20 2 6 1 17 4 13 8 18 5\n",
      "1031, 1 16 4 10 2 7 17 12 15 6 19\n",
      "1032, 7 6 4 15 19 1 16 2 3\n",
      "1033, 15 3 17 6 16 12 19 2 5\n",
      "1034, 4 19 9 5 14 18 12 15 16 11\n",
      "1035, 9 3 2 16 15 12 5 20 6 13\n",
      "1036, 9 19 7 12 15 3 4 11 14\n",
      "1037, 1 17 20 7 6 14 2 13 3 18\n",
      "1038, 6 15 14 20 11 9 7 3 13\n",
      "1039, 5 20 10 12 19 13 15 6 7 1 9\n",
      "1040, 1 16 18 12 19 6 20 14 9\n",
      "1041, 5 11 2 1 3 14 7 20 13 15\n",
      "1042, 6 11 1 12 10 7 13 19 8 16\n",
      "1043, 11 2 7 8 15 16 13 6 17 10\n",
      "1044, 5 10 9 20 1 12 14 2 7 18\n",
      "1045, 10 3 13 11 16 8 20 9 17 15\n",
      "1046, 6 14 19 10 2 16 7 20 18 4\n",
      "1047, 14 7 19 16 15 13 20 10 1 3 6\n",
      "1048, 3 6 12 16 19 5 17 8 7 1\n",
      "1049, 6 16 14 2 1 5 18 3 13 12\n",
      "1050, 9 2 20 19 13 8 15 18 6 12\n",
      "1051, 5 17 4 15 11 13 9 18 7\n",
      "1052, 19 17 3 11 13 15 5 10 4 8\n",
      "1053, 17 20 13 19 16 10 5 1 14 2 9\n",
      "1054, 20 18 12 19 7 1 17 4 14\n",
      "1055, 10 9 2 17 19 7 13 4 18 6\n",
      "1056, 20 11 9 2 6 7 12 3 16 1\n",
      "1057, 13 3 4 20 9 7 2 19 10\n",
      "1058, 12 16 4 1 10 19 11 18 9 7\n",
      "1059, 11 19 2 10 4 16 18 9 1 14\n",
      "1060, 7 2 3 9 8 13 14 10 6\n",
      "1061, 4 18 5 13 20 1 6 19 2 14 8\n",
      "1062, 6 5 13 16 18 19 12 9 2 8\n",
      "1063, 2 18 12 5 14 4 10 6 9 19\n",
      "1064, 1 5 18 20 17 10 16 15\n",
      "1065, 9 2 19 3 8 6 14 20 10\n",
      "1066, 10 4 5 14 11 17 18 9 1 16 7\n",
      "1067, 17 14 11 13 2 5 1 9 6 20\n",
      "1068, 8 19 15 3 13 14 9 11 12 6\n",
      "1069, 11 6 14 1 7 10 3 13 4 17\n",
      "1070, 5 18 12 4 9 10 16 20 2 1\n",
      "1071, 2 18 19 15 7 9 11 10 14 16\n",
      "1072, 16 14 8 10 6 17 11 1 13\n",
      "1073, 3 10 18 19 8 16 2 6 20 1\n",
      "1074, 15 1 8 11 4 18 14 12 17 13\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "sampling_index = []\n",
    "all_labels_test = []\n",
    "with open('/path/to/test_labels_edited.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        sampling_index.append(row[0])\n",
    "        all_labels_test.append(str.split(row[1]))\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410, 18 3 4 12 20 14 8 9 17\n",
      "411, 10 12 1 11 6 19 20 16 9 8 14\n",
      "412, 10 3 12 17 7 19 2 9 5 4\n",
      "413, 7 18 1 2 4 15 8 19 3 20\n",
      "414, 18 9 5 4 20 6 7 10 12 17\n",
      "415, 16 1 6 5 15 8 3 10 18 17\n",
      "416, 17 19 3 16 15 18 8 7 9 14\n",
      "417, 3 16 1 5 15 18 10 13 9 4 8\n",
      "418, 4 3 13 9 2 8 6 12 17 20\n",
      "419, 4 3 13 9 2 8 6 12 17 20\n",
      "420, 13 19 16 10 5 7 12 3 14 8\n",
      "421, 5 15 17 6 8 18 13 12 3 1\n",
      "422, 3 20 10 11 19 6 18 16 9 1\n",
      "423, 10 19 16 2 20 1 4 9 14\n",
      "424, 13 16 4 1 12 5 2 3 20\n",
      "425, 16 1 17 2 5 7 15 13 14 18\n",
      "426, 2 9 16 12 8 4 7 1 15\n",
      "427, 8 12 7 5 19 20 9 17 6 11\n",
      "428, 12 5 8 16 6 11 19 9 10 3 18\n",
      "429, 20 17 19 14 15 1 6 8 4 9\n",
      "430, 4 3 13 6 16 14 19 18 17 5\n",
      "431, 10 5 13 11 16 8 2 6 19\n",
      "432, 1 16 9 17 3 14 2 12 4\n",
      "433, 1 9 13 8 5 6 2 14 10 7\n",
      "434, 17 5 11 9 4 2 10 15 19 8\n",
      "435, 13 7 12 6 4 15 9 3 10 11\n",
      "436, 4 18 5 11 8 7 16 2 10\n",
      "437, 2 12 18 16 17 20 13 11 10 1\n",
      "438, 18 3 4 12 10 9 20 19 14 11 6\n",
      "439, 16 17 6 14 20 12 7 11 2\n",
      "440, 4 12 8 14 9 1 2 11 15 17 19\n",
      "441, 14 20 11 7 12 2 8 19 16 10 4\n",
      "442, 2 1 15 7 5 4 3 11 16\n",
      "443, 19 18 8 10 12 14 1 15 13 2\n",
      "444, 14 5 7 20 15 16 1 9 8 11\n",
      "445, 5 20 6 15 10 11 3 18 14 12\n",
      "446, 18 11 14 6 2 15 16 13 12 19\n",
      "447, 17 2 12 10 14 9 4 6 5\n",
      "448, 11 19 3 8 15 7 1 12 18\n",
      "449, 13 4 15 5 20 11 14 19 18 16\n",
      "450, 7 6 15 17 16 8 13 1 10 14\n",
      "451, 11 7 6 9 16 15 4 18 14 17\n",
      "452, 20 9 8 5 3 18 2 13 19 16\n",
      "453, 18 15 14 8 11 12 20 9 4 6\n",
      "454, 15 16 1 4 18 2 13 12 6\n",
      "455, 5 14 19 13 18 3 7 17 6 15 8\n",
      "456, 17 1 9 18 4 14 13 11 19\n",
      "457, 2 12 7 17 6 1 8 5 20 19 16\n",
      "458, 7 1 9 14 17 13 8 2 18\n",
      "459, 7 19 5 16 4 1 3 12 11\n",
      "460, 19 9 5 12 18 16 17 10 6\n",
      "461, 17 11 20 9 2 16 10 18 1 19\n",
      "462, 20 12 5 18 16 7 6 17 4 9\n",
      "463, 1 3 12 7 17 13 10 20 11 16\n",
      "464, 18 7 9 20 2 15 6 16 5\n",
      "465, 6 16 14 4 5 1 9 8 15\n",
      "466, 15 9 7 11 17 1 18 16 8 14\n",
      "467, 11 18 8 9 1 10 15 14 6 7\n",
      "468, 3 7 8 14 6 18 9 15 16\n",
      "469, 10 9 14 3 18 16 19 17 4 8\n",
      "470, 5 2 11 8 16 14 12 4 3\n",
      "471, 6 1 4 15 2 8 9 10 5 7 18\n",
      "472, 13 10 11 3 17 5 18 2 4\n",
      "473, 14 11 3 2 8 5 7 16 10\n",
      "474, 14 6 5 4 3 18 10 12 16 13\n",
      "475, 17 10 14 1 15 4 7 2 3\n",
      "476, 8 13 14 17 18 7 19 11 1 9\n",
      "477, 1 12 16 5 17 6 2 4\n",
      "478, 14 3 6 4 13 17 11 9 5 1 19\n",
      "479, 17 14 19 10 8 7 3 11 5 6 2\n",
      "480, 14 15 5 13 19 6 2 18 3 17\n",
      "481, 3 20 6 8 13 14 12 10 4 1 5\n",
      "482, 16 6 7 12 2 1 13 8 4\n",
      "483, 7 2 13 14 3 16 17 18 12\n",
      "484, 19 18 15 6 3 13 17 12 14\n",
      "485, 1 3 8 15 2 16 14 17\n",
      "486, 9 3 5 16 18 17 7 13 4\n",
      "487, 6 2 8 17 14 3 18 5 9 10 7\n",
      "488, 3 13 4 10 12 18 16 11 2\n",
      "489, 3 14 15 1 13 16 18 19 5\n",
      "490, 1 3 9 16 19 5 4 18 17\n",
      "491, 16 18 3 19 20 7 17 5 9 10\n",
      "492, 15 18 1 17 7 19 2 5 12\n",
      "493, 12 16 17 6 7 15 5 2 4\n",
      "494, 20 19 2 6 5 9 11 17 12 10\n",
      "495, 16 8 11 10 5 2 13 7 1 14 3\n",
      "496, 14 6 16 4 7 13 9 2 19\n",
      "497, 11 2 13 20 3 10 5 16 15 7\n",
      "498, 12 6 14 4 10 8 16 2 19 17\n",
      "499, 3 12 1 9 7 5 14 11 13\n",
      "500, 10 9 17 16 18 5 2 11 3 15\n",
      "501, 12 20 16 8 9 18 17 1 4 3 13\n",
      "502, 20 1 2 19 13 3 11 6 12 8\n",
      "503, 10 14 17 5 15 8 1 16 12 2\n",
      "504, 11 4 5 16 15 9 1 14 13 17\n",
      "505, 2 13 19 15 5 4 9 20 17 11\n",
      "506, 16 7 3 20 18 2 4 6 17 13 19\n",
      "507, 11 10 17 20 19 14 5 13 12 6\n",
      "508, 10 12 9 18 3 8 15 13 2 14 1\n",
      "509, 16 19 10 3 1 8 12 9 18 6\n",
      "510, 4 12 15 14 3 18 1 6 17\n",
      "511, 4 12 15 14 3 18 1 6 17\n",
      "512, 4 12 15 14 3 18 1 6 17\n",
      "513, 4 12 15 14 3 18 1 6 17\n",
      "514, 4 12 15 14 3 18 1 6 17\n",
      "515, 4 12 15 14 3 18 1 6 17\n",
      "516, 4 13 8 2 19 12 6 7 16\n",
      "517, 2 3 19 9 12 5 14 15 7\n",
      "518, 5 15 8 18 10 16 12 13 2 14\n",
      "519, 2 15 13 3 6 4 8 1 9 5\n",
      "520, 19 9 6 16 10 8 1 4 15 11\n",
      "521, 3 13 2 1 7 14 12 17 19\n",
      "522, 5 7 1 3 20 11 19 2 8 6\n",
      "523, 2 14 13 3 12 5 16 11 8 15\n",
      "524, 2 13 9 18 8 5 20 1 19\n",
      "525, 19 3 10 14 7 13 8 4 20\n",
      "526, 9 20 19 8 3 14 10 6 11 13\n",
      "527, 1 11 3 14 20 16 12 15 19\n",
      "528, 3 11 10 5 19 12 1 2 6 9\n",
      "529, 9 15 7 10 6 11 18 19\n",
      "530, 19 10 13 8 18 20 1 5\n",
      "531, 7 8 13 2 9 14 15\n",
      "532, 11 4 6 3 14 7 9 15 18 20\n",
      "533, 2 13 11 15 4 17 8 10 18 7\n",
      "534, 16 15 10 17 4 12 8 19 7 1\n",
      "535, 12 14 9 1 8 15 6 4 5 17\n",
      "536, 11 4 10 15 16 6 2 8 12\n",
      "537, 2 5 10 11 14 15 1 20 13 9 3 7\n",
      "538, 15 2 10 4 16 8 3 20 6 7\n",
      "539, 12 18 16 3 1 20 11 15 10\n",
      "540, 12 18 16 3 1 20 11 15 10\n",
      "541, 15 1 12 9 20 19 2 13 3\n",
      "542, 3 5 20 15 4 7 10 11 18 12\n",
      "543, 17 18 13 14 16 2 4 12 7 20 19\n",
      "544, 5 8 19 7 14 10 20 6 11 13 15\n",
      "545, 2 1 9 7 5 6 18 13 20 12\n",
      "546, 6 4 20 15 1 16 2 8 5 19\n",
      "547, 17 7 4 2 11 1 5 20 16 3\n",
      "548, 14 8 15 17 5 3 12 19 6 1 10\n",
      "549, 4 13 8 2 7 11 15 19 5 20\n",
      "550, 10 2 12 1 4 20 5 6 18 3\n",
      "551, 10 2 12 1 4 20 5 6 18 3\n",
      "552, 3 16 17 19 2 20 18 12\n",
      "553, 6 14 19 10 8 20 18 4 15 12 2 3 1 17 5 16 9\n",
      "554, 11 19 15 7 10 3 9 8 12 5 16 1 6 14 13\n",
      "555, 20 14 16 2 13 11 9 7 10 12 18 6 17 8\n",
      "556, 9 5 13 10 8 16 17 6 4 7\n",
      "557, 18 15 5 16 13 19 17 10 1 11 9 6 4 3 8 2 20\n",
      "558, 5 19 3 20 12 7 11 16 18 1\n",
      "559, 13 11 7 17 10 14 18 1 20 4 3\n",
      "560, 5 4 10 17 19 7 16 3 2\n",
      "561, 20 14 7 17 1 5\n",
      "562, 12 19 15 16 17 1\n",
      "563, 20 17 12 3 13 7 1 4 8 5 15 10 16 14 11\n",
      "564, 12 2 18 3 17 10 6 16 13 1 19 15 7 9 4 5 20\n",
      "565, 18 4 15 2 8 17 9 3 7 12 11 16 10 19 14 13 1\n",
      "566, 15 8 9 12 7 4 2 20 19 17 18 1 10 6 3 11 16 13\n",
      "567, 6 4 8 19 14 15 12 13 2 20 18 10\n",
      "568, 4 7 13 8 11 5 18 9 16 2 15 12 10 3 1\n",
      "569, 14 7 3 4 13 17 19 15 5 9 12 2\n",
      "570, 10 13 19 4 17 7 3 20 6 14 11 15 18 5 12\n",
      "571, 9 19 18 14 7 16 8 5 1 3 17 15 2 20\n",
      "572, 20 10 15 5 9 14\n",
      "573, 13 4 5 16 14 2 17 12\n",
      "574, 13 19 6 15 3 16 1 12 9 11 17 4 5 7 8\n",
      "575, 6 17 18 8 13 16 10 1 20 9 4\n",
      "576, 13 16 9 19 20 15 14 18 4 12\n",
      "577, 9 14 6 8 20 18 2 11 17 16 1 5 10 15 4 7 13 3 12\n",
      "578, 13 2 19 9 7 15\n",
      "579, 18 16 15 2 3 20 19\n",
      "580, 12 20 17 10 11 13 9 8 19 7\n",
      "581, 1 7 10 12 2 6 14 13 15 18 16 17 4 19 9 3 20\n",
      "582, 3 14 16 15 9 11 5 7 19 18 12 13 17 10 4 8 2 1 20 6\n",
      "583, 5 13 18 16 9 3 2\n",
      "584, 13 18 3 11 7 19 4 20 9 14 16 2 8 10 6 1\n",
      "585, 14 10 2 3 18 9 12 13 15 16 4 8 11 7 5 19 17 20 6\n",
      "586, 6 5 17 10 1 8 18\n",
      "587, 19 12 16 3 9 1 4 6 20 7 18 8 17\n",
      "588, 3 7 1 13 12 17 4 20 19 15 16 2 5 9 8 18 10 6 14 11\n",
      "589, 18 3 6 11 12 13 2 7 16 8\n",
      "590, 9 3 13 20 14 6 11 8 16 18 7\n",
      "591, 9 2 12 6 17 20 18 1 11 7 16 4 5 13\n",
      "592, 13 3 5 12 1 11 19 16 7 10 17\n",
      "593, 1 20 14 19 17 15 9 7 6\n",
      "594, 20 6 2 12 5 10 13 1 7 16 8 19 18 4 14\n",
      "595, 5 17 2 18 7 12 13 15 8 16 4 1 9 6 19 20 3 11 10\n",
      "596, 4 5 19 3 7 18 1 10 9 13 16 17 8 2 12 14 11 15 6\n",
      "597, 16 20 10 11 5 14 8 7 13 15 4 2 18 12 19\n",
      "598, 12 8 20 19 16 3 15 4 10\n",
      "599, 4 16 13 11 8 2 20 3 17 5 9 18 10 15 12\n",
      "600, 4 9 17 18 6 2 15 3 16\n",
      "601, 9 19 13 10 14 8\n",
      "602, 8 16 5 6 4 19 15 2 3 20 11 1 7 14 12 13 9 17\n",
      "603, 20 8 7 18 14 5 1 3 11 4 9 17 13 16 15\n",
      "604, 8 11 12 3 2 20 17 9 13 18 4 10 15 16 14 19 6 7\n",
      "605, 6 4 10 14 13 15 9 17 11 12 2 16\n",
      "606, 9 12 18 4 7 10 8 2 11 17 6 3 16\n",
      "607, 7 13 16 17 6 3 14 20 2 10 18 8 9 11 12 19 4 15 1 5\n",
      "608, 3 5 1 9 20 6 15 7 4 19 14\n",
      "609, 3 6 5 14 4 1 12 13 19 7 2 15 20 8 17 10 11 9 16\n",
      "610, 8 16 2 18 14 6 5 7 3 9 1 17 20 13 4\n",
      "611, 7 12 17 8 2 10 3 5 16 11 14 18 4\n",
      "612, 7 9 12 11 14 18 8 19 3 5 15 4 6 10 1 2 16\n",
      "613, 9 13 20 10 19 6 11 7 14 12 15 2 16 3 8\n",
      "614, 7 17 2 18 15 13 8 20 11 6 16\n",
      "615, 17 10 20 3 8 2 1 4 19 14 15 5 7 11 16\n",
      "616, 14 4 17 7 6 15 5 16 1 13 2 3 9 8 12 19 18 20\n",
      "617, 19 4 6 16 12 11 5 20 3 10 9 17 7 8 15 1 14 18 2\n",
      "618, 17 3 18 20 8 16 11 10 13 1 2 6 12 7 9 4 5\n",
      "619, 12 13 18 10 2 7 17 14 16 19 5 15 4\n",
      "620, 12 19 20 4 1 3 16 10 17 5\n",
      "621, 6 8 9 10 5 2 19 20 7 17 18 11 1 13 15 16 12 14 3\n",
      "622, 14 17 18 4 11 20 9 10 19 3 15 7\n",
      "623, 3 2 19 20 5 11 17 6 14 12 8 13\n",
      "624, 4 16 10 2 13 19 17 7 9 15\n",
      "625, 9 11 16 3 2 19 17 12 15 13 18 5 14 4\n",
      "626, 13 8 15 11 4 7 16 2 19 18 5 1\n",
      "627, 12 16 10 6 9 3 15 13 7 20 19 14 5 2 17\n",
      "628, 10 11 13 4 15 2 19 8 20 7 9\n",
      "629, 9 12 1 8 5 18 2 14 19 4 6 3\n",
      "630, 2 17 3 9 13 15 8 16 5 4\n",
      "631, 11 7 10 2 3 9 6 20 8 16 19 17\n",
      "632, 11 2 17 10 13 3 16 5 1 14 6 12\n",
      "633, 4 15 8 17 19 20 5 1\n",
      "634, 12 9 13 20 14 16 15 18 3\n",
      "635, 7 4 17 6 19 16 10 14 20 18 3 8 11 12 2\n",
      "636, 17 8 18 6 15 10 4 14 19 2 12 7 3 11 16 5\n",
      "637, 1 2 15 5 20 19 16 12 13 3 14 7 9 18 11 8 4 10 17 6\n",
      "638, 16 14 2 5 6 7 10 4 1 11 9 17 20 13\n",
      "639, 5 12 20 18 13 4 11 17 1 16 3 10 9 19\n",
      "640, 20 14 15 7 12 18 16 19 9 17 5 10 3 1 6\n",
      "641, 3 14 5 17 19 1 10 18\n",
      "642, 18 8 6 5 10 2 16 4\n",
      "643, 15 18 9 11 2 12 4 10 13 1 7 14 6 16\n",
      "644, 9 12 2 13 15 1 4 11 17\n",
      "645, 12 8 15 16 5 13 18 4 2 6 9\n",
      "646, 20 6 5 8 4 13 3 9 16 12\n",
      "647, 10 18 11 2 3 8 13 20 16 12\n",
      "648, 12 17 9 3 10 16 15 6 18\n",
      "649, 12 17 9 3 10 16 15 6 18\n",
      "650, 12 17 9 3 10 16 15 6 18\n",
      "651, 18 8 14 15 4 6 17 11 7\n",
      "652, 18 8 14 15 4 6 17 11 7\n",
      "653, 20 3 10 15 1 14 7 19 4 18 16 13 6 11 12 17 2 5 8 9\n",
      "654, 5 7 9 1 8 11 4 14 3 18 13\n",
      "655, 14 7 13 6 1 20 2 4 11 12 17 10 15 19 5 9 18\n",
      "656, 13 11 17 5 18 2 10 6 1 9 16 14 20 4 3 12 8 19\n",
      "657, 7 3 5 19 10 12 15 1 2 9 6 16 17 13 14 4\n",
      "658, 7 16 13 19 5 18 1 15 2 6 4 8 17\n",
      "659, 5 12 8 10 2 16 6 7 17 15 13 20 4 1 19 3\n",
      "660, 2 6 5 11 8 10 20 9 1 13 7 18 17 4 15\n",
      "661, 14 11 1 7 17 19 4 5 15 2 12 8 10 18 16 13 6 20 3 9\n",
      "662, 2 6 1 11 9 12 18 3 10 19 17 13 15 20 4\n",
      "663, 9 13 18 17 4 7 16 19 10 6 12\n",
      "664, 14 7 3 8 2 9 13 11\n",
      "665, 15 18 13 2 4 5 16 7 8 3 9\n",
      "666, 14 7 6 9 20 10 19 1 2 12 15 11 8 3 18 4 16 13 17 5\n",
      "667, 18 4 9 2 7 16 10 1 5 8 19 13 14\n",
      "668, 20 19 16 1 12 8 13 7 18 5 14\n",
      "669, 3 11 9 12 7 19 1 6 17 5 13 18 10 4 20 8 14 2 16 15\n",
      "670, 3 13 8 17 16 12 2 9 4 11 15 6 14 5 7 1 19 18 10 20\n",
      "671, 7 2 6 1 20 4 8 13 18 11 12\n",
      "672, 16 3 6 9 14 5 18 1 17 15 20 10 19 13\n",
      "673, 14 9 19 18 12 7 3 4 10 6 15 5 11 8 2 1 13 17 20\n",
      "674, 20 4 1 15 19 16 10 14 12 8 5 7 18 6\n",
      "675, 9 15 13 8 7 18 2 17\n",
      "676, 9 19 17 10 11 6 8 2 14 7 18 13 16 4 15 3 1 12 20 5\n",
      "677, 18 20 17 15 4 8 1 9\n",
      "678, 8 20 17 13 16 12 10 1 15 5 3 19 7 2 18 6 14 4 11 9\n",
      "679, 19 15 4 8 17 16 7 9 12 13 18 10\n",
      "680, 6 2 3 14 16 5 19 20 11 10 12\n",
      "681, 11 1 7 19 2 5 4 13\n",
      "682, 4 13 7 14 2 16 20 11 10 1 3 15 9 12 8\n",
      "683, 13 15 18 12 9 4 14 17 11 8 19 20 6 2 16\n",
      "684, 18 2 17 4 20 7 3 5 6 19 1 16 8 15\n",
      "685, 18 19 16 2 17 14 4 3 11 7 15 6 8 9 12\n",
      "686, 17 1 16 10 14 12 5 20 3 15 6 18 8 7 4\n",
      "687, 15 12 20 16 17 3 11 10\n",
      "688, 8 16 2 4 7 14 9 10 20 18 6 11 17 13\n",
      "689, 3 12 11 1 18 16 6 2 14 15 4 10 17\n",
      "690, 3 12 11 1 18 16 6 2 14 15 4 10 17\n",
      "691, 3 12 11 1 18 16 6 2 14 15 4 10 17\n",
      "692, 9 14 4 8 3 1 6 19 5 18 11 13 10\n",
      "693, 11 19 9 14 4 2 15 20 17 10\n",
      "694, 18 20 13 17 8 5 7 4 2 16 19 9 14 11 15 1 6 3 12 10\n",
      "695, 13 3 6 11 14 10 8 12 18 7 19 1 2 17 9\n",
      "696, 17 6 18 5 2 4 9 14 11 10 19 7 12 16 20 15 1 13 8\n",
      "697, 15 5 19 18 8 20 4 7 10 3 2 9 16 11 13\n",
      "698, 18 16 20 15 14 8 3 7 17 9\n",
      "699, 12 19 10 14 11 6 20 7 1 15 13 8 4 2 16 3\n",
      "700, 2 4 16 6 20 12 5 9 17 10 7 14 8 11\n",
      "701, 6 16 3 13 17 14 10 2 5 18 11 7 9 19 15 1\n",
      "702, 6 16 3 13 17 14 10 2 5 18 11 7 9 19 15 1\n",
      "703, 14 3 16 11 20 5 17 8 18 6\n",
      "704, 11 16 13 12 7 17 20 15 3 5 18\n",
      "705, 13 19 5 15 10 12 7 6 17 18 3 9 4\n",
      "706, 2 10 11 5 1 19 18 12 14 15 8 20 7 16 17 13 4 9 3 6\n",
      "707, 20 5 11 2 8 14 17 1\n",
      "708, 11 6 14 17 4 8 20 15 7 1 5 13 12\n",
      "709, 6 10 5 7 13 19 15 3 18 20 12 9 8\n",
      "710, 10 14 8 17 5 19 4 15 16 1 18 11 2 3 20 13 6 12 7 9\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "sampling_index = []\n",
    "all_labels_validation = []\n",
    "with open('/path/to/validation_labels_edited.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        sampling_index.append(row[0])\n",
    "        all_labels_validation.append(str.split(row[1]))\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(img1, img2):\n",
    "    err = np.sum((img1.astype(\"float\") - img2.astype(\"float\")) ** 2)\n",
    "    err /= float(img1.shape[0] * img2.shape[1])\n",
    "    return err\n",
    "\n",
    "def diffImg(t0, t1):\n",
    "    return cv2.absdiff(t0, t1)\n",
    "\n",
    "def getdiffDir(imgs):\n",
    "    #print(np.shape(imgs))\n",
    "    diff = []\n",
    "    all_ssim = []\n",
    "    all_mse = []\n",
    "    t = diffImg(imgs[0], imgs[1])\n",
    "    for i, img in enumerate(imgs[2:-1]):\n",
    "        #print(\"ssim\", ssim(img, imgs[0]))\n",
    "        im = diffImg(imgs[i-1], img)\n",
    "        t += im\n",
    "        all_ssim.append(1 - ssim(img.reshape(64,48), imgs[0].reshape(64,48)))\n",
    "        all_mse.append(mse(img.reshape(64,48), imgs[0].reshape(64,48)))\n",
    "    diff.append(t)\n",
    "    return all_ssim, all_mse, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Montalbano training subs\n",
    "class MontalbanoDataset(Dataset):\n",
    "    def __init__(self, path='/path/to/montalbano_preprocessed_frames/training',\n",
    "                kendon_path='/path/to/montalbano_kendon/training'):\n",
    "        self.x_samples = []\n",
    "        self.y_labels = []\n",
    "        self.file_names = []\n",
    "        self.path =  []\n",
    "        self.labelindex =  []\n",
    "        self.gesture_peak = []\n",
    "        self.use_snapture = []\n",
    "        #self.z_labels = []\n",
    "        for folder in listdir(os.path.join(path)): #Sample00021_color, Sample00022_color, etc.\n",
    "            print(folder)\n",
    "            if (folder.startswith(\"\")): #use for testing\n",
    "                print(folder)\n",
    "                sample_index = int(folder[6:-6])\n",
    "                t = []\n",
    "                all_sequences = []\n",
    "                one_sequence = []\n",
    "                started = False\n",
    "                for filename in sorted(os.listdir(os.path.join(path,folder)) ,key=lambda x: int(re.split(r'(\\d+)', x)[1])):\n",
    "                    if filename.endswith(\".jpg\"):\n",
    "                        if ('-start' in filename): \n",
    "                                started = True\n",
    "                        if ('-end' in filename):\n",
    "                                all_sequences.append(one_sequence)\n",
    "                                started = False\n",
    "                                one_sequence = []\n",
    "                        if (started):\n",
    "                                file = Image.open(os.path.join(path, folder, filename))\n",
    "                                img_data = np.asarray(file)\n",
    "                                img_data = np.transpose(img_data / 255.0)\n",
    "                                # store loaded image  \n",
    "                                one_sequence.append(img_data)\n",
    "\n",
    "                print(folder, ' contains ', str(len(all_sequences)), ' gestures')\n",
    "                if len(all_sequences) != len(all_labels_training[sample_index-1]):\n",
    "                    print(\"error\")\n",
    "                for gesture_index, sequence in enumerate(all_sequences):\n",
    "                    if len(sequence) < 45:\n",
    "                        sequence = np.pad(sequence, ((45-len(sequence), 0),(0,0),(0,0)), 'constant', constant_values=[0])  \n",
    "                    sequence = np.asarray(sequence[:45])\n",
    "                    kendon_stroke_index = round(len(sequence) / 2)\n",
    "                    seq = sequence[kendon_stroke_index-2:kendon_stroke_index+2]\n",
    "                    all_ssim, all_mse, diff = getdiffDir(seq)\n",
    "                    if (np.mean(all_ssim) <= 0.027 ):\n",
    "                        self.use_snapture.append(True)\n",
    "                    else:\n",
    "                        self.use_snapture.append(False)\n",
    "                    self.x_samples.append(np.reshape(sequence, (-1, 1, 64, 48)))\n",
    "                    self.y_labels.append(int(all_labels_training[sample_index-1][gesture_index])-1)\n",
    "                    self.file_names.append(folder+'/'+filename)\n",
    "                    self.path.append(folder)\n",
    "                    self.labelindex.append(str(sample_index-1) + '/' + str(gesture_index))\n",
    "                    img_file_kendon = Image.open(os.path.join(kendon_path, folder, str(gesture_index+1)+\"_hand.jpg\")).convert('L') #files start from 1\n",
    "                    img_data_kendon = np.asarray(img_file_kendon)\n",
    "                    self.gesture_peak.append(np.reshape(np.transpose(img_data_kendon / 255.0), (1, 64, 48)))\n",
    "        self.y_labels = np.asarray(self.y_labels)\n",
    "    def __len__(self):\n",
    "        return len(self.x_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_samples[idx], self.y_labels[idx], self.path[idx], self.labelindex[idx], self.gesture_peak[idx], self.use_snapture[idx]#, self.file_names[idx]\n",
    "        #return self.x_samples[idx], self.y_labels[idx], self.z_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MontalbanoValidationset(Dataset):\n",
    "    def __init__(self, path='/path/to/montalbano_preprocessed_frames/validation',\n",
    "                kendon_path='/path/to/montalbano_kendon/validation'):\n",
    "        self.x_samples = []\n",
    "        self.y_labels = []\n",
    "        self.path =  []\n",
    "        self.labelindex =  []\n",
    "        self.file_names =  []\n",
    "        self.gesture_peak = []\n",
    "        self.use_snapture = []\n",
    "        for folder in listdir(path): #Sample00021_color, Sample00022_color, etc.\n",
    "            print(folder)\n",
    "            if (folder.startswith(str(\"\"))):\n",
    "                print(folder)\n",
    "                sample_index = int(folder[6:-6])\n",
    "                print(sample_index)\n",
    "                sample_index = sample_index - 409\n",
    "\n",
    "                all_sequences = []\n",
    "                one_sequence = []\n",
    "                started = False\n",
    "                for filename in sorted(os.listdir(os.path.join(path,folder)) ,key=lambda x: int(re.split(r'(\\d+)', x)[1])):\n",
    "                    if filename.endswith(\".jpg\"):\n",
    "                            if ('-start' in filename): \n",
    "                                    started = True\n",
    "                            if ('-end' in filename): \n",
    "                                    all_sequences.append(one_sequence)\n",
    "                                    started = False\n",
    "                                    one_sequence = []\n",
    "                            if (started):\n",
    "                                    file = Image.open(os.path.join(path, folder, filename))\n",
    "                                    img_data = np.asarray(file)\n",
    "                                    img_data = np.transpose(img_data / 255.0)\n",
    "                                    one_sequence.append(img_data)\n",
    "\n",
    "                print(folder, ' contains ', str(len(all_sequences)), ' gestures')\n",
    "                if len(all_sequences) != len(all_labels_validation[sample_index-1]):\n",
    "                    print(\"error\")\n",
    "                for gesture_index, sequence in enumerate(all_sequences):\n",
    "                    if len(sequence) < 45:\n",
    "                        sequence = np.pad(sequence, ((45-len(sequence), 0),(0,0),(0,0)), 'constant', constant_values=[0])  \n",
    "                    sequence = np.asarray(sequence[:45])\n",
    "\n",
    "                    kendon_stroke_index = round(len(sequence) / 2)\n",
    "                    seq = sequence[kendon_stroke_index-2:kendon_stroke_index+2]\n",
    "                    all_ssim, all_mse, diff = getdiffDir(seq)\n",
    "                    if (np.mean(all_ssim) <= 0.027 ):\n",
    "                        self.use_snapture.append(True)\n",
    "                    else:\n",
    "                        self.use_snapture.append(False)\n",
    "\n",
    "                    self.x_samples.append(np.reshape(sequence, (-1, 1, 64, 48)))\n",
    "                    self.y_labels.append(int(all_labels_validation[sample_index-1][gesture_index])-1)\n",
    "                    self.file_names.append(folder+'/'+filename)\n",
    "                    self.path.append(folder)\n",
    "                    self.labelindex.append(str(sample_index-1+409) + '/' + str(gesture_index))\n",
    "                    img_file_kendon = Image.open(os.path.join(kendon_path, folder, str(gesture_index+1)+\"_hand.jpg\")).convert('L') #Files start from 1\n",
    "                    img_data_kendon = np.asarray(img_file_kendon)\n",
    "                    self.gesture_peak.append(np.reshape(np.transpose(img_data_kendon / 255.0), (1, 64, 48)))\n",
    "\n",
    "        self.y_labels = np.asarray(self.y_labels)\n",
    "    def __len__(self):\n",
    "        return len(self.x_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_samples[idx], self.y_labels[idx], self.path[idx], self.labelindex[idx], self.gesture_peak[idx], self.use_snapture[idx]\n",
    "        #return self.x_samples[idx], self.y_labels[idx], self.z_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Montalbano test subset\n",
    "class MontalbanoTestset(Dataset):\n",
    "\n",
    "    def __init__(self, path='/path/to/montalbano_preprocessed_frames/test',\n",
    "                kendon_path='/path/to/montalbano_kendon/test'):\n",
    "        self.x_samples = []\n",
    "        self.y_labels = []\n",
    "        self.path =  []\n",
    "        self.labelindex =  []\n",
    "        self.file_names =  []\n",
    "        self.gesture_peak = []\n",
    "        self.use_snapture = []\n",
    "\n",
    "        for folder in listdir(path): #Sample00021_color, Sample00022_color, etc.\n",
    "            print(folder)\n",
    "            if (folder.startswith(str(\"\"))):\n",
    "                print(folder)\n",
    "                sample_index = int(folder[6:-6])\n",
    "                print(sample_index)\n",
    "                sample_index = sample_index - 799\n",
    "                t = []\n",
    "                #loaded_labels = []\n",
    "\n",
    "                all_sequences = []\n",
    "                one_sequence = []\n",
    "                started = False\n",
    "                for filename in sorted(os.listdir(os.path.join(path,folder)) ,key=lambda x: int(re.split(r'(\\d+)', x)[1])):\n",
    "                    if filename.endswith(\".jpg\"):\n",
    "                            if ('-start' in filename): \n",
    "                                    started = True\n",
    "                            if ('-end' in filename): \n",
    "                                    all_sequences.append(one_sequence)\n",
    "                                    started = False\n",
    "                                    one_sequence = []\n",
    "                            if (started):\n",
    "                                    file = Image.open(os.path.join(path, folder, filename))\n",
    "                                    img_data = np.asarray(file)\n",
    "                                    img_data = np.transpose(img_data / 255.0)\n",
    "                                    # store loaded image  \n",
    "                                    one_sequence.append(img_data)\n",
    "                        \n",
    "                print(folder, ' contains ', str(len(all_sequences)), ' gestures')\n",
    "                if len(all_sequences) != len(all_labels_test[sample_index-1]):\n",
    "                    print(\"error\")\n",
    "                for gesture_index, sequence in enumerate(all_sequences):\n",
    "                    if len(sequence) < 45:\n",
    "                        sequence = np.pad(sequence, ((45-len(sequence), 0),(0,0),(0,0)), 'constant', constant_values=[0])  \n",
    "                    sequence = np.asarray(sequence[:45])\n",
    "\n",
    "                    kendon_stroke_index = round(len(sequence) / 2)\n",
    "                    seq = sequence[kendon_stroke_index-2:kendon_stroke_index+2]\n",
    "                    all_ssim, all_mse, diff = getdiffDir(seq)\n",
    "                    if (np.mean(all_ssim) <= 0.027 ):\n",
    "                        self.use_snapture.append(True)\n",
    "                    else:\n",
    "                        self.use_snapture.append(False)\n",
    "\n",
    "                    self.x_samples.append(np.reshape(sequence, (-1, 1, 64, 48)))\n",
    "                    self.y_labels.append(int(all_labels_test[sample_index-1][gesture_index])-1)\n",
    "                    self.file_names.append(folder+'/'+filename)\n",
    "                    self.path.append(folder)\n",
    "                    self.labelindex.append(str(sample_index-1+799) + '/' + str(gesture_index))\n",
    "                    img_file_kendon = Image.open(os.path.join(kendon_path, folder, str(gesture_index+1)+\"_hand.jpg\")).convert('L') #Files start from 1\n",
    "                    img_data_kendon = np.asarray(img_file_kendon)\n",
    "                    self.gesture_peak.append(np.reshape(np.transpose(img_data_kendon / 255.0), (1, 64, 48)))\n",
    "\n",
    "        self.y_labels = np.asarray(self.y_labels)\n",
    "    def __len__(self):\n",
    "        return len(self.x_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_samples[idx], self.y_labels[idx], self.path[idx], self.labelindex[idx], self.gesture_peak[idx], self.use_snapture[idx]\n",
    "        #return self.x_samples[idx], self.y_labels[idx], self.z_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample00308_color\n",
      "Sample00308_color\n",
      "Sample00308_color  contains  20  gestures\n",
      "Sample00309_color\n",
      "Sample00309_color\n",
      "Sample00309_color  contains  20  gestures\n",
      "Sample00310_color\n",
      "Sample00310_color\n",
      "Sample00310_color  contains  20  gestures\n"
     ]
    }
   ],
   "source": [
    "training_set = MontalbanoDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample00629_color\n",
      "Sample00629_color\n",
      "629\n",
      "Sample00629_color  contains  12  gestures\n",
      "Sample00631_color\n",
      "Sample00631_color\n",
      "631\n",
      "Sample00631_color  contains  12  gestures\n",
      "Sample00630_color\n",
      "Sample00630_color\n",
      "630\n",
      "Sample00630_color  contains  10  gestures\n"
     ]
    }
   ],
   "source": [
    "validation_set = MontalbanoValidationset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample01054_color\n",
      "Sample01054_color\n",
      "1054\n",
      "Sample01054_color  contains  9  gestures\n",
      "Sample01056_color\n",
      "Sample01056_color\n",
      "1056\n",
      "Sample01056_color  contains  10  gestures\n",
      "Sample01055_color\n",
      "Sample01055_color\n",
      "1055\n",
      "Sample01055_color  contains  10  gestures\n"
     ]
    }
   ],
   "source": [
    "test_set = MontalbanoTestset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7069, 6)\n",
      "(3505, 6)\n",
      "(2719, 6)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(training_set))\n",
    "print(np.shape(validation_set))\n",
    "print(np.shape(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13293,)\n"
     ]
    }
   ],
   "source": [
    "concat_dataset = torch.utils.data.ConcatDataset([training_set, validation_set, test_set])\n",
    "split_targets = concat_dataset.datasets[0].y_labels\n",
    "split_targets = np.append(split_targets, concat_dataset.datasets[1].y_labels)\n",
    "split_targets = np.append(split_targets, concat_dataset.datasets[2].y_labels)\n",
    "print(np.shape(split_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(split_targets.shape[0]), test_size=0.3, stratify=split_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9305\n",
      "3988\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices))\n",
    "print(len(test_indices))\n",
    "train_dataset = Subset(concat_dataset, indices=train_indices)\n",
    "test_dataset = Subset(concat_dataset, indices=test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[459 466 468 463 463 469 448 478 470 459 456 466 470 471 459 484 478 466\n",
      " 459 453]\n",
      "[197 200 201 199 198 201 192 205 202 196 195 200 202 202 196 207 205 199\n",
      " 197 194]\n",
      "Train balance 1.0152505446623095\n",
      "Test balance 1.015228426395939\n"
     ]
    }
   ],
   "source": [
    "# Check class balance\n",
    "_, train_counts = np.unique(split_targets[train_indices], return_counts=True)\n",
    "_, test_counts = np.unique(split_targets[test_indices], return_counts=True)\n",
    "print(train_counts)\n",
    "print(test_counts)\n",
    "print('Train balance {}\\nTest balance {}'.format(#not very expressive\n",
    "    train_counts[1]/train_counts[0], test_counts[1]/test_counts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9305, 6)\n",
      "(3988, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/7ali/gproj/venv/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_dataset))\n",
    "print(np.shape(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "cnn_init1 = None\n",
    "linear_init = None\n",
    "cnn_init5 = None\n",
    "linear_init64 = None\n",
    "linear_init128 = None\n",
    "linear_init256 = None\n",
    "linear_init1650 = None\n",
    "linear_init500 = None\n",
    "linear_init1000 = None\n",
    "lstm_init = None\n",
    "def weights_init(m):\n",
    "    global linear_init\n",
    "    global linear_init64\n",
    "    global linear_init128\n",
    "    global linear_init256\n",
    "    global linear_init500\n",
    "    global linear_init1650\n",
    "    global linear_init1000\n",
    "    global lstm_init\n",
    "    global cnn_init1\n",
    "    global cnn_init5\n",
    "    if isinstance(m, Conv2d):\n",
    "        zeros_(m.bias)\n",
    "        if m.in_channels == 1:\n",
    "            if cnn_init1 is None:\n",
    "                cnn_init1 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = cnn_init1\n",
    "        elif m.in_channels == 5:\n",
    "            if cnn_init5 is None:\n",
    "                cnn_init5 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = cnn_init5\n",
    "    elif isinstance(m, Linear):\n",
    "        zeros_(m.bias)\n",
    "        if m.in_features == 64:\n",
    "            if linear_init64 is None:\n",
    "                linear_init64 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init64\n",
    "        if m.in_features == 128:\n",
    "            if linear_init128 is None:\n",
    "                linear_init128 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init128\n",
    "        if m.in_features == 256:\n",
    "            if linear_init256 is None:\n",
    "                linear_init256 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init256\n",
    "        if m.in_features == 500:\n",
    "            if linear_init500 is None:\n",
    "                linear_init500 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init500\n",
    "        if m.in_features == 1000:\n",
    "            if linear_init1000 is None:\n",
    "                linear_init1000 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init1000\n",
    "        elif m.in_features == 1650:\n",
    "            if linear_init1650 is None:\n",
    "                linear_init1650 = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init1650\n",
    "        else:\n",
    "            if linear_init is None:\n",
    "                linear_init = xavier_uniform_(m.weight, calculate_gain('relu'))\n",
    "            else:\n",
    "                m.weight = linear_init\n",
    "        \n",
    "    elif isinstance(m, LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                    zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                if lstm_init is None:\n",
    "                    lstm_init = xavier_uniform_(param)\n",
    "                else:\n",
    "                    param = lstm_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 5, (11,11))\n",
    "        weights_init(self.conv1)\n",
    "        self.bn1 = BatchNorm2d(5)\n",
    "        self.act1 = Tanh()\n",
    "        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = Conv2d(5, 10, (6,6))\n",
    "        weights_init(self.conv2)\n",
    "        self.bn2 = BatchNorm2d(10)\n",
    "        self.act2 = Tanh()\n",
    "        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc1 = Linear(10*7*11, 500)\n",
    "        weights_init(self.fc1)\n",
    "        self.bn3 = BatchNorm1d(500)\n",
    "        self.act3 = Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.bn1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.pool1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.bn2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.pool2(X)\n",
    "        X = X.view(-1, 10*7*11)\n",
    "        X = self.fc1(X)\n",
    "        X = self.bn3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snap(Module):\n",
    "    def __init__(self):\n",
    "        super(Snap, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 5, (11,11))\n",
    "        self.act1 = Tanh()\n",
    "        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = Conv2d(5, 10, (6,6))\n",
    "        self.act2 = Tanh()\n",
    "        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc1 = Linear(10*7*11, 500)\n",
    "        self.act3 = Tanh()\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.conv1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.pool1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.pool2(X)\n",
    "        X = X.view(-1, 10*7*11)\n",
    "        X = self.fc1(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence level\n",
    "class CNNLSTM(Module):        \n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=500, \n",
    "            hidden_size=512,\n",
    "            hidden_size_snapture=512,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            num_units=512,\n",
    "            num_units_snapture=762,\n",
    "            num_classes=20,\n",
    "            snapture=True\n",
    "    ):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        #cnn for static part\n",
    "        if snapture:\n",
    "            #cnn for frames\n",
    "            self.snap = Snap().to('cuda:0')\n",
    "            self.snap.double()\n",
    "    \n",
    "        #cnn for frames\n",
    "        self.cnn = CNN().to('cuda:0')\n",
    "        self.cnn.double()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.dropout1.double()\n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.num_units=num_units\n",
    "        self.num_classes=num_classes\n",
    "        self.snapture=snapture\n",
    "        \n",
    "        self.rnn = LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first).to('cuda:0')\n",
    "        \n",
    "        weights_init(self.rnn)\n",
    "      \n",
    "        self.rnn.double()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.dropout2.double()\n",
    "        \n",
    "        self.act3 = Tanh()\n",
    "        self.linear2 = Linear(num_units_snapture,hidden_size_snapture).to('cuda:0')\n",
    "        self.linear2.double()\n",
    "        self.dropout3 = nn.Dropout()\n",
    "        self.dropout3.double()\n",
    "        self.act4 = Tanh()\n",
    "        self.linear3 = Linear(hidden_size_snapture,num_classes).to('cuda:0')\n",
    "        self.linear3.double()\n",
    "\n",
    "    def forward(self, x, gesture_peak):\n",
    "        x = x.contiguous()\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        c_out = self.dropout1(c_out)\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out = self.dropout2(r_out)\n",
    "                \n",
    "        gesture_peak_maps = self.snap(gesture_peak)\n",
    "        gesture_peak_maps = torch.cat((r_out[:, -1, :], gesture_peak_maps), dim=1)\n",
    "        r_out2 = self.linear2(gesture_peak_maps)\n",
    "        r_out2 = self.act4(r_out2)\n",
    "        r_out2 = self.dropout3(r_out2)\n",
    "        r_out3 = self.linear3(r_out2)\n",
    "        return F.log_softmax(r_out3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (snap): Snap(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(11, 11), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=770, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "    (fc2): Linear(in_features=500, out_features=250, bias=True)\n",
      "    (act4): Tanh()\n",
      "  )\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(11, 11), stride=(1, 1))\n",
      "    (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=770, out_features=500, bias=True)\n",
      "    (bn3): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (rnn): LSTM(500, 512, num_layers=2, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (act3): Tanh()\n",
      "  (linear2): Linear(in_features=762, out_features=512, bias=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (act4): Tanh()\n",
      "  (linear3): Linear(in_features=512, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = CNNLSTM()\n",
    "model.to('cuda:0')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        #for i, (images, labels, paths, labelindex, kendon, names) in enumerate(train_loader):\n",
    "        for i, train_data in enumerate(train_loader):\n",
    "            images, labels, gesture_peak = train_data[0].to('cuda:0'), train_data[1].to('cuda:0'), train_data[4].to('cuda:0')\n",
    "            outputs = model(images.double(), gesture_peak.double()).to('cuda:0') #statless\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(float(loss.item()))\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%2 == 0:\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', loss, '\\t', 'val_loss :', 0)\n",
    "    return loss_list, val_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = torch.zeros(20, 20)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data in test_loader:\n",
    "            data, labels, gesture_peak = test_data[0].to('cuda:0'), test_data[1].to('cuda:0'), test_data[4].to('cuda:0')\n",
    "            predictions = model(data.double(), gesture_peak.double()).to('cuda:0') #statelss\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc_on_test = float(num_correct)/float(total)\n",
    "        \n",
    "    print(f\"Test Accuracy of the model: {acc_on_test*100:.2f}\")\n",
    "    return acc_on_test, [], [], confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_montal(num_trials, cv_split):\n",
    "    num_epochs = 100\n",
    "    cv_result_extended = []\n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    \n",
    "    run_times = []\n",
    "    test_scores = []\n",
    "    all_confusion = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    loss_list = []\n",
    "    acc_per_epoch = []\n",
    "    all_models = []\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0,  drop_last=True, )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=0,  drop_last=True,)\n",
    "        model = CNNLSTM()\n",
    "        model = model.double()\n",
    "        optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        start = time.process_time() \n",
    "        print(device)\n",
    "        loss_list, val_losses, this_model = train_model(model, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "        a, b, c, d = test_model(model, train_loader, device)\n",
    "        acc, preds, labels, confusion_matrix = test_model(model, test_loader, device)\n",
    "        test_scores.append(acc)\n",
    "        run_times.append(time.process_time() - start)\n",
    "        pred_history.append(preds)\n",
    "        true_history.append(labels)\n",
    "        all_confusion.append(confusion_matrix)\n",
    "        all_models.append(this_model)\n",
    "\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch :  1 \t loss : tensor(2.2915, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.7047, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(1.1063, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(1.0680, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.5408, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.5313, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.3979, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2890, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.2550, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.3122, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.2256, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0899, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.1104, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0974, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.1215, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.1463, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0873, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0941, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0525, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0963, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0590, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.0563, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0797, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.1366, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0375, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.0206, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0378, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  55 \t loss : tensor(0.0891, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  57 \t loss : tensor(0.0357, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  59 \t loss : tensor(0.0443, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  61 \t loss : tensor(0.0656, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  63 \t loss : tensor(0.0710, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  65 \t loss : tensor(0.0161, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  67 \t loss : tensor(0.0222, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  69 \t loss : tensor(0.0537, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  71 \t loss : tensor(0.0270, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  73 \t loss : tensor(0.0194, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  75 \t loss : tensor(0.1095, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  77 \t loss : tensor(0.0644, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  79 \t loss : tensor(0.0103, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  81 \t loss : tensor(0.0461, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  83 \t loss : tensor(0.0067, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  85 \t loss : tensor(0.0207, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  87 \t loss : tensor(0.0287, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  89 \t loss : tensor(0.0578, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  91 \t loss : tensor(0.0597, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  93 \t loss : tensor(0.0729, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  95 \t loss : tensor(0.0536, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  97 \t loss : tensor(0.0099, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  99 \t loss : tensor(0.0400, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 99.64\n",
      "Test Accuracy of the model: 99.64\n",
      "Test Accuracy of the model: 71.09\n",
      "Test Accuracy of the model: 71.09\n",
      "ho\n",
      "0\n",
      "Epoch :  1 \t loss : tensor(2.0105, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.2427, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.9198, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.5814, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.4446, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.1935, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.4425, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2502, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1883, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1909, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.1046, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.1716, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0665, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.1689, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0537, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0419, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0850, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.1470, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0236, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0336, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0560, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.1293, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0883, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.0425, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0211, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.0268, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0371, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  55 \t loss : tensor(0.0445, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  57 \t loss : tensor(0.1324, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  59 \t loss : tensor(0.0646, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  61 \t loss : tensor(0.0170, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  63 \t loss : tensor(0.0641, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  65 \t loss : tensor(0.0454, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  67 \t loss : tensor(0.0457, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  69 \t loss : tensor(0.1151, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  71 \t loss : tensor(0.1055, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  73 \t loss : tensor(0.0725, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  75 \t loss : tensor(0.1421, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  77 \t loss : tensor(0.0652, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  79 \t loss : tensor(0.0785, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  81 \t loss : tensor(0.0132, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  83 \t loss : tensor(0.0547, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  85 \t loss : tensor(0.0115, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  87 \t loss : tensor(0.0600, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  89 \t loss : tensor(0.1134, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  91 \t loss : tensor(0.1141, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  93 \t loss : tensor(0.0357, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  95 \t loss : tensor(0.0077, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  97 \t loss : tensor(0.0082, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  99 \t loss : tensor(0.0373, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 99.88\n",
      "Test Accuracy of the model: 99.88\n",
      "Test Accuracy of the model: 75.96\n",
      "Test Accuracy of the model: 75.96\n",
      "ho\n",
      "0\n",
      "Epoch :  1 \t loss : tensor(1.7676, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.0668, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.6276, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.6113, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3671, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.2879, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.1441, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2085, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.0902, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.0732, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0593, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0438, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0885, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0972, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0310, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0508, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.1086, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0554, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0682, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.1148, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0956, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.0671, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0917, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.1261, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0417, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.1400, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0077, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  55 \t loss : tensor(0.0061, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  57 \t loss : tensor(0.0933, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  59 \t loss : tensor(0.0294, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  61 \t loss : tensor(0.1701, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  63 \t loss : tensor(0.0721, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  65 \t loss : tensor(0.0040, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  67 \t loss : tensor(0.1279, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  69 \t loss : tensor(0.0888, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  71 \t loss : tensor(0.0763, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  73 \t loss : tensor(0.0440, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  75 \t loss : tensor(0.0733, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  77 \t loss : tensor(0.1375, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  79 \t loss : tensor(0.0181, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  81 \t loss : tensor(0.0488, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  83 \t loss : tensor(0.0352, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  85 \t loss : tensor(0.0035, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  87 \t loss : tensor(0.1200, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  89 \t loss : tensor(0.0028, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  91 \t loss : tensor(0.0049, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  93 \t loss : tensor(0.0098, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  95 \t loss : tensor(0.0366, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  97 \t loss : tensor(0.0843, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  99 \t loss : tensor(0.0076, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 99.77\n",
      "Test Accuracy of the model: 99.77\n",
      "Test Accuracy of the model: 76.64\n",
      "Test Accuracy of the model: 76.64\n",
      "ho\n",
      "0\n",
      "Epoch :  1 \t loss : tensor(1.8797, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.0876, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.6236, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.2931, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3016, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.2119, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2847, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.0983, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.1180, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.2029, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.0961, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.2052, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0477, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0130, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0122, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0702, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0913, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0884, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.1739, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0194, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0145, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.1176, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0846, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.0104, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0791, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.0666, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0227, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  55 \t loss : tensor(0.0383, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  57 \t loss : tensor(0.0167, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  59 \t loss : tensor(0.0264, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  61 \t loss : tensor(0.0129, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  63 \t loss : tensor(0.0795, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  65 \t loss : tensor(0.0306, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  67 \t loss : tensor(0.0680, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  69 \t loss : tensor(0.0759, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  71 \t loss : tensor(0.1746, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  73 \t loss : tensor(0.1708, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  75 \t loss : tensor(0.0669, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  77 \t loss : tensor(0.0379, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  79 \t loss : tensor(0.0145, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  81 \t loss : tensor(0.0406, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  83 \t loss : tensor(0.1113, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  85 \t loss : tensor(0.0469, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  87 \t loss : tensor(0.0772, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  89 \t loss : tensor(0.0239, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  91 \t loss : tensor(0.0054, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  93 \t loss : tensor(0.0217, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  95 \t loss : tensor(0.0740, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  97 \t loss : tensor(0.0823, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  99 \t loss : tensor(0.0170, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 99.52\n",
      "Test Accuracy of the model: 99.52\n",
      "Test Accuracy of the model: 75.83\n",
      "Test Accuracy of the model: 75.83\n",
      "ho\n",
      "0\n",
      "Epoch :  1 \t loss : tensor(1.5907, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(0.8543, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.4244, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.4895, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.1640, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.1889, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.1250, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.1483, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.0591, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1136, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.1968, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.0966, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.1133, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.0536, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0495, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0147, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.0226, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.1077, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0771, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0212, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0423, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.0181, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0819, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.0088, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0227, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.0122, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0175, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  55 \t loss : tensor(0.0328, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  57 \t loss : tensor(0.0661, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  59 \t loss : tensor(0.0178, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  61 \t loss : tensor(0.0085, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  63 \t loss : tensor(0.0121, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  65 \t loss : tensor(0.0519, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  67 \t loss : tensor(0.0477, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  69 \t loss : tensor(0.0205, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  71 \t loss : tensor(0.0274, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  73 \t loss : tensor(0.0150, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  75 \t loss : tensor(0.0650, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  77 \t loss : tensor(0.0646, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  79 \t loss : tensor(0.0557, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  81 \t loss : tensor(0.0870, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  83 \t loss : tensor(0.0153, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  85 \t loss : tensor(0.0225, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  87 \t loss : tensor(0.0211, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  89 \t loss : tensor(0.0288, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  91 \t loss : tensor(0.0273, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  93 \t loss : tensor(0.0031, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  95 \t loss : tensor(0.0257, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  97 \t loss : tensor(0.0181, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  99 \t loss : tensor(0.0031, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 99.82\n",
      "Test Accuracy of the model: 99.82\n",
      "Test Accuracy of the model: 76.66\n",
      "Test Accuracy of the model: 76.66\n",
      "ho\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 5\n",
    "run_times, test_scores, pred_history, true_history, loss_list, val_losses, a, all_confusion, all_models = cnnlstm_montal(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(a)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766633064516129\n",
      "[0.7109375, 0.7595766129032258, 0.7663810483870968, 0.7583165322580645, 0.766633064516129]\n",
      "0.7523689516129032\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(test_scores)\n",
    "print(np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test scores  0.7523689516129032\n",
      "std test scores  0.020993775517921445\n",
      "best case test scores  0.766633064516129\n",
      "worst case test scores  0.7109375\n"
     ]
    }
   ],
   "source": [
    "print(\"mean test scores \", np.mean(test_scores))\n",
    "print(\"std test scores \", np.std(test_scores))\n",
    "print(\"best case test scores \", np.max(test_scores))\n",
    "print(\"worst case test scores \", np.min(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19094.902075337, 19097.073360261005, 19093.231940133002, 19129.848867698005, 19158.285496345008]\n",
      "mean training time:  19114.668347954805\n",
      "std training time:  25.662082997355192\n"
     ]
    }
   ],
   "source": [
    "print(run_times)\n",
    "print(\"mean training time: \", np.mean(run_times))\n",
    "print(\"std training time: \", np.std(run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[143.,  10.,   3.,   2.,   0.,   1.,   0.,   1.,   1.,   5.,   4.,   3.,\n",
      "           0.,   3.,   1.,   1.,  14.,   0.,   4.,   0.],\n",
      "        [ 23.,  95.,   7.,   5.,   0.,   0.,   0.,   3.,   0.,  12.,  18.,   1.,\n",
      "           0.,  14.,   8.,   0.,   9.,   3.,   1.,   0.],\n",
      "        [ 14.,   2., 142.,   1.,   0.,   0.,   0.,   3.,   0.,  15.,   6.,   4.,\n",
      "           1.,   2.,   4.,   0.,   2.,   2.,   0.,   1.],\n",
      "        [  1.,   4.,   3., 131.,   0.,   0.,   0.,   5.,   1.,  24.,   8.,  10.,\n",
      "           0.,   1.,   1.,   1.,   0.,   7.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0., 190.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,\n",
      "           4.,   0.,   0.,   1.,   1.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   2.,   5.,   3., 148.,  16.,   0.,  14.,   2.,   0.,   1.,\n",
      "           1.,   1.,   2.,   0.,   0.,   1.,   2.,   1.],\n",
      "        [  1.,   1.,   0.,   0.,   2.,   6., 160.,   0.,   6.,   0.,   0.,   1.,\n",
      "           1.,   1.,   1.,   2.,   0.,   0.,   5.,   3.],\n",
      "        [  4.,   3.,  19.,   9.,   0.,   0.,   1., 119.,   1.,   2.,   3.,   8.,\n",
      "           0.,   1.,   3.,   0.,   6.,  22.,   1.,   0.],\n",
      "        [  1.,   0.,   0.,   1.,   0.,   7.,  16.,   0., 167.,   3.,   0.,   0.,\n",
      "           2.,   1.,   1.,   0.,   0.,   1.,   1.,   0.],\n",
      "        [  4.,   1.,   8.,   7.,   0.,   2.,   1.,   2.,   2., 133.,   8.,   3.,\n",
      "           0.,   0.,  11.,   1.,   0.,   5.,   2.,   4.],\n",
      "        [  6.,  10.,   5.,   8.,   0.,   0.,   0.,   4.,   0.,   4., 117.,   1.,\n",
      "           0.,  15.,  19.,   1.,   0.,   2.,   0.,   3.],\n",
      "        [ 10.,   1.,   5.,   8.,   0.,   0.,   0.,   5.,   1.,  16.,   4., 124.,\n",
      "           1.,   1.,   2.,   0.,   2.,  14.,   0.,   4.],\n",
      "        [  1.,   0.,   0.,   1.,   3.,   0.,   4.,   0.,   2.,   0.,   1.,   0.,\n",
      "         183.,   2.,   0.,   1.,   0.,   1.,   2.,   1.],\n",
      "        [  3.,   3.,   1.,   3.,   0.,   0.,   1.,   3.,   0.,   4.,   5.,   0.,\n",
      "           1., 134.,  17.,  20.,   1.,   1.,   5.,   0.],\n",
      "        [  3.,   6.,   1.,   1.,   1.,   2.,   1.,   1.,   0.,  10.,   9.,   0.,\n",
      "           1.,  19., 113.,   6.,   0.,   0.,  12.,   9.],\n",
      "        [  1.,   0.,   1.,   2.,   3.,   0.,   1.,   2.,   1.,   0.,   2.,   1.,\n",
      "           0.,  16.,   2., 165.,   0.,   0.,   6.,   4.],\n",
      "        [ 23.,   6.,   3.,   0.,   0.,   0.,   0.,  10.,   0.,   0.,   1.,   1.,\n",
      "           0.,   1.,   1.,   1., 156.,   0.,   1.,   0.],\n",
      "        [  1.,   3.,   3.,   9.,   0.,   1.,   0.,  18.,   1.,  28.,   7.,  18.,\n",
      "           0.,   1.,   2.,   0.,   2., 100.,   1.,   3.],\n",
      "        [  1.,   1.,   0.,   0.,   1.,   1.,   4.,   0.,   1.,   2.,   0.,   0.,\n",
      "           0.,   3.,   7.,   2.,   2.,   0., 158.,  14.],\n",
      "        [  0.,   3.,   0.,   1.,   0.,   0.,   2.,   0.,   1.,   1.,   0.,   2.,\n",
      "           0.,   6.,   7.,   3.,   0.,   2.,  21., 143.]]), tensor([[147.,  11.,   4.,   2.,   0.,   0.,   0.,   2.,   0.,   1.,   4.,   3.,\n",
      "           0.,   1.,   1.,   0.,  20.,   0.,   1.,   0.],\n",
      "        [ 13., 118.,   1.,   2.,   0.,   1.,   0.,   9.,   0.,   3.,  26.,   1.,\n",
      "           0.,   7.,  11.,   0.,   5.,   1.,   1.,   0.],\n",
      "        [  0.,   5., 149.,   4.,   0.,   0.,   0.,  12.,   0.,  12.,   8.,   1.,\n",
      "           0.,   1.,   2.,   0.,   4.,   2.,   0.,   0.],\n",
      "        [  1.,   3.,   1., 143.,   0.,   0.,   0.,   9.,   1.,  15.,   7.,   4.,\n",
      "           0.,   1.,   0.,   0.,   0.,   6.,   0.,   3.],\n",
      "        [  0.,   0.,   0.,   0., 192.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,\n",
      "           3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   1.,   3.,   2., 165.,   5.,   2.,   8.,   3.,   0.,   1.,\n",
      "           1.,   1.,   2.,   1.,   0.,   1.,   0.,   2.],\n",
      "        [  0.,   0.,   1.,   0.,   3.,  13., 158.,   1.,   5.,   0.,   0.,   0.,\n",
      "           2.,   0.,   1.,   1.,   0.,   0.,   3.,   3.],\n",
      "        [  2.,   7.,  11.,   3.,   0.,   0.,   0., 157.,   0.,   0.,   4.,   2.,\n",
      "           0.,   1.,   0.,   0.,   7.,   9.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   2.,   1.,  12.,   1.,   0., 179.,   2.,   0.,   0.,\n",
      "           2.,   0.,   1.,   0.,   0.,   1.,   0.,   0.],\n",
      "        [  3.,   2.,  18.,  13.,   1.,   3.,   0.,   2.,   0., 129.,   5.,   1.,\n",
      "           0.,   6.,   6.,   1.,   0.,   1.,   0.,   4.],\n",
      "        [  3.,  14.,   6.,   5.,   0.,   1.,   0.,   2.,   0.,   5., 127.,   1.,\n",
      "           0.,  13.,   8.,   4.,   1.,   2.,   0.,   2.],\n",
      "        [  4.,   2.,   6.,  13.,   0.,   0.,   0.,  10.,   0.,  10.,   2., 141.,\n",
      "           0.,   3.,   0.,   0.,   1.,   7.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   0.,   2.,   0.,   1.,   0.,   1.,   0.,\n",
      "         187.,   0.,   1.,   3.,   0.,   0.,   0.,   2.],\n",
      "        [  0.,   4.,   1.,   0.,   2.,   1.,   1.,   4.,   0.,   6.,   5.,   0.,\n",
      "           1., 143.,  19.,   9.,   2.,   2.,   1.,   0.],\n",
      "        [  1.,   3.,   1.,   1.,   0.,   0.,   2.,   1.,   1.,   6.,  12.,   0.,\n",
      "           1.,  18., 123.,   5.,   1.,   4.,   6.,   9.],\n",
      "        [  0.,   1.,   0.,   3.,   2.,   0.,   1.,   2.,   0.,   1.,   3.,   1.,\n",
      "           0.,  17.,   1., 167.,   0.,   0.,   4.,   4.],\n",
      "        [  6.,   4.,   4.,   2.,   0.,   0.,   0.,   7.,   1.,   0.,   3.,   0.,\n",
      "           0.,   1.,   1.,   0., 175.,   0.,   0.,   0.],\n",
      "        [  1.,   6.,   0.,  14.,   0.,   3.,   0.,  21.,   0.,  14.,  16.,  12.,\n",
      "           0.,   0.,   2.,   1.,   0., 105.,   0.,   3.],\n",
      "        [  0.,   1.,   0.,   0.,   0.,   2.,   5.,   0.,   2.,   2.,   1.,   0.,\n",
      "           0.,   3.,   8.,   4.,   0.,   0., 144.,  24.],\n",
      "        [  0.,   0.,   0.,   3.,   1.,   1.,   2.,   0.,   0.,   1.,   1.,   2.,\n",
      "           1.,   1.,   7.,   4.,   0.,   1.,   3., 165.]]), tensor([[138.,  18.,   2.,   0.,   0.,   0.,   0.,   3.,   0.,   4.,   4.,   7.,\n",
      "           0.,   2.,   2.,   0.,  14.,   0.,   2.,   0.],\n",
      "        [ 12., 122.,   0.,   0.,   0.,   0.,   1.,  10.,   1.,   8.,  13.,   3.,\n",
      "           0.,   6.,   9.,   0.,   9.,   5.,   0.,   0.],\n",
      "        [  2.,   6., 142.,   0.,   0.,   0.,   0.,  10.,   0.,  10.,   7.,   7.,\n",
      "           0.,   1.,   2.,   0.,   8.,   6.,   0.,   0.],\n",
      "        [  0.,   4.,   1., 154.,   0.,   1.,   0.,   4.,   0.,  11.,   5.,   2.,\n",
      "           0.,   1.,   0.,   3.,   0.,   8.,   0.,   3.],\n",
      "        [  0.,   1.,   0.,   0., 192.,   1.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
      "           2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   0.,   2.,   7., 149.,   9.,   1.,  15.,   1.,   0.,   3.,\n",
      "           0.,   1.,   0.,   1.,   1.,   2.,   0.,   5.],\n",
      "        [  0.,   2.,   0.,   0.,   2.,   3., 162.,   0.,   9.,   0.,   0.,   0.,\n",
      "           1.,   0.,   2.,   1.,   0.,   0.,   4.,   3.],\n",
      "        [  0.,   4.,   2.,  10.,   0.,   0.,   0., 160.,   0.,   2.,   7.,   4.,\n",
      "           0.,   1.,   0.,   1.,   3.,  10.,   0.,   0.],\n",
      "        [  0.,   1.,   0.,   2.,   1.,   3.,  10.,   0., 179.,   2.,   0.,   0.,\n",
      "           1.,   0.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  4.,   2.,   6.,   2.,   2.,   0.,   0.,   0.,   2., 143.,   4.,   7.,\n",
      "           0.,   0.,   7.,   1.,   0.,  11.,   0.,   4.],\n",
      "        [  1.,  13.,   4.,   4.,   1.,   1.,   0.,   4.,   1.,   8., 127.,   2.,\n",
      "           0.,  14.,   7.,   1.,   1.,   5.,   0.,   1.],\n",
      "        [  5.,   2.,   8.,   4.,   0.,   0.,   0.,   5.,   0.,   9.,   2., 152.,\n",
      "           0.,   2.,   0.,   0.,   0.,  10.,   0.,   1.],\n",
      "        [  1.,   1.,   0.,   0.,   4.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
      "         189.,   2.,   0.,   1.,   0.,   0.,   1.,   1.],\n",
      "        [  1.,   8.,   1.,   1.,   1.,   1.,   0.,   3.,   0.,   1.,   8.,   0.,\n",
      "           0., 142.,  11.,  13.,   3.,   3.,   2.,   1.],\n",
      "        [  1.,   5.,   1.,   2.,   2.,   0.,   0.,   1.,   0.,   9.,  12.,   0.,\n",
      "           0.,  19., 121.,   4.,   0.,   5.,   5.,   8.],\n",
      "        [  0.,   1.,   0.,   2.,   2.,   0.,   1.,   0.,   1.,   1.,   1.,   0.,\n",
      "           0.,   9.,   2., 177.,   1.,   1.,   3.,   4.],\n",
      "        [  8.,   6.,   2.,   0.,   1.,   0.,   0.,  16.,   0.,   0.,   2.,   2.,\n",
      "           0.,   1.,   1.,   0., 164.,   0.,   0.,   0.],\n",
      "        [  2.,   3.,   1.,  13.,   0.,   0.,   0.,  18.,   2.,   6.,   6.,  15.,\n",
      "           0.,   3.,   1.,   0.,   1., 123.,   0.,   3.],\n",
      "        [  1.,   1.,   0.,   0.,   0.,   0.,   9.,   0.,   1.,   2.,   0.,   0.,\n",
      "           1.,   6.,   5.,   5.,   0.,   0., 147.,  19.],\n",
      "        [  1.,   1.,   0.,   3.,   0.,   0.,   2.,   0.,   2.,   2.,   1.,   2.,\n",
      "           1.,   0.,   8.,   2.,   0.,   1.,  10., 158.]]), tensor([[126.,  14.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,   6.,   6.,\n",
      "           0.,   2.,   1.,   0.,  30.,   0.,   2.,   1.],\n",
      "        [ 11., 126.,   1.,   1.,   0.,   0.,   0.,   5.,   1.,   2.,  21.,   1.,\n",
      "           0.,  12.,   9.,   0.,   6.,   3.,   1.,   0.],\n",
      "        [  6.,   4., 142.,   0.,   0.,   0.,   0.,  10.,   0.,   7.,  13.,   2.,\n",
      "           0.,   0.,   3.,   0.,   8.,   4.,   0.,   0.],\n",
      "        [  0.,   5.,   1., 120.,   0.,   1.,   0.,   5.,   2.,  16.,   9.,   7.,\n",
      "           0.,   3.,   1.,   3.,   0.,  24.,   0.,   1.],\n",
      "        [  0.,   0.,   1.,   0., 178.,   3.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
      "           9.,   0.,   1.,   4.,   0.,   0.,   0.,   0.],\n",
      "        [  4.,   1.,   0.,   1.,   1., 169.,   5.,   0.,   6.,   1.,   2.,   2.,\n",
      "           1.,   1.,   4.,   0.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   0.,   1.,   0.,   1.,  12., 158.,   1.,   5.,   1.,   0.,   0.,\n",
      "           2.,   0.,   1.,   2.,   0.,   0.,   4.,   3.],\n",
      "        [  1.,   7.,   5.,   4.,   0.,   0.,   0., 140.,   0.,   1.,   8.,   6.,\n",
      "           0.,   3.,   0.,   0.,   4.,  23.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   5.,   1.,  15.,   7.,   0., 162.,   6.,   0.,   0.,\n",
      "           2.,   0.,   0.,   0.,   0.,   2.,   0.,   1.],\n",
      "        [  0.,   5.,   8.,   1.,   1.,   1.,   0.,   0.,   0., 144.,  12.,   3.,\n",
      "           0.,   2.,  12.,   0.,   0.,   6.,   0.,   1.],\n",
      "        [  3.,  14.,   1.,   0.,   0.,   1.,   0.,   3.,   0.,   2., 138.,   1.,\n",
      "           0.,  11.,   7.,   1.,   2.,  10.,   0.,   0.],\n",
      "        [  7.,   3.,   5.,   3.,   0.,   0.,   0.,   1.,   0.,  17.,   1., 139.,\n",
      "           0.,   0.,   0.,   0.,   3.,  16.,   0.,   4.],\n",
      "        [  1.,   0.,   0.,   0.,   2.,   0.,   2.,   0.,   1.,   0.,   0.,   0.,\n",
      "         188.,   3.,   0.,   1.,   0.,   0.,   1.,   3.],\n",
      "        [  2.,   7.,   0.,   0.,   0.,   0.,   1.,   1.,   0.,   1.,  11.,   1.,\n",
      "           0., 143.,  18.,   7.,   2.,   4.,   0.,   3.],\n",
      "        [  0.,   4.,   1.,   0.,   0.,   0.,   2.,   0.,   0.,   4.,  13.,   0.,\n",
      "           1.,  13., 139.,   5.,   0.,   5.,   1.,   7.],\n",
      "        [  0.,   0.,   0.,   1.,   1.,   0.,   0.,   2.,   0.,   1.,   2.,   0.,\n",
      "           1.,  11.,   4., 171.,   0.,   0.,   6.,   4.],\n",
      "        [  7.,   3.,   2.,   0.,   0.,   0.,   0.,  10.,   0.,   0.,   1.,   1.,\n",
      "           1.,   2.,   1.,   0., 177.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   0.,   3.,   0.,   1.,   1.,   9.,   0.,   8.,   8.,  11.,\n",
      "           0.,   0.,   2.,   0.,   1., 152.,   0.,   1.],\n",
      "        [  2.,   0.,   0.,   0.,   0.,   1.,   5.,   0.,   0.,   3.,   1.,   0.,\n",
      "           0.,   2.,  27.,   1.,   0.,   0., 131.,  23.],\n",
      "        [  2.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,   4.,   3.,   1.,\n",
      "           0.,   2.,   9.,   1.,   0.,   2.,   2., 166.]]), tensor([[142.,   8.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,   2.,   2.,\n",
      "           0.,   0.,   4.,   0.,  23.,   0.,   5.,   2.],\n",
      "        [ 16., 113.,   0.,   0.,   0.,   0.,   0.,  11.,   1.,   7.,  17.,   1.,\n",
      "           0.,   1.,  12.,   2.,   9.,   7.,   1.,   0.],\n",
      "        [  4.,   1., 150.,   0.,   0.,   0.,   0.,  12.,   0.,   5.,   9.,   7.,\n",
      "           0.,   0.,   4.,   0.,   5.,   2.,   0.,   1.],\n",
      "        [  0.,   2.,   1., 137.,   1.,   0.,   0.,   7.,   2.,  18.,   6.,  14.,\n",
      "           0.,   1.,   0.,   0.,   0.,   7.,   0.,   3.],\n",
      "        [  0.,   0.,   0.,   0., 189.,   1.,   1.,   0.,   0.,   1.,   0.,   0.,\n",
      "           4.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   0.,   0.,   0.,   5., 137.,  13.,   1.,  31.,   3.,   1.,   0.,\n",
      "           3.,   0.,   3.,   0.,   0.,   0.,   0.,   2.],\n",
      "        [  0.,   1.,   0.,   0.,   3.,   3., 167.,   0.,   4.,   0.,   0.,   0.,\n",
      "           3.,   1.,   1.,   0.,   0.,   0.,   5.,   3.],\n",
      "        [  1.,   5.,   1.,   6.,   0.,   0.,   0., 155.,   0.,   2.,   4.,   7.,\n",
      "           0.,   3.,   0.,   1.,   5.,  14.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   1.,   1.,   4.,   7.,   0., 187.,   0.,   0.,   0.,\n",
      "           1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.],\n",
      "        [  3.,   3.,   4.,  10.,   1.,   0.,   0.,   0.,   4., 139.,   5.,  10.,\n",
      "           0.,   0.,   8.,   2.,   0.,   5.,   1.,   0.],\n",
      "        [  2.,  13.,   4.,   8.,   0.,   0.,   0.,   6.,   0.,  11., 124.,   0.,\n",
      "           0.,   6.,  13.,   1.,   1.,   4.,   0.,   1.],\n",
      "        [  1.,   1.,   5.,   5.,   0.,   0.,   0.,   4.,   0.,  11.,   1., 157.,\n",
      "           0.,   0.,   0.,   0.,   2.,   7.,   0.,   5.],\n",
      "        [  0.,   0.,   0.,   0.,   2.,   0.,   2.,   0.,   1.,   1.,   1.,   0.,\n",
      "         190.,   2.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   5.,   1.,   2.,   1.,   0.,   1.,   3.,   0.,   1.,   9.,   0.,\n",
      "           0., 142.,  17.,  10.,   3.,   3.,   0.,   4.],\n",
      "        [  0.,   6.,   1.,   1.,   2.,   1.,   1.,   1.,   0.,  11.,  10.,   0.,\n",
      "           0.,  16., 121.,   2.,   2.,   3.,  10.,   7.],\n",
      "        [  0.,   0.,   0.,   0.,   6.,   1.,   0.,   2.,   0.,   0.,   3.,   0.,\n",
      "           1.,  11.,   2., 173.,   0.,   0.,   6.,   1.],\n",
      "        [ 15.,   2.,   2.,   1.,   1.,   0.,   0.,   7.,   0.,   0.,   0.,   3.,\n",
      "           0.,   0.,   1.,   0., 171.,   0.,   0.,   0.],\n",
      "        [  2.,   0.,   2.,  11.,   0.,   0.,   0.,  21.,   1.,  12.,   2.,  15.,\n",
      "           0.,   0.,   0.,   1.,   0., 130.,   0.,   1.],\n",
      "        [  0.,   1.,   0.,   0.,   0.,   0.,   8.,   0.,   0.,   1.,   0.,   0.,\n",
      "           1.,   2.,   8.,   3.,   0.,   0., 158.,  13.],\n",
      "        [  0.,   2.,   0.,   1.,   1.,   0.,   2.,   1.,   2.,   4.,   1.,   1.,\n",
      "           0.,   3.,   7.,   3.,   0.,   0.,   6., 160.]])]\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(all_confusion))\n",
    "print(len(true_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1 = []\n",
    "for conf in all_confusion:\n",
    "    recall = np.diag(conf.numpy()) / np.sum(conf.numpy(), axis = 1)\n",
    "    precision = np.diag(conf.numpy()) / np.sum(conf.numpy(), axis = 0)\n",
    "    recall = np.mean(recall)\n",
    "    precision = np.mean(precision)\n",
    "    all_f1.append(2 * (precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7128494067803341, 0.7617556149125486, 0.7676749654257194, 0.7643632460965029, 0.7679821350391547]\n",
      "f1 score:  0.754925073650852\n",
      "f1 score:  0.02116196459693246\n"
     ]
    }
   ],
   "source": [
    "print(all_f1)\n",
    "print(\"f1 score: \", np.mean(all_f1))\n",
    "print(\"f1 score: \", np.std(all_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[143.,  10.,   3.,   2.,   0.,   1.,   0.,   1.,   1.,   5.,   4.,   3.,\n",
      "           0.,   3.,   1.,   1.,  14.,   0.,   4.,   0.],\n",
      "        [ 23.,  95.,   7.,   5.,   0.,   0.,   0.,   3.,   0.,  12.,  18.,   1.,\n",
      "           0.,  14.,   8.,   0.,   9.,   3.,   1.,   0.],\n",
      "        [ 14.,   2., 142.,   1.,   0.,   0.,   0.,   3.,   0.,  15.,   6.,   4.,\n",
      "           1.,   2.,   4.,   0.,   2.,   2.,   0.,   1.],\n",
      "        [  1.,   4.,   3., 131.,   0.,   0.,   0.,   5.,   1.,  24.,   8.,  10.,\n",
      "           0.,   1.,   1.,   1.,   0.,   7.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0., 190.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,\n",
      "           4.,   0.,   0.,   1.,   1.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   2.,   5.,   3., 148.,  16.,   0.,  14.,   2.,   0.,   1.,\n",
      "           1.,   1.,   2.,   0.,   0.,   1.,   2.,   1.],\n",
      "        [  1.,   1.,   0.,   0.,   2.,   6., 160.,   0.,   6.,   0.,   0.,   1.,\n",
      "           1.,   1.,   1.,   2.,   0.,   0.,   5.,   3.],\n",
      "        [  4.,   3.,  19.,   9.,   0.,   0.,   1., 119.,   1.,   2.,   3.,   8.,\n",
      "           0.,   1.,   3.,   0.,   6.,  22.,   1.,   0.],\n",
      "        [  1.,   0.,   0.,   1.,   0.,   7.,  16.,   0., 167.,   3.,   0.,   0.,\n",
      "           2.,   1.,   1.,   0.,   0.,   1.,   1.,   0.],\n",
      "        [  4.,   1.,   8.,   7.,   0.,   2.,   1.,   2.,   2., 133.,   8.,   3.,\n",
      "           0.,   0.,  11.,   1.,   0.,   5.,   2.,   4.],\n",
      "        [  6.,  10.,   5.,   8.,   0.,   0.,   0.,   4.,   0.,   4., 117.,   1.,\n",
      "           0.,  15.,  19.,   1.,   0.,   2.,   0.,   3.],\n",
      "        [ 10.,   1.,   5.,   8.,   0.,   0.,   0.,   5.,   1.,  16.,   4., 124.,\n",
      "           1.,   1.,   2.,   0.,   2.,  14.,   0.,   4.],\n",
      "        [  1.,   0.,   0.,   1.,   3.,   0.,   4.,   0.,   2.,   0.,   1.,   0.,\n",
      "         183.,   2.,   0.,   1.,   0.,   1.,   2.,   1.],\n",
      "        [  3.,   3.,   1.,   3.,   0.,   0.,   1.,   3.,   0.,   4.,   5.,   0.,\n",
      "           1., 134.,  17.,  20.,   1.,   1.,   5.,   0.],\n",
      "        [  3.,   6.,   1.,   1.,   1.,   2.,   1.,   1.,   0.,  10.,   9.,   0.,\n",
      "           1.,  19., 113.,   6.,   0.,   0.,  12.,   9.],\n",
      "        [  1.,   0.,   1.,   2.,   3.,   0.,   1.,   2.,   1.,   0.,   2.,   1.,\n",
      "           0.,  16.,   2., 165.,   0.,   0.,   6.,   4.],\n",
      "        [ 23.,   6.,   3.,   0.,   0.,   0.,   0.,  10.,   0.,   0.,   1.,   1.,\n",
      "           0.,   1.,   1.,   1., 156.,   0.,   1.,   0.],\n",
      "        [  1.,   3.,   3.,   9.,   0.,   1.,   0.,  18.,   1.,  28.,   7.,  18.,\n",
      "           0.,   1.,   2.,   0.,   2., 100.,   1.,   3.],\n",
      "        [  1.,   1.,   0.,   0.,   1.,   1.,   4.,   0.,   1.,   2.,   0.,   0.,\n",
      "           0.,   3.,   7.,   2.,   2.,   0., 158.,  14.],\n",
      "        [  0.,   3.,   0.,   1.,   0.,   0.,   2.,   0.,   1.,   1.,   0.,   2.,\n",
      "           0.,   6.,   7.,   3.,   0.,   2.,  21., 143.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[147.,  11.,   4.,   2.,   0.,   0.,   0.,   2.,   0.,   1.,   4.,   3.,\n",
      "           0.,   1.,   1.,   0.,  20.,   0.,   1.,   0.],\n",
      "        [ 13., 118.,   1.,   2.,   0.,   1.,   0.,   9.,   0.,   3.,  26.,   1.,\n",
      "           0.,   7.,  11.,   0.,   5.,   1.,   1.,   0.],\n",
      "        [  0.,   5., 149.,   4.,   0.,   0.,   0.,  12.,   0.,  12.,   8.,   1.,\n",
      "           0.,   1.,   2.,   0.,   4.,   2.,   0.,   0.],\n",
      "        [  1.,   3.,   1., 143.,   0.,   0.,   0.,   9.,   1.,  15.,   7.,   4.,\n",
      "           0.,   1.,   0.,   0.,   0.,   6.,   0.,   3.],\n",
      "        [  0.,   0.,   0.,   0., 192.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,\n",
      "           3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   1.,   3.,   2., 165.,   5.,   2.,   8.,   3.,   0.,   1.,\n",
      "           1.,   1.,   2.,   1.,   0.,   1.,   0.,   2.],\n",
      "        [  0.,   0.,   1.,   0.,   3.,  13., 158.,   1.,   5.,   0.,   0.,   0.,\n",
      "           2.,   0.,   1.,   1.,   0.,   0.,   3.,   3.],\n",
      "        [  2.,   7.,  11.,   3.,   0.,   0.,   0., 157.,   0.,   0.,   4.,   2.,\n",
      "           0.,   1.,   0.,   0.,   7.,   9.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   2.,   1.,  12.,   1.,   0., 179.,   2.,   0.,   0.,\n",
      "           2.,   0.,   1.,   0.,   0.,   1.,   0.,   0.],\n",
      "        [  3.,   2.,  18.,  13.,   1.,   3.,   0.,   2.,   0., 129.,   5.,   1.,\n",
      "           0.,   6.,   6.,   1.,   0.,   1.,   0.,   4.],\n",
      "        [  3.,  14.,   6.,   5.,   0.,   1.,   0.,   2.,   0.,   5., 127.,   1.,\n",
      "           0.,  13.,   8.,   4.,   1.,   2.,   0.,   2.],\n",
      "        [  4.,   2.,   6.,  13.,   0.,   0.,   0.,  10.,   0.,  10.,   2., 141.,\n",
      "           0.,   3.,   0.,   0.,   1.,   7.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   0.,   2.,   0.,   1.,   0.,   1.,   0.,\n",
      "         187.,   0.,   1.,   3.,   0.,   0.,   0.,   2.],\n",
      "        [  0.,   4.,   1.,   0.,   2.,   1.,   1.,   4.,   0.,   6.,   5.,   0.,\n",
      "           1., 143.,  19.,   9.,   2.,   2.,   1.,   0.],\n",
      "        [  1.,   3.,   1.,   1.,   0.,   0.,   2.,   1.,   1.,   6.,  12.,   0.,\n",
      "           1.,  18., 123.,   5.,   1.,   4.,   6.,   9.],\n",
      "        [  0.,   1.,   0.,   3.,   2.,   0.,   1.,   2.,   0.,   1.,   3.,   1.,\n",
      "           0.,  17.,   1., 167.,   0.,   0.,   4.,   4.],\n",
      "        [  6.,   4.,   4.,   2.,   0.,   0.,   0.,   7.,   1.,   0.,   3.,   0.,\n",
      "           0.,   1.,   1.,   0., 175.,   0.,   0.,   0.],\n",
      "        [  1.,   6.,   0.,  14.,   0.,   3.,   0.,  21.,   0.,  14.,  16.,  12.,\n",
      "           0.,   0.,   2.,   1.,   0., 105.,   0.,   3.],\n",
      "        [  0.,   1.,   0.,   0.,   0.,   2.,   5.,   0.,   2.,   2.,   1.,   0.,\n",
      "           0.,   3.,   8.,   4.,   0.,   0., 144.,  24.],\n",
      "        [  0.,   0.,   0.,   3.,   1.,   1.,   2.,   0.,   0.,   1.,   1.,   2.,\n",
      "           1.,   1.,   7.,   4.,   0.,   1.,   3., 165.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[138.,  18.,   2.,   0.,   0.,   0.,   0.,   3.,   0.,   4.,   4.,   7.,\n",
      "           0.,   2.,   2.,   0.,  14.,   0.,   2.,   0.],\n",
      "        [ 12., 122.,   0.,   0.,   0.,   0.,   1.,  10.,   1.,   8.,  13.,   3.,\n",
      "           0.,   6.,   9.,   0.,   9.,   5.,   0.,   0.],\n",
      "        [  2.,   6., 142.,   0.,   0.,   0.,   0.,  10.,   0.,  10.,   7.,   7.,\n",
      "           0.,   1.,   2.,   0.,   8.,   6.,   0.,   0.],\n",
      "        [  0.,   4.,   1., 154.,   0.,   1.,   0.,   4.,   0.,  11.,   5.,   2.,\n",
      "           0.,   1.,   0.,   3.,   0.,   8.,   0.,   3.],\n",
      "        [  0.,   1.,   0.,   0., 192.,   1.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
      "           2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   0.,   2.,   7., 149.,   9.,   1.,  15.,   1.,   0.,   3.,\n",
      "           0.,   1.,   0.,   1.,   1.,   2.,   0.,   5.],\n",
      "        [  0.,   2.,   0.,   0.,   2.,   3., 162.,   0.,   9.,   0.,   0.,   0.,\n",
      "           1.,   0.,   2.,   1.,   0.,   0.,   4.,   3.],\n",
      "        [  0.,   4.,   2.,  10.,   0.,   0.,   0., 160.,   0.,   2.,   7.,   4.,\n",
      "           0.,   1.,   0.,   1.,   3.,  10.,   0.,   0.],\n",
      "        [  0.,   1.,   0.,   2.,   1.,   3.,  10.,   0., 179.,   2.,   0.,   0.,\n",
      "           1.,   0.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  4.,   2.,   6.,   2.,   2.,   0.,   0.,   0.,   2., 143.,   4.,   7.,\n",
      "           0.,   0.,   7.,   1.,   0.,  11.,   0.,   4.],\n",
      "        [  1.,  13.,   4.,   4.,   1.,   1.,   0.,   4.,   1.,   8., 127.,   2.,\n",
      "           0.,  14.,   7.,   1.,   1.,   5.,   0.,   1.],\n",
      "        [  5.,   2.,   8.,   4.,   0.,   0.,   0.,   5.,   0.,   9.,   2., 152.,\n",
      "           0.,   2.,   0.,   0.,   0.,  10.,   0.,   1.],\n",
      "        [  1.,   1.,   0.,   0.,   4.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
      "         189.,   2.,   0.,   1.,   0.,   0.,   1.,   1.],\n",
      "        [  1.,   8.,   1.,   1.,   1.,   1.,   0.,   3.,   0.,   1.,   8.,   0.,\n",
      "           0., 142.,  11.,  13.,   3.,   3.,   2.,   1.],\n",
      "        [  1.,   5.,   1.,   2.,   2.,   0.,   0.,   1.,   0.,   9.,  12.,   0.,\n",
      "           0.,  19., 121.,   4.,   0.,   5.,   5.,   8.],\n",
      "        [  0.,   1.,   0.,   2.,   2.,   0.,   1.,   0.,   1.,   1.,   1.,   0.,\n",
      "           0.,   9.,   2., 177.,   1.,   1.,   3.,   4.],\n",
      "        [  8.,   6.,   2.,   0.,   1.,   0.,   0.,  16.,   0.,   0.,   2.,   2.,\n",
      "           0.,   1.,   1.,   0., 164.,   0.,   0.,   0.],\n",
      "        [  2.,   3.,   1.,  13.,   0.,   0.,   0.,  18.,   2.,   6.,   6.,  15.,\n",
      "           0.,   3.,   1.,   0.,   1., 123.,   0.,   3.],\n",
      "        [  1.,   1.,   0.,   0.,   0.,   0.,   9.,   0.,   1.,   2.,   0.,   0.,\n",
      "           1.,   6.,   5.,   5.,   0.,   0., 147.,  19.],\n",
      "        [  1.,   1.,   0.,   3.,   0.,   0.,   2.,   0.,   2.,   2.,   1.,   2.,\n",
      "           1.,   0.,   8.,   2.,   0.,   1.,  10., 158.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[126.,  14.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,   6.,   6.,\n",
      "           0.,   2.,   1.,   0.,  30.,   0.,   2.,   1.],\n",
      "        [ 11., 126.,   1.,   1.,   0.,   0.,   0.,   5.,   1.,   2.,  21.,   1.,\n",
      "           0.,  12.,   9.,   0.,   6.,   3.,   1.,   0.],\n",
      "        [  6.,   4., 142.,   0.,   0.,   0.,   0.,  10.,   0.,   7.,  13.,   2.,\n",
      "           0.,   0.,   3.,   0.,   8.,   4.,   0.,   0.],\n",
      "        [  0.,   5.,   1., 120.,   0.,   1.,   0.,   5.,   2.,  16.,   9.,   7.,\n",
      "           0.,   3.,   1.,   3.,   0.,  24.,   0.,   1.],\n",
      "        [  0.,   0.,   1.,   0., 178.,   3.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
      "           9.,   0.,   1.,   4.,   0.,   0.,   0.,   0.],\n",
      "        [  4.,   1.,   0.,   1.,   1., 169.,   5.,   0.,   6.,   1.,   2.,   2.,\n",
      "           1.,   1.,   4.,   0.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   0.,   1.,   0.,   1.,  12., 158.,   1.,   5.,   1.,   0.,   0.,\n",
      "           2.,   0.,   1.,   2.,   0.,   0.,   4.,   3.],\n",
      "        [  1.,   7.,   5.,   4.,   0.,   0.,   0., 140.,   0.,   1.,   8.,   6.,\n",
      "           0.,   3.,   0.,   0.,   4.,  23.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   5.,   1.,  15.,   7.,   0., 162.,   6.,   0.,   0.,\n",
      "           2.,   0.,   0.,   0.,   0.,   2.,   0.,   1.],\n",
      "        [  0.,   5.,   8.,   1.,   1.,   1.,   0.,   0.,   0., 144.,  12.,   3.,\n",
      "           0.,   2.,  12.,   0.,   0.,   6.,   0.,   1.],\n",
      "        [  3.,  14.,   1.,   0.,   0.,   1.,   0.,   3.,   0.,   2., 138.,   1.,\n",
      "           0.,  11.,   7.,   1.,   2.,  10.,   0.,   0.],\n",
      "        [  7.,   3.,   5.,   3.,   0.,   0.,   0.,   1.,   0.,  17.,   1., 139.,\n",
      "           0.,   0.,   0.,   0.,   3.,  16.,   0.,   4.],\n",
      "        [  1.,   0.,   0.,   0.,   2.,   0.,   2.,   0.,   1.,   0.,   0.,   0.,\n",
      "         188.,   3.,   0.,   1.,   0.,   0.,   1.,   3.],\n",
      "        [  2.,   7.,   0.,   0.,   0.,   0.,   1.,   1.,   0.,   1.,  11.,   1.,\n",
      "           0., 143.,  18.,   7.,   2.,   4.,   0.,   3.],\n",
      "        [  0.,   4.,   1.,   0.,   0.,   0.,   2.,   0.,   0.,   4.,  13.,   0.,\n",
      "           1.,  13., 139.,   5.,   0.,   5.,   1.,   7.],\n",
      "        [  0.,   0.,   0.,   1.,   1.,   0.,   0.,   2.,   0.,   1.,   2.,   0.,\n",
      "           1.,  11.,   4., 171.,   0.,   0.,   6.,   4.],\n",
      "        [  7.,   3.,   2.,   0.,   0.,   0.,   0.,  10.,   0.,   0.,   1.,   1.,\n",
      "           1.,   2.,   1.,   0., 177.,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   0.,   3.,   0.,   1.,   1.,   9.,   0.,   8.,   8.,  11.,\n",
      "           0.,   0.,   2.,   0.,   1., 152.,   0.,   1.],\n",
      "        [  2.,   0.,   0.,   0.,   0.,   1.,   5.,   0.,   0.,   3.,   1.,   0.,\n",
      "           0.,   2.,  27.,   1.,   0.,   0., 131.,  23.],\n",
      "        [  2.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,   4.,   3.,   1.,\n",
      "           0.,   2.,   9.,   1.,   0.,   2.,   2., 166.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[142.,   8.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,   2.,   2.,\n",
      "           0.,   0.,   4.,   0.,  23.,   0.,   5.,   2.],\n",
      "        [ 16., 113.,   0.,   0.,   0.,   0.,   0.,  11.,   1.,   7.,  17.,   1.,\n",
      "           0.,   1.,  12.,   2.,   9.,   7.,   1.,   0.],\n",
      "        [  4.,   1., 150.,   0.,   0.,   0.,   0.,  12.,   0.,   5.,   9.,   7.,\n",
      "           0.,   0.,   4.,   0.,   5.,   2.,   0.,   1.],\n",
      "        [  0.,   2.,   1., 137.,   1.,   0.,   0.,   7.,   2.,  18.,   6.,  14.,\n",
      "           0.,   1.,   0.,   0.,   0.,   7.,   0.,   3.],\n",
      "        [  0.,   0.,   0.,   0., 189.,   1.,   1.,   0.,   0.,   1.,   0.,   0.,\n",
      "           4.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   0.,   0.,   0.,   5., 137.,  13.,   1.,  31.,   3.,   1.,   0.,\n",
      "           3.,   0.,   3.,   0.,   0.,   0.,   0.,   2.],\n",
      "        [  0.,   1.,   0.,   0.,   3.,   3., 167.,   0.,   4.,   0.,   0.,   0.,\n",
      "           3.,   1.,   1.,   0.,   0.,   0.,   5.,   3.],\n",
      "        [  1.,   5.,   1.,   6.,   0.,   0.,   0., 155.,   0.,   2.,   4.,   7.,\n",
      "           0.,   3.,   0.,   1.,   5.,  14.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   1.,   1.,   4.,   7.,   0., 187.,   0.,   0.,   0.,\n",
      "           1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.],\n",
      "        [  3.,   3.,   4.,  10.,   1.,   0.,   0.,   0.,   4., 139.,   5.,  10.,\n",
      "           0.,   0.,   8.,   2.,   0.,   5.,   1.,   0.],\n",
      "        [  2.,  13.,   4.,   8.,   0.,   0.,   0.,   6.,   0.,  11., 124.,   0.,\n",
      "           0.,   6.,  13.,   1.,   1.,   4.,   0.,   1.],\n",
      "        [  1.,   1.,   5.,   5.,   0.,   0.,   0.,   4.,   0.,  11.,   1., 157.,\n",
      "           0.,   0.,   0.,   0.,   2.,   7.,   0.,   5.],\n",
      "        [  0.,   0.,   0.,   0.,   2.,   0.,   2.,   0.,   1.,   1.,   1.,   0.,\n",
      "         190.,   2.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   5.,   1.,   2.,   1.,   0.,   1.,   3.,   0.,   1.,   9.,   0.,\n",
      "           0., 142.,  17.,  10.,   3.,   3.,   0.,   4.],\n",
      "        [  0.,   6.,   1.,   1.,   2.,   1.,   1.,   1.,   0.,  11.,  10.,   0.,\n",
      "           0.,  16., 121.,   2.,   2.,   3.,  10.,   7.],\n",
      "        [  0.,   0.,   0.,   0.,   6.,   1.,   0.,   2.,   0.,   0.,   3.,   0.,\n",
      "           1.,  11.,   2., 173.,   0.,   0.,   6.,   1.],\n",
      "        [ 15.,   2.,   2.,   1.,   1.,   0.,   0.,   7.,   0.,   0.,   0.,   3.,\n",
      "           0.,   0.,   1.,   0., 171.,   0.,   0.,   0.],\n",
      "        [  2.,   0.,   2.,  11.,   0.,   0.,   0.,  21.,   1.,  12.,   2.,  15.,\n",
      "           0.,   0.,   0.,   1.,   0., 130.,   0.,   1.],\n",
      "        [  0.,   1.,   0.,   0.,   0.,   0.,   8.,   0.,   0.,   1.,   0.,   0.,\n",
      "           1.,   2.,   8.,   3.,   0.,   0., 158.,  13.],\n",
      "        [  0.,   2.,   0.,   1.,   1.,   0.,   2.,   1.,   2.,   4.,   1.,   1.,\n",
      "           0.,   3.,   7.,   3.,   0.,   0.,   6., 160.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139.2  12.2   3.    0.8   0.    0.2   0.    1.2   0.2   4.    4.    4.2\n",
      "    0.    1.6   1.8   0.2  20.2   0.    2.8   0.6]\n",
      " [ 15.  114.8   1.8   1.6   0.    0.2   0.2   7.6   0.6   6.4  19.    1.4\n",
      "    0.    8.    9.8   0.4   7.6   3.8   0.8   0. ]\n",
      " [  5.2   3.6 145.    1.    0.    0.    0.    9.4   0.    9.8   8.6   4.2\n",
      "    0.2   0.8   3.    0.    5.4   3.2   0.    0.4]\n",
      " [  0.4   3.6   1.4 137.    0.2   0.4   0.    6.    1.2  16.8   7.    7.4\n",
      "    0.    1.4   0.4   1.4   0.   10.4   0.    2.2]\n",
      " [  0.    0.2   0.2   0.  188.2   1.2   0.8   0.    0.    0.6   0.4   0.\n",
      "    4.4   0.    0.2   1.    0.2   0.    0.    0. ]\n",
      " [  1.6   0.8   0.6   2.2   3.6 153.6   9.6   0.8  14.8   2.    0.6   1.4\n",
      "    1.2   0.8   2.2   0.4   0.2   0.8   0.4   2.2]\n",
      " [  0.2   0.8   0.4   0.    2.2   7.4 161.    0.4   5.8   0.2   0.    0.2\n",
      "    1.8   0.4   1.2   1.2   0.    0.    4.2   3. ]\n",
      " [  1.6   5.2   7.6   6.4   0.    0.    0.2 146.2   0.2   1.4   5.2   5.4\n",
      "    0.    1.8   0.6   0.4   5.   15.6   0.2   0.2]\n",
      " [  0.2   0.2   0.    2.2   0.8   8.2   8.2   0.  174.8   2.6   0.    0.\n",
      "    1.6   0.2   0.4   0.2   0.    0.8   0.2   0.6]\n",
      " [  2.8   2.6   8.8   6.6   1.    1.2   0.2   0.8   1.6 137.6   6.8   4.8\n",
      "    0.    1.6   8.8   1.    0.    5.6   0.6   2.6]\n",
      " [  3.   12.8   4.    5.    0.2   0.6   0.    3.8   0.2   6.  126.6   1.\n",
      "    0.   11.8  10.8   1.6   1.    4.6   0.    1.4]\n",
      " [  5.4   1.8   5.8   6.6   0.    0.    0.    5.    0.2  12.6   2.  142.6\n",
      "    0.2   1.2   0.4   0.    1.6  10.8   0.    3. ]\n",
      " [  0.6   0.2   0.    0.2   3.    0.    2.4   0.    1.    0.2   0.6   0.\n",
      "  187.4   1.8   0.2   1.4   0.    0.2   0.8   1.6]\n",
      " [  1.2   5.4   0.8   1.2   0.8   0.4   0.8   2.8   0.    2.6   7.6   0.2\n",
      "    0.4 140.8  16.4  11.8   2.2   2.6   1.6   1.6]\n",
      " [  1.    4.8   1.    1.    1.    0.6   1.2   0.8   0.2   8.   11.2   0.\n",
      "    0.6  17.  123.4   4.4   0.6   3.4   6.8   8. ]\n",
      " [  0.2   0.4   0.2   1.6   2.8   0.2   0.6   1.6   0.4   0.6   2.2   0.4\n",
      "    0.4  12.8   2.2 170.6   0.2   0.2   5.    3.4]\n",
      " [ 11.8   4.2   2.6   0.6   0.4   0.    0.   10.    0.2   0.    1.4   1.4\n",
      "    0.2   1.    1.    0.2 168.6   0.    0.2   0. ]\n",
      " [  1.4   2.6   1.2  10.    0.    1.    0.2  17.4   0.8  13.6   7.8  14.2\n",
      "    0.    0.8   1.4   0.4   0.8 122.    0.2   2.2]\n",
      " [  0.8   0.8   0.    0.    0.2   0.8   6.2   0.    0.8   2.    0.4   0.\n",
      "    0.4   3.2  11.    3.    0.4   0.  147.6  18.6]\n",
      " [  0.6   1.2   0.    1.8   0.4   0.2   1.8   0.2   1.    2.4   1.2   1.6\n",
      "    0.4   2.4   7.6   2.6   0.    1.2   8.4 158.4]]\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for conf in all_confusion:\n",
    "    temp.append(np.array(conf))\n",
    "print(np.mean(np.array(temp), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def displayConfMat(confusion_matrix, save_file_name):\n",
    "    #confusion_matrix = confusion_matrix.numpy()\n",
    "    #print(confusion_matrix)\n",
    "    font = {'size'   : 3.5}\n",
    "    plt.rc('font', **font)\n",
    "    figure(num=None, figsize=(1080, 1080), dpi=300, facecolor='w', edgecolor='k')\n",
    "    plt_conf = ConfusionMatrixDisplay(confusion_matrix=np.array(confusion_matrix),\n",
    "                                  display_labels=np.array(classes))\n",
    "    plt_conf.plot(xticks_rotation='vertical', cmap='Blues',values_format='.5g')\n",
    "    plt.gcf().subplots_adjust(bottom=0.19)\n",
    "    plt_conf.figure_.savefig(save_file_name, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Results for CNNLSTM - best case\n",
      "Accuracy: 0.76663\n",
      "F1: 0.76798\n",
      "===============================\n",
      "===============================\n",
      "Results for CNNLSTM - worst case\n",
      "Accuracy: 0.71094\n",
      "F1: 0.71285\n",
      "===============================\n",
      "===============================\n",
      "Results for CNNLSTM - avg case\n",
      "Accuracy: 0.75237\n",
      "F1: 0.75493\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 20\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - best case\")\n",
    "best = np.argmax(test_scores)\n",
    "print(\"Accuracy:\", round(test_scores[best], 5))\n",
    "print(\"F1:\", round(all_f1[best], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_confusion[best], \"montalbano_snapture_best_30_70_4_9_2021.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - worst case\")\n",
    "worst = np.argmin(test_scores)\n",
    "print(\"Accuracy:\", round(test_scores[worst], 5))\n",
    "print(\"F1:\", round(all_f1[worst], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_confusion[worst], \"montalbano_snapture_worst_30_70_4_9_2021.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - avg case\")\n",
    "worst = np.mean(test_scores)\n",
    "print(\"Accuracy:\", round(np.mean(test_scores), 5))\n",
    "print(\"F1:\", round(np.mean(all_f1), 5))\n",
    "print(\"===============================\")\n",
    "temp = []\n",
    "for conf in all_confusion:\n",
    "    temp.append(np.array(conf))\n",
    "#print(np.mean(np.array(temp), axis=0))\n",
    "displayConfMat(np.mean(np.array(temp), axis=0), \"montalbano_snapture_avg_30_70_4_9_2021.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(model.state_dict(), \"/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(all_models[0].state_dict(), \"/model1_4_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(all_models[1].state_dict(), \"/model2_4_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(all_models[2].state_dict(), \"/model3_4_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(all_models[3].state_dict(), \"/model4_4_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(all_models[4].state_dict(), \"/model5_4_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(img1, img2):\n",
    "    err = np.sum((img1.astype(\"float\") - img2.astype(\"float\")) ** 2)\n",
    "    err /= float(img1.shape[0] * img2.shape[1])\n",
    "    return err\n",
    "\n",
    "def diffImg(t0, t1):\n",
    "    return cv2.absdiff(t0.cpu().numpy(), t1.cpu().numpy())\n",
    "\n",
    "def getdiffDir(imgs):\n",
    "    #print(np.shape(imgs))\n",
    "    diff = []\n",
    "    all_ssim = []\n",
    "    all_mse = []\n",
    "    t = diffImg(imgs[0], imgs[1])\n",
    "    for i, img in enumerate(imgs[2:-1]):\n",
    "        #print(\"ssim\", ssim(img, imgs[0]))\n",
    "        im = diffImg(imgs[i-1], img)\n",
    "        t += im\n",
    "        all_ssim.append(1 - ssim(img.cpu().numpy().reshape(64,48), imgs[0].cpu().numpy().reshape(64,48)))\n",
    "        all_mse.append(mse(img.cpu().numpy().reshape(64,48), imgs[0].cpu().numpy().reshape(64,48)))\n",
    "    diff.append(t)\n",
    "    return all_ssim, all_mse, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence level\n",
    "class CNNLSTMwPause(Module):        \n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=500, \n",
    "            hidden_size=512,\n",
    "            hidden_size_snapture=512,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            num_units=512,\n",
    "            num_units_snapture=1012,\n",
    "            num_classes=20,\n",
    "            snapture=True\n",
    "    ):\n",
    "        super(CNNLSTMwPause, self).__init__()\n",
    "        #cnn for static part\n",
    "        if snapture:\n",
    "            #cnn for frames\n",
    "            self.snap = Snap().to('cuda:0')\n",
    "            self.snap.double()\n",
    "    \n",
    "        #cnn for frames\n",
    "        self.cnn = CNN().to('cuda:0')\n",
    "        self.cnn.double()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.dropout1.double()\n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.num_units=num_units\n",
    "        self.num_classes=num_classes\n",
    "        self.snapture=snapture\n",
    "        \n",
    "        self.rnn = LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first).to('cuda:0')\n",
    "        \n",
    "        weights_init(self.rnn)\n",
    "      \n",
    "        self.rnn.double()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.dropout2.double()\n",
    "        \n",
    "        self.act3 = Tanh()\n",
    "        self.linearCNNLSTMONLY = Linear(512,num_classes).to('cuda:0')\n",
    "        self.linearCNNLSTMONLY.double()\n",
    "        self.linear2 = Linear(num_units_snapture,hidden_size_snapture).to('cuda:0')\n",
    "        self.linear2.double()\n",
    "        self.dropout3 = nn.Dropout()\n",
    "        self.dropout3.double()\n",
    "        self.act4 = Tanh()\n",
    "        self.linear3 = Linear(hidden_size_snapture,num_classes).to('cuda:0')\n",
    "        self.linear3.double()\n",
    "\n",
    "    def forward(self, x, gesture_peak):\n",
    "        x = x.contiguous()\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        c_out = self.dropout1(c_out)\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out = self.dropout2(r_out)\n",
    "        \n",
    "        use_snapture=np.ones((20), dtype=bool)\n",
    "        for use in use_snapture:\n",
    "            for sequence in range(len(x)):\n",
    "                seq = x[sequence]\n",
    "\n",
    "                kendon_stroke_index = round(len(seq) / 2)\n",
    "                seq = seq[kendon_stroke_index-2:kendon_stroke_index+2]\n",
    "                all_error = []\n",
    "                all_ssim, all_mse, diff = getdiffDir(seq)\n",
    "                if (np.mean(all_ssim) < 0.027 ):\n",
    "                    use_snapture[use] = True\n",
    "                else:\n",
    "                    use_snapture[use] = False\n",
    "\n",
    "        r_out2 = self.linearCNNLSTMONLY(r_out[:, -1, :])        \n",
    "        \n",
    "        for iitem, item in enumerate(use_snapture):\n",
    "            if item:\n",
    "                gesture_peak_maps = self.snap(gesture_peak)\n",
    "                gesture_peak_maps = torch.cat((r_out[:, -1, :], gesture_peak_maps), dim=1)\n",
    "                temp = self.linear2(gesture_peak_maps[iitem])\n",
    "                temp= self.act4(temp)\n",
    "                temp = self.dropout3(temp)\n",
    "                r_out2[iitem] = self.linear3(temp)\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMwPause(\n",
      "  (snap): Snap(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(11, 11), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=770, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(11, 11), stride=(1, 1))\n",
      "    (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=770, out_features=500, bias=True)\n",
      "    (bn3): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (rnn): LSTM(500, 512, num_layers=2, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (act3): Tanh()\n",
      "  (linearCNNLSTMONLY): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (linear2): Linear(in_features=1012, out_features=512, bias=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (act4): Tanh()\n",
      "  (linear3): Linear(in_features=512, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "modelwpause = CNNLSTMwPause()\n",
    "modelwpause.to('cuda:0')\n",
    "optimizer = Adam(modelwpause.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(modelwpause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_montalwpause(num_trials, cv_split):\n",
    "    num_epochs = 100\n",
    "    cv_result_extended = []\n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    \n",
    "    run_times = []\n",
    "    test_scores = []\n",
    "    all_confusion = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    loss_list = []\n",
    "    acc_per_epoch = []\n",
    "    all_models = []\n",
    "\n",
    "    for i in range(num_trials):\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0,  drop_last=True, )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=0,  drop_last=True,)\n",
    "        modelwpause = CNNLSTMwPause()\n",
    "        modelwpause = modelwpause.double()\n",
    "        optimizer = Adam(modelwpause.parameters(), lr=0.001)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        start = time.process_time() \n",
    "        print(device)\n",
    "        loss_list, val_losses, this_model = train_model(modelwpause, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "        a, b, c, d = test_model(modelwpause, train_loader, device)\n",
    "        acc, preds, labels, confusion_matrix = test_model(modelwpause, test_loader, device)\n",
    "        test_scores.append(acc)\n",
    "        run_times.append(time.process_time() - start)\n",
    "        pred_history.append(preds)\n",
    "        true_history.append(labels)\n",
    "        all_confusion.append(confusion_matrix)\n",
    "        all_models.append(this_model)\n",
    "\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch :  1 \t loss : tensor(2.3777, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.8073, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(1.2656, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(1.2294, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.8171, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.8553, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.5930, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.2950, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.3427, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1480, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.2646, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.2902, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.2139, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.2555, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.1094, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.1060, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.1323, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.0906, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.1527, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.1692, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0500, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.0727, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0526, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.0633, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0842, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.1173, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0428, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  55 \t loss : tensor(0.0389, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  57 \t loss : tensor(0.0744, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  59 \t loss : tensor(0.1028, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  61 \t loss : tensor(0.0299, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  63 \t loss : tensor(0.0386, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  65 \t loss : tensor(0.0854, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  67 \t loss : tensor(0.0204, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  69 \t loss : tensor(0.1287, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  71 \t loss : tensor(0.0191, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  73 \t loss : tensor(0.1252, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  75 \t loss : tensor(0.0513, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  77 \t loss : tensor(0.0332, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  79 \t loss : tensor(0.0525, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  81 \t loss : tensor(0.1286, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  83 \t loss : tensor(0.0471, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  85 \t loss : tensor(0.0299, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  87 \t loss : tensor(0.1159, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  89 \t loss : tensor(0.1118, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  91 \t loss : tensor(0.0153, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  93 \t loss : tensor(0.0637, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  95 \t loss : tensor(0.0994, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  97 \t loss : tensor(0.0060, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  99 \t loss : tensor(0.0764, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Test Accuracy of the model: 98.83\n",
      "Test Accuracy of the model: 98.83\n",
      "Test Accuracy of the model: 74.75\n",
      "Test Accuracy of the model: 74.75\n",
      "ho\n",
      "0\n",
      "Epoch :  1 \t loss : tensor(2.0744, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.2037, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.8579, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.6375, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  9 \t loss : tensor(0.3866, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  11 \t loss : tensor(0.3962, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  13 \t loss : tensor(0.2484, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  15 \t loss : tensor(0.0492, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  17 \t loss : tensor(0.2644, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  19 \t loss : tensor(0.1845, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  21 \t loss : tensor(0.1795, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  23 \t loss : tensor(0.1864, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  25 \t loss : tensor(0.0818, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  27 \t loss : tensor(0.1595, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  29 \t loss : tensor(0.0336, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  31 \t loss : tensor(0.0976, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  33 \t loss : tensor(0.2625, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  35 \t loss : tensor(0.1997, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  37 \t loss : tensor(0.0547, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  39 \t loss : tensor(0.0566, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  41 \t loss : tensor(0.0579, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  43 \t loss : tensor(0.0616, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  45 \t loss : tensor(0.0497, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  47 \t loss : tensor(0.0388, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  49 \t loss : tensor(0.0588, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  51 \t loss : tensor(0.0787, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  53 \t loss : tensor(0.0304, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4757564f908d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_times_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scores_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_history_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_history_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_confusion_w_pause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_models_w_pause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnnlstm_montalwpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-048d030e715d>\u001b[0m in \u001b[0;36mcnnlstm_montalwpause\u001b[0;34m(num_trials, cv_split)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelwpause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#should be val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblah\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelwpause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelwpause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-18734ff10d3b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_epochs, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Backprop and perform Adam optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/7ali/gproj/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/7ali/gproj/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 5\n",
    "run_times_w_pause, test_scores_w_pause, pred_history_w_pause, true_history_w_pause, loss_list_w_pause, val_losses_w_pause, a_w_pause, all_confusion_w_pause, all_models_w_pause = cnnlstm_montalwpause(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we interrupted the last one sine it takes too long to estimate the pause at run time\n",
    "# we run again with pre pause estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence level\n",
    "class CNNLSTMPause(Module):        \n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=500, \n",
    "            hidden_size=512,\n",
    "            hidden_size_snapture=512,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            num_units=512,\n",
    "            num_units_snapture=1012,\n",
    "            num_classes=20,\n",
    "            snapture=True\n",
    "    ):\n",
    "        super(CNNLSTMPause, self).__init__()\n",
    "        #cnn for static part\n",
    "        if snapture:\n",
    "            #cnn for frames\n",
    "            self.snap = Snap().to('cuda:0')\n",
    "            self.snap.double()\n",
    "    \n",
    "        #cnn for frames\n",
    "        self.cnn = CNN().to('cuda:0')\n",
    "        self.cnn.double()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.dropout1.double()\n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.num_units=num_units\n",
    "        self.num_classes=num_classes\n",
    "        self.snapture=snapture\n",
    "        \n",
    "        self.rnn = LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first).to('cuda:0')\n",
    "        \n",
    "        weights_init(self.rnn)\n",
    "      \n",
    "        self.rnn.double()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.dropout2.double()\n",
    "        \n",
    "        self.act3 = Tanh()\n",
    "        self.linearCNNLSTMONLY = Linear(512,num_classes).to('cuda:0')\n",
    "        self.linearCNNLSTMONLY.double()\n",
    "        \n",
    "        self.linear2 = Linear(num_units_snapture,hidden_size_snapture).to('cuda:0')\n",
    "        self.linear2.double()\n",
    "        self.dropout3 = nn.Dropout()\n",
    "        self.dropout3.double()\n",
    "        self.act4 = Tanh()\n",
    "        self.linear3 = Linear(hidden_size_snapture,num_classes).to('cuda:0')\n",
    "        self.linear3.double()\n",
    "\n",
    "    def forward(self, x, gesture_peak, use_snapture):\n",
    "        x = x.contiguous()\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        c_out = self.dropout1(c_out)\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out = self.dropout2(r_out)\n",
    "        \n",
    "        r_out2 = self.linearCNNLSTMONLY(r_out[:, -1, :])        \n",
    "        \n",
    "        for iitem, item in enumerate(use_snapture):\n",
    "            if item:\n",
    "                gesture_peak_maps = self.snap(gesture_peak)\n",
    "                gesture_peak_maps = torch.cat((r_out[:, -1, :], gesture_peak_maps), dim=1)\n",
    "                temp = self.linear2(gesture_peak_maps[iitem])\n",
    "                temp= self.act4(temp)\n",
    "                temp = self.dropout3(temp)\n",
    "                r_out2[iitem] = self.linear3(temp)\n",
    "                \n",
    "        return F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMPause(\n",
      "  (snap): Snap(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(11, 11), stride=(1, 1))\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=770, out_features=500, bias=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (cnn): CNN(\n",
      "    (conv1): Conv2d(1, 5, kernel_size=(11, 11), stride=(1, 1))\n",
      "    (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act1): Tanh()\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(5, 10, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act2): Tanh()\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (fc1): Linear(in_features=770, out_features=500, bias=True)\n",
      "    (bn3): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act3): Tanh()\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (rnn): LSTM(500, 512, num_layers=2, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (act3): Tanh()\n",
      "  (linearCNNLSTMONLY): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (linear2): Linear(in_features=1012, out_features=512, bias=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (act4): Tanh()\n",
      "  (linear3): Linear(in_features=512, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = CNNLSTMPause()\n",
    "model.to('cuda:0')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        #for i, (images, labels, paths, labelindex, kendon, names) in enumerate(train_loader):\n",
    "        for i, train_data in enumerate(train_loader):\n",
    "            images, labels, gesture_peak, use_snapture = train_data[0].to('cuda:0'), train_data[1].to('cuda:0'), train_data[4].to('cuda:0'), train_data[5].to('cuda:0')\n",
    "            outputs = model(images.double(), gesture_peak.double(), use_snapture.double()).to('cuda:0') #statless\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(float(loss.item()))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%2 == 0:\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', loss, '\\t', 'val_loss :', 0)\n",
    "    return loss_list, val_losses, model\n",
    "    \n",
    "    \n",
    "def test_model(model, test_loader, device):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = torch.zeros(20, 20)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data in test_loader:\n",
    "            data, labels, gesture_peak, use_snapture = test_data[0].to('cuda:0'), test_data[1].to('cuda:0'), test_data[4].to('cuda:0'), test_data[5].to('cuda:0')\n",
    "            predictions = model(data.double(), gesture_peak.double(), use_snapture.double()).to('cuda:0') #statelss\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc_on_test = float(num_correct)/float(total)\n",
    "    \n",
    "    print(f\"Test Accuracy of the model: {acc_on_test*100:.2f}\")\n",
    "    return acc_on_test, [], [], confusion_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm_montal_pause(num_trials, cv_split):\n",
    "    num_epochs = 100\n",
    "    cv_result_extended = []\n",
    "    test_scores = []\n",
    "    run_times = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    \n",
    "    run_times = []\n",
    "    test_scores = []\n",
    "    all_confusion = []\n",
    "    pred_history = []\n",
    "    true_history = []\n",
    "    loss_list = []\n",
    "    acc_per_epoch = []\n",
    "    all_models = []\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0,  drop_last=True, )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=0,  drop_last=True,)\n",
    "        model = CNNLSTMPause()\n",
    "        model = model.double()\n",
    "        optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        start = time.process_time() \n",
    "        print(device)\n",
    "        loss_list, val_losses, this_model = train_model(model, optimizer, num_epochs, train_loader, test_loader, device)#should be val\n",
    "        a, b, c, d = test_model(model, train_loader, device)\n",
    "        acc, preds, labels, confusion_matrix = test_model(model, test_loader, device)\n",
    "        test_scores.append(acc)\n",
    "        run_times.append(time.process_time() - start)\n",
    "        pred_history.append(preds)\n",
    "        true_history.append(labels)\n",
    "        all_confusion.append(confusion_matrix)\n",
    "        all_models.append(this_model)\n",
    "\n",
    "    return run_times, test_scores, pred_history, true_history, loss_list, val_losses, acc, all_confusion, all_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch :  1 \t loss : tensor(1.8171, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  3 \t loss : tensor(1.2976, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  5 \t loss : tensor(0.6462, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n",
      "Epoch :  7 \t loss : tensor(0.6452, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>) \t val_loss : 0\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "cv_split = 5\n",
    "run_times_pause, test_scores_pause, pred_history_pause, true_history_pause, loss_list_pause, val_losses_pause, a_pause, all_confusion_pause, all_models_pause = cnnlstm_montal_pause(num_trials, cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7585685483870968\n",
      "[0.7626008064516129, 0.7779737903225806, 0.7734375, 0.7779737903225806, 0.7585685483870968]\n",
      "0.7701108870967742\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "print(a_pause)\n",
    "print(test_scores_pause)\n",
    "print(np.mean(test_scores_pause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test scores  0.7701108870967742\n",
      "std test scores  0.008054113754727428\n",
      "best case test scores  0.7779737903225806\n",
      "worst case test scores  0.7585685483870968\n"
     ]
    }
   ],
   "source": [
    "print(\"mean test scores \", np.mean(test_scores_pause))\n",
    "print(\"std test scores \", np.std(test_scores_pause))\n",
    "print(\"best case test scores \", np.max(test_scores_pause))\n",
    "print(\"worst case test scores \", np.min(test_scores_pause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44664.291611921, 44779.30118971701, 44657.74700256699, 44690.81897405902, 44693.77633712199]\n",
      "mean training time:  44697.187023077204\n",
      "std training time:  43.43396017614419\n"
     ]
    }
   ],
   "source": [
    "print(run_times_pause)\n",
    "print(\"mean training time: \", np.mean(run_times_pause))\n",
    "print(\"std training time: \", np.std(run_times_pause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[163.,   5.,   4.,   2.,   0.,   0.,   0.,   2.,   0.,   3.,   3.,   3.,\n",
      "           0.,   0.,   3.,   0.,   5.,   0.,   1.,   1.],\n",
      "        [ 17.,  97.,   2.,   0.,   0.,   0.,   0.,  10.,   0.,   5.,  36.,   1.,\n",
      "           0.,   5.,  17.,   0.,   4.,   5.,   0.,   0.],\n",
      "        [  6.,   1., 158.,   0.,   1.,   0.,   0.,   2.,   0.,  15.,   8.,   2.,\n",
      "           0.,   1.,   2.,   0.,   2.,   3.,   0.,   0.],\n",
      "        [  0.,   0.,   1., 163.,   0.,   1.,   0.,   5.,   1.,  13.,   4.,   2.,\n",
      "           0.,   0.,   1.,   0.,   0.,   8.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   1., 187.,   2.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
      "           2.,   1.,   1.,   2.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   4.,   4.,   2., 156.,   4.,   0.,  17.,   3.,   2.,   0.,\n",
      "           0.,   0.,   2.,   0.,   0.,   3.,   0.,   3.],\n",
      "        [  0.,   0.,   1.,   1.,   0.,  10., 162.,   0.,   7.,   0.,   0.,   0.,\n",
      "           2.,   1.,   1.,   1.,   0.,   0.,   2.,   3.],\n",
      "        [  3.,   3.,   8.,   8.,   0.,   0.,   0., 148.,   0.,   2.,  10.,   2.,\n",
      "           0.,   0.,   1.,   0.,   2.,  16.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   4.,   2.,   5.,   3.,   0., 182.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   1.,   0.,   2.,   0.,   1.],\n",
      "        [  1.,   1.,   8.,   6.,   0.,   2.,   0.,   1.,   0., 160.,   5.,   5.,\n",
      "           0.,   0.,   3.,   0.,   0.,   4.,   0.,   0.],\n",
      "        [  1.,   9.,   5.,   7.,   1.,   0.,   0.,   3.,   0.,  11., 132.,   1.,\n",
      "           0.,   4.,  14.,   2.,   0.,   5.,   0.,   0.],\n",
      "        [  8.,   2.,   6.,  15.,   0.,   0.,   0.,   7.,   0.,  24.,   1., 128.,\n",
      "           0.,   0.,   0.,   0.,   1.,   6.,   0.,   2.],\n",
      "        [  0.,   0.,   1.,   0.,   9.,   0.,   2.,   0.,   1.,   0.,   0.,   0.,\n",
      "         187.,   0.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  1.,   8.,   1.,   4.,   2.,   1.,   0.,   2.,   1.,   2.,   7.,   0.,\n",
      "           0., 130.,  25.,   7.,   1.,   5.,   2.,   2.],\n",
      "        [  2.,   9.,   3.,   2.,   1.,   0.,   1.,   0.,   0.,   4.,  14.,   0.,\n",
      "           1.,   8., 124.,   1.,   1.,   7.,   6.,  12.],\n",
      "        [  0.,   0.,   2.,   5.,   1.,   0.,   0.,   0.,   1.,   1.,   2.,   0.,\n",
      "           0.,  12.,   6., 166.,   0.,   0.,   6.,   2.],\n",
      "        [ 27.,   7.,   7.,   0.,   0.,   0.,   0.,   4.,   0.,   0.,   0.,   2.,\n",
      "           0.,   0.,   1.,   0., 154.,   1.,   0.,   0.],\n",
      "        [  1.,   4.,   1.,  11.,   0.,   1.,   0.,  10.,   0.,  20.,   6.,   5.,\n",
      "           0.,   0.,   2.,   0.,   1., 133.,   0.,   1.],\n",
      "        [  0.,   2.,   1.,   0.,   0.,   0.,   8.,   0.,   0.,   2.,   0.,   0.,\n",
      "           2.,   1.,  13.,   2.,   0.,   0., 144.,  21.],\n",
      "        [  0.,   2.,   0.,   4.,   0.,   1.,   1.,   0.,   1.,   6.,   0.,   3.,\n",
      "           0.,   1.,   9.,   4.,   0.,   3.,   7., 152.]]), tensor([[156.,   9.,   2.,   1.,   0.,   0.,   0.,   2.,   0.,   4.,   1.,   6.,\n",
      "           0.,   0.,   1.,   0.,  12.,   1.,   1.,   0.],\n",
      "        [ 11., 120.,   3.,   2.,   0.,   0.,   0.,   9.,   0.,   8.,  22.,   0.,\n",
      "           0.,   4.,   4.,   0.,   7.,   7.,   0.,   2.],\n",
      "        [  8.,   4., 142.,   1.,   0.,   0.,   0.,   7.,   0.,  12.,   8.,   3.,\n",
      "           0.,   0.,   2.,   1.,   6.,   7.,   0.,   0.],\n",
      "        [  0.,   4.,   0., 151.,   0.,   0.,   0.,   5.,   0.,  13.,   5.,   7.,\n",
      "           0.,   0.,   1.,   1.,   0.,  12.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 192.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           4.,   0.,   1.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  2.,   0.,   0.,   3.,   1., 151.,  10.,   0.,  15.,   2.,   0.,   3.,\n",
      "           1.,   0.,   5.,   0.,   0.,   1.,   0.,   5.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   2., 173.,   0.,   5.,   0.,   0.,   0.,\n",
      "           0.,   0.,   2.,   0.,   0.,   0.,   3.,   2.],\n",
      "        [  1.,   3.,   6.,  10.,   0.,   0.,   0., 142.,   0.,   0.,   7.,   4.,\n",
      "           0.,   2.,   1.,   1.,   6.,  20.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   1.,   1.,   4.,   5.,   0., 186.,   1.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   1.,   0.,   2.],\n",
      "        [  3.,   3.,   7.,   5.,   1.,   0.,   0.,   0.,   2., 146.,   6.,   5.,\n",
      "           1.,   0.,   3.,   1.,   0.,   8.,   1.,   2.],\n",
      "        [  6.,  14.,   4.,   5.,   0.,   0.,   0.,   1.,   0.,   6., 130.,   2.,\n",
      "           0.,   8.,   7.,   2.,   2.,   7.,   0.,   0.],\n",
      "        [  4.,   1.,   4.,   6.,   0.,   1.,   0.,   7.,   0.,  13.,   2., 148.,\n",
      "           0.,   1.,   0.,   0.,   2.,  11.,   0.,   0.],\n",
      "        [  0.,   0.,   1.,   0.,   3.,   0.,   3.,   0.,   0.,   0.,   0.,   0.,\n",
      "         193.,   0.,   2.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   9.,   0.,   4.,   2.,   0.,   1.,   2.,   0.,   1.,   7.,   0.,\n",
      "           1., 146.,  10.,   7.,   4.,   1.,   4.,   2.],\n",
      "        [  1.,   8.,   1.,   0.,   1.,   0.,   1.,   0.,   1.,   4.,   8.,   1.,\n",
      "           1.,  10., 124.,   2.,   1.,  10.,  11.,   8.],\n",
      "        [  0.,   0.,   1.,   0.,   5.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,\n",
      "           0.,  11.,   2., 171.,   0.,   1.,   6.,   7.],\n",
      "        [ 10.,   5.,   4.,   0.,   0.,   0.,   0.,  12.,   0.,   0.,   0.,   3.,\n",
      "           0.,   0.,   1.,   0., 168.,   0.,   1.,   0.],\n",
      "        [  4.,   4.,   2.,  14.,   0.,   1.,   1.,   4.,   1.,  11.,   7.,  11.,\n",
      "           0.,   1.,   1.,   0.,   0., 132.,   0.,   2.],\n",
      "        [  2.,   0.,   0.,   0.,   0.,   0.,   6.,   0.,   0.,   1.,   1.,   1.,\n",
      "           0.,   2.,   8.,   3.,   0.,   0., 159.,  14.],\n",
      "        [  1.,   2.,   0.,   3.,   0.,   0.,   3.,   0.,   1.,   6.,   0.,   1.,\n",
      "           1.,   1.,   3.,   1.,   0.,   6.,   7., 157.]]), tensor([[142.,  11.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,   3.,\n",
      "           0.,   1.,   2.,   0.,  27.,   0.,   2.,   0.],\n",
      "        [ 12., 121.,   2.,   1.,   0.,   0.,   0.,   7.,   0.,   2.,  23.,   2.,\n",
      "           0.,   8.,   6.,   0.,   9.,   7.,   0.,   0.],\n",
      "        [  3.,   2., 154.,   0.,   0.,   0.,   0.,  15.,   0.,   3.,  10.,   1.,\n",
      "           0.,   1.,   1.,   0.,   5.,   5.,   0.,   0.],\n",
      "        [  0.,   5.,   1., 139.,   0.,   1.,   0.,  15.,   0.,   9.,   7.,   6.,\n",
      "           1.,   2.,   0.,   1.,   1.,  10.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0., 192.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
      "           2.,   0.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   3.,   2., 160.,   8.,   0.,   8.,   3.,   0.,   1.,\n",
      "           1.,   0.,   2.,   0.,   0.,   6.,   0.,   4.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   7., 167.,   0.,   5.,   1.,   1.,   0.,\n",
      "           0.,   0.,   1.,   1.,   0.,   0.,   1.,   2.],\n",
      "        [  2.,   8.,   5.,   5.,   0.,   0.,   0., 160.,   0.,   0.,   3.,   4.,\n",
      "           0.,   1.,   1.,   1.,   7.,   7.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,  11.,   2.,   2., 181.,   2.,   0.,   0.,\n",
      "           1.,   0.,   0.,   1.,   1.,   1.,   0.,   0.],\n",
      "        [  1.,   7.,   7.,   4.,   1.,   2.,   0.,   1.,   0., 127.,  14.,   7.,\n",
      "           0.,   0.,   2.,   1.,   0.,  16.,   3.,   2.],\n",
      "        [  4.,  24.,   4.,   3.,   2.,   2.,   0.,   2.,   0.,   2., 124.,   1.,\n",
      "           0.,   7.,   9.,   5.,   0.,   5.,   0.,   0.],\n",
      "        [  2.,   1.,   5.,   6.,   0.,   0.,   0.,   9.,   0.,   8.,   2., 150.,\n",
      "           0.,   0.,   0.,   1.,   3.,  10.,   0.,   1.],\n",
      "        [  1.,   0.,   0.,   0.,   7.,   2.,   1.,   0.,   1.,   0.,   1.,   0.,\n",
      "         184.,   0.,   0.,   1.,   1.,   0.,   0.,   1.],\n",
      "        [  1.,   6.,   1.,   2.,   1.,   2.,   2.,   2.,   0.,   1.,   4.,   0.,\n",
      "           0., 144.,  11.,  16.,   1.,   0.,   4.,   2.],\n",
      "        [  1.,  10.,   3.,   1.,   1.,   1.,   3.,   0.,   0.,   3.,  11.,   1.,\n",
      "           0.,  13., 118.,   5.,   0.,  10.,   9.,   5.],\n",
      "        [  1.,   1.,   0.,   0.,   2.,   0.,   1.,   0.,   0.,   1.,   3.,   0.,\n",
      "           1.,   6.,   1., 183.,   0.,   0.,   4.,   2.],\n",
      "        [  3.,   3.,   3.,   0.,   0.,   0.,   0.,   8.,   0.,   0.,   0.,   0.,\n",
      "           0.,   1.,   0.,   0., 187.,   0.,   0.,   0.],\n",
      "        [  5.,   2.,   2.,  16.,   0.,   2.,   0.,  25.,   0.,   4.,   1.,   9.,\n",
      "           0.,   1.,   0.,   0.,   1., 130.,   0.,   1.],\n",
      "        [  1.,   0.,   0.,   0.,   0.,   0.,   7.,   1.,   0.,   1.,   2.,   0.,\n",
      "           0.,   2.,   5.,   4.,   0.,   0., 158.,  15.],\n",
      "        [  1.,   1.,   0.,   2.,   0.,   0.,   2.,   0.,   5.,   2.,   3.,   5.,\n",
      "           0.,   1.,  12.,   6.,   0.,   1.,   4., 148.]]), tensor([[158.,   5.,   6.,   1.,   0.,   0.,   0.,   0.,   0.,   3.,   1.,   1.,\n",
      "           1.,   1.,   2.,   0.,  12.,   1.,   3.,   0.],\n",
      "        [ 18., 116.,   1.,   2.,   0.,   1.,   0.,   5.,   0.,   3.,  20.,   1.,\n",
      "           0.,   8.,   9.,   1.,   4.,   9.,   0.,   2.],\n",
      "        [  5.,   1., 142.,   3.,   1.,   1.,   0.,   7.,   0.,  12.,   7.,   4.,\n",
      "           0.,   2.,   1.,   0.,   6.,   7.,   0.,   1.],\n",
      "        [  0.,   5.,   0., 152.,   0.,   0.,   0.,   4.,   1.,  17.,   3.,   5.,\n",
      "           0.,   0.,   1.,   1.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 190.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
      "           4.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   1.,   0.,   2.,   3., 157.,  11.,   0.,  19.,   2.,   1.,   0.,\n",
      "           2.,   0.,   0.,   0.,   0.,   2.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0.,   2.,   7., 169.,   0.,   6.,   0.,   0.,   0.,\n",
      "           2.,   0.,   1.,   1.,   0.,   0.,   1.,   2.],\n",
      "        [  2.,  12.,   3.,  10.,   0.,   0.,   0., 133.,   0.,   2.,   7.,   6.,\n",
      "           0.,   3.,   0.,   0.,   3.,  21.,   1.,   1.],\n",
      "        [  0.,   0.,   0.,   2.,   1.,   5.,   7.,   0., 183.,   0.,   0.,   0.,\n",
      "           1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   3.,   6.,   4.,   1.,   1.,   1.,   0.,   1., 147.,   9.,   9.,\n",
      "           0.,   1.,   1.,   0.,   0.,   7.,   1.,   4.],\n",
      "        [  5.,  14.,   3.,   4.,   1.,   0.,   0.,   3.,   0.,  10., 119.,   1.,\n",
      "           0.,   5.,  12.,   2.,   1.,  14.,   0.,   0.],\n",
      "        [  5.,   1.,   2.,   5.,   0.,   0.,   0.,   2.,   1.,  12.,   0., 158.,\n",
      "           0.,   0.,   0.,   0.,   2.,   9.,   1.,   2.],\n",
      "        [  0.,   0.,   0.,   0.,   1.,   1.,   1.,   0.,   1.,   0.,   0.,   0.,\n",
      "         193.,   0.,   0.,   2.,   0.,   0.,   1.,   1.],\n",
      "        [  1.,   7.,   0.,   3.,   2.,   1.,   1.,   3.,   0.,   3.,   6.,   1.,\n",
      "           0., 138.,  15.,   9.,   1.,   2.,   6.,   2.],\n",
      "        [  3.,  10.,   1.,   1.,   1.,   0.,   7.,   1.,   0.,  10.,   9.,   0.,\n",
      "           1.,  12., 116.,   2.,   0.,   7.,   4.,  10.],\n",
      "        [  0.,   0.,   0.,   0.,   5.,   0.,   2.,   0.,   0.,   0.,   2.,   1.,\n",
      "           0.,   8.,   2., 175.,   0.,   0.,   6.,   6.],\n",
      "        [  5.,   3.,   1.,   0.,   0.,   1.,   0.,   5.,   0.,   0.,   0.,   5.,\n",
      "           0.,   1.,   0.,   2., 180.,   1.,   0.,   0.],\n",
      "        [  1.,   1.,   1.,   8.,   0.,   1.,   0.,   6.,   1.,  13.,   4.,  19.,\n",
      "           0.,   1.,   0.,   0.,   0., 134.,   0.,   3.],\n",
      "        [  0.,   1.,   0.,   0.,   0.,   0.,  10.,   0.,   1.,   1.,   1.,   0.,\n",
      "           0.,   3.,   1.,   2.,   0.,   0., 160.,  17.],\n",
      "        [  1.,   1.,   0.,   2.,   0.,   0.,   2.,   0.,   0.,   1.,   0.,   3.,\n",
      "           2.,   0.,   5.,   3.,   0.,   2.,   5., 167.]]), tensor([[145.,   6.,   2.,   2.,   0.,   0.,   0.,   0.,   0.,   1.,   4.,   0.,\n",
      "           0.,   2.,   1.,   1.,  27.,   2.,   2.,   1.],\n",
      "        [ 10., 109.,   3.,   2.,   0.,   0.,   0.,   9.,   1.,   3.,  30.,   0.,\n",
      "           0.,   7.,   6.,   0.,  12.,   6.,   0.,   0.],\n",
      "        [  6.,   2., 145.,   1.,   0.,   0.,   0.,  14.,   0.,   2.,  12.,   2.,\n",
      "           0.,   1.,   3.,   0.,   3.,  10.,   0.,   0.],\n",
      "        [  0.,   3.,   2., 156.,   0.,   0.,   0.,   5.,   2.,   7.,   7.,   6.,\n",
      "           0.,   1.,   1.,   1.,   1.,   7.,   0.,   0.],\n",
      "        [  0.,   0.,   1.,   0., 180.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
      "          11.,   0.,   0.,   3.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   2.,   1., 164.,   5.,   0.,   7.,   5.,   0.,   0.,\n",
      "           6.,   1.,   1.,   2.,   0.,   1.,   0.,   6.],\n",
      "        [  0.,   0.,   0.,   1.,   2.,  12., 156.,   0.,   6.,   1.,   0.,   0.,\n",
      "           5.,   0.,   1.,   1.,   0.,   0.,   4.,   3.],\n",
      "        [  0.,   2.,   5.,   5.,   0.,   0.,   0., 158.,   0.,   0.,   4.,   6.,\n",
      "           0.,   2.,   0.,   1.,   7.,  15.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   2.,   0.,  12.,   4.,   0., 174.,   4.,   0.,   0.,\n",
      "           1.,   0.,   0.,   1.,   0.,   0.,   0.,   2.],\n",
      "        [  2.,   3.,   9.,  15.,   1.,   1.,   0.,   2.,   1., 110.,  12.,   7.,\n",
      "           0.,   0.,   4.,   1.,   0.,  24.,   1.,   3.],\n",
      "        [  2.,   7.,   3.,   8.,   1.,   0.,   0.,   9.,   0.,   4., 134.,   1.,\n",
      "           0.,   5.,   5.,   4.,   1.,  10.,   0.,   0.],\n",
      "        [  1.,   0.,   7.,  12.,   0.,   0.,   0.,   9.,   0.,   7.,   1., 140.,\n",
      "           0.,   0.,   0.,   0.,   2.,  16.,   0.,   5.],\n",
      "        [  0.,   0.,   0.,   0.,   5.,   1.,   0.,   0.,   1.,   0.,   1.,   0.,\n",
      "         189.,   1.,   0.,   0.,   0.,   0.,   1.,   1.],\n",
      "        [  1.,  11.,   1.,   2.,   0.,   0.,   1.,   2.,   0.,   0.,  10.,   1.,\n",
      "           0., 135.,  11.,  13.,   2.,   4.,   2.,   4.],\n",
      "        [  0.,   4.,   2.,   1.,   0.,   1.,   1.,   0.,   2.,   4.,  19.,   0.,\n",
      "           2.,  14., 122.,   3.,   1.,   7.,   5.,   6.],\n",
      "        [  0.,   0.,   0.,   3.,   0.,   0.,   0.,   0.,   1.,   1.,   5.,   1.,\n",
      "           0.,   8.,   2., 181.,   0.,   0.,   2.,   2.],\n",
      "        [  4.,   3.,   2.,   1.,   0.,   0.,   0.,   7.,   0.,   0.,   1.,   1.,\n",
      "           0.,   0.,   0.,   2., 180.,   2.,   0.,   0.],\n",
      "        [  1.,   0.,   3.,  11.,   0.,   1.,   0.,  19.,   0.,   6.,   4.,   9.,\n",
      "           0.,   1.,   0.,   0.,   0., 141.,   0.,   1.],\n",
      "        [  0.,   1.,   0.,   1.,   0.,   3.,   5.,   0.,   0.,   0.,   1.,   0.,\n",
      "           1.,   2.,   9.,   3.,   0.,   0., 140.,  30.],\n",
      "        [  0.,   1.,   0.,   2.,   0.,   2.,   1.,   1.,   0.,   4.,   2.,   2.,\n",
      "           1.,   2.,   9.,   5.,   0.,   5.,   5., 151.]])]\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion_pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[163.,   5.,   4.,   2.,   0.,   0.,   0.,   2.,   0.,   3.,   3.,   3.,\n",
      "           0.,   0.,   3.,   0.,   5.,   0.,   1.,   1.],\n",
      "        [ 17.,  97.,   2.,   0.,   0.,   0.,   0.,  10.,   0.,   5.,  36.,   1.,\n",
      "           0.,   5.,  17.,   0.,   4.,   5.,   0.,   0.],\n",
      "        [  6.,   1., 158.,   0.,   1.,   0.,   0.,   2.,   0.,  15.,   8.,   2.,\n",
      "           0.,   1.,   2.,   0.,   2.,   3.,   0.,   0.],\n",
      "        [  0.,   0.,   1., 163.,   0.,   1.,   0.,   5.,   1.,  13.,   4.,   2.,\n",
      "           0.,   0.,   1.,   0.,   0.,   8.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   1., 187.,   2.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
      "           2.,   1.,   1.,   2.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   4.,   4.,   2., 156.,   4.,   0.,  17.,   3.,   2.,   0.,\n",
      "           0.,   0.,   2.,   0.,   0.,   3.,   0.,   3.],\n",
      "        [  0.,   0.,   1.,   1.,   0.,  10., 162.,   0.,   7.,   0.,   0.,   0.,\n",
      "           2.,   1.,   1.,   1.,   0.,   0.,   2.,   3.],\n",
      "        [  3.,   3.,   8.,   8.,   0.,   0.,   0., 148.,   0.,   2.,  10.,   2.,\n",
      "           0.,   0.,   1.,   0.,   2.,  16.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   4.,   2.,   5.,   3.,   0., 182.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   1.,   0.,   2.,   0.,   1.],\n",
      "        [  1.,   1.,   8.,   6.,   0.,   2.,   0.,   1.,   0., 160.,   5.,   5.,\n",
      "           0.,   0.,   3.,   0.,   0.,   4.,   0.,   0.],\n",
      "        [  1.,   9.,   5.,   7.,   1.,   0.,   0.,   3.,   0.,  11., 132.,   1.,\n",
      "           0.,   4.,  14.,   2.,   0.,   5.,   0.,   0.],\n",
      "        [  8.,   2.,   6.,  15.,   0.,   0.,   0.,   7.,   0.,  24.,   1., 128.,\n",
      "           0.,   0.,   0.,   0.,   1.,   6.,   0.,   2.],\n",
      "        [  0.,   0.,   1.,   0.,   9.,   0.,   2.,   0.,   1.,   0.,   0.,   0.,\n",
      "         187.,   0.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  1.,   8.,   1.,   4.,   2.,   1.,   0.,   2.,   1.,   2.,   7.,   0.,\n",
      "           0., 130.,  25.,   7.,   1.,   5.,   2.,   2.],\n",
      "        [  2.,   9.,   3.,   2.,   1.,   0.,   1.,   0.,   0.,   4.,  14.,   0.,\n",
      "           1.,   8., 124.,   1.,   1.,   7.,   6.,  12.],\n",
      "        [  0.,   0.,   2.,   5.,   1.,   0.,   0.,   0.,   1.,   1.,   2.,   0.,\n",
      "           0.,  12.,   6., 166.,   0.,   0.,   6.,   2.],\n",
      "        [ 27.,   7.,   7.,   0.,   0.,   0.,   0.,   4.,   0.,   0.,   0.,   2.,\n",
      "           0.,   0.,   1.,   0., 154.,   1.,   0.,   0.],\n",
      "        [  1.,   4.,   1.,  11.,   0.,   1.,   0.,  10.,   0.,  20.,   6.,   5.,\n",
      "           0.,   0.,   2.,   0.,   1., 133.,   0.,   1.],\n",
      "        [  0.,   2.,   1.,   0.,   0.,   0.,   8.,   0.,   0.,   2.,   0.,   0.,\n",
      "           2.,   1.,  13.,   2.,   0.,   0., 144.,  21.],\n",
      "        [  0.,   2.,   0.,   4.,   0.,   1.,   1.,   0.,   1.,   6.,   0.,   3.,\n",
      "           0.,   1.,   9.,   4.,   0.,   3.,   7., 152.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion_pause[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[156.,   9.,   2.,   1.,   0.,   0.,   0.,   2.,   0.,   4.,   1.,   6.,\n",
      "           0.,   0.,   1.,   0.,  12.,   1.,   1.,   0.],\n",
      "        [ 11., 120.,   3.,   2.,   0.,   0.,   0.,   9.,   0.,   8.,  22.,   0.,\n",
      "           0.,   4.,   4.,   0.,   7.,   7.,   0.,   2.],\n",
      "        [  8.,   4., 142.,   1.,   0.,   0.,   0.,   7.,   0.,  12.,   8.,   3.,\n",
      "           0.,   0.,   2.,   1.,   6.,   7.,   0.,   0.],\n",
      "        [  0.,   4.,   0., 151.,   0.,   0.,   0.,   5.,   0.,  13.,   5.,   7.,\n",
      "           0.,   0.,   1.,   1.,   0.,  12.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 192.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           4.,   0.,   1.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  2.,   0.,   0.,   3.,   1., 151.,  10.,   0.,  15.,   2.,   0.,   3.,\n",
      "           1.,   0.,   5.,   0.,   0.,   1.,   0.,   5.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   2., 173.,   0.,   5.,   0.,   0.,   0.,\n",
      "           0.,   0.,   2.,   0.,   0.,   0.,   3.,   2.],\n",
      "        [  1.,   3.,   6.,  10.,   0.,   0.,   0., 142.,   0.,   0.,   7.,   4.,\n",
      "           0.,   2.,   1.,   1.,   6.,  20.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   1.,   1.,   4.,   5.,   0., 186.,   1.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   1.,   0.,   2.],\n",
      "        [  3.,   3.,   7.,   5.,   1.,   0.,   0.,   0.,   2., 146.,   6.,   5.,\n",
      "           1.,   0.,   3.,   1.,   0.,   8.,   1.,   2.],\n",
      "        [  6.,  14.,   4.,   5.,   0.,   0.,   0.,   1.,   0.,   6., 130.,   2.,\n",
      "           0.,   8.,   7.,   2.,   2.,   7.,   0.,   0.],\n",
      "        [  4.,   1.,   4.,   6.,   0.,   1.,   0.,   7.,   0.,  13.,   2., 148.,\n",
      "           0.,   1.,   0.,   0.,   2.,  11.,   0.,   0.],\n",
      "        [  0.,   0.,   1.,   0.,   3.,   0.,   3.,   0.,   0.,   0.,   0.,   0.,\n",
      "         193.,   0.,   2.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  1.,   9.,   0.,   4.,   2.,   0.,   1.,   2.,   0.,   1.,   7.,   0.,\n",
      "           1., 146.,  10.,   7.,   4.,   1.,   4.,   2.],\n",
      "        [  1.,   8.,   1.,   0.,   1.,   0.,   1.,   0.,   1.,   4.,   8.,   1.,\n",
      "           1.,  10., 124.,   2.,   1.,  10.,  11.,   8.],\n",
      "        [  0.,   0.,   1.,   0.,   5.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,\n",
      "           0.,  11.,   2., 171.,   0.,   1.,   6.,   7.],\n",
      "        [ 10.,   5.,   4.,   0.,   0.,   0.,   0.,  12.,   0.,   0.,   0.,   3.,\n",
      "           0.,   0.,   1.,   0., 168.,   0.,   1.,   0.],\n",
      "        [  4.,   4.,   2.,  14.,   0.,   1.,   1.,   4.,   1.,  11.,   7.,  11.,\n",
      "           0.,   1.,   1.,   0.,   0., 132.,   0.,   2.],\n",
      "        [  2.,   0.,   0.,   0.,   0.,   0.,   6.,   0.,   0.,   1.,   1.,   1.,\n",
      "           0.,   2.,   8.,   3.,   0.,   0., 159.,  14.],\n",
      "        [  1.,   2.,   0.,   3.,   0.,   0.,   3.,   0.,   1.,   6.,   0.,   1.,\n",
      "           1.,   1.,   3.,   1.,   0.,   6.,   7., 157.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion_pause[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[142.,  11.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,   3.,\n",
      "           0.,   1.,   2.,   0.,  27.,   0.,   2.,   0.],\n",
      "        [ 12., 121.,   2.,   1.,   0.,   0.,   0.,   7.,   0.,   2.,  23.,   2.,\n",
      "           0.,   8.,   6.,   0.,   9.,   7.,   0.,   0.],\n",
      "        [  3.,   2., 154.,   0.,   0.,   0.,   0.,  15.,   0.,   3.,  10.,   1.,\n",
      "           0.,   1.,   1.,   0.,   5.,   5.,   0.,   0.],\n",
      "        [  0.,   5.,   1., 139.,   0.,   1.,   0.,  15.,   0.,   9.,   7.,   6.,\n",
      "           1.,   2.,   0.,   1.,   1.,  10.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0., 192.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
      "           2.,   0.,   0.,   1.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   3.,   2., 160.,   8.,   0.,   8.,   3.,   0.,   1.,\n",
      "           1.,   0.,   2.,   0.,   0.,   6.,   0.,   4.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   7., 167.,   0.,   5.,   1.,   1.,   0.,\n",
      "           0.,   0.,   1.,   1.,   0.,   0.,   1.,   2.],\n",
      "        [  2.,   8.,   5.,   5.,   0.,   0.,   0., 160.,   0.,   0.,   3.,   4.,\n",
      "           0.,   1.,   1.,   1.,   7.,   7.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,  11.,   2.,   2., 181.,   2.,   0.,   0.,\n",
      "           1.,   0.,   0.,   1.,   1.,   1.,   0.,   0.],\n",
      "        [  1.,   7.,   7.,   4.,   1.,   2.,   0.,   1.,   0., 127.,  14.,   7.,\n",
      "           0.,   0.,   2.,   1.,   0.,  16.,   3.,   2.],\n",
      "        [  4.,  24.,   4.,   3.,   2.,   2.,   0.,   2.,   0.,   2., 124.,   1.,\n",
      "           0.,   7.,   9.,   5.,   0.,   5.,   0.,   0.],\n",
      "        [  2.,   1.,   5.,   6.,   0.,   0.,   0.,   9.,   0.,   8.,   2., 150.,\n",
      "           0.,   0.,   0.,   1.,   3.,  10.,   0.,   1.],\n",
      "        [  1.,   0.,   0.,   0.,   7.,   2.,   1.,   0.,   1.,   0.,   1.,   0.,\n",
      "         184.,   0.,   0.,   1.,   1.,   0.,   0.,   1.],\n",
      "        [  1.,   6.,   1.,   2.,   1.,   2.,   2.,   2.,   0.,   1.,   4.,   0.,\n",
      "           0., 144.,  11.,  16.,   1.,   0.,   4.,   2.],\n",
      "        [  1.,  10.,   3.,   1.,   1.,   1.,   3.,   0.,   0.,   3.,  11.,   1.,\n",
      "           0.,  13., 118.,   5.,   0.,  10.,   9.,   5.],\n",
      "        [  1.,   1.,   0.,   0.,   2.,   0.,   1.,   0.,   0.,   1.,   3.,   0.,\n",
      "           1.,   6.,   1., 183.,   0.,   0.,   4.,   2.],\n",
      "        [  3.,   3.,   3.,   0.,   0.,   0.,   0.,   8.,   0.,   0.,   0.,   0.,\n",
      "           0.,   1.,   0.,   0., 187.,   0.,   0.,   0.],\n",
      "        [  5.,   2.,   2.,  16.,   0.,   2.,   0.,  25.,   0.,   4.,   1.,   9.,\n",
      "           0.,   1.,   0.,   0.,   1., 130.,   0.,   1.],\n",
      "        [  1.,   0.,   0.,   0.,   0.,   0.,   7.,   1.,   0.,   1.,   2.,   0.,\n",
      "           0.,   2.,   5.,   4.,   0.,   0., 158.,  15.],\n",
      "        [  1.,   1.,   0.,   2.,   0.,   0.,   2.,   0.,   5.,   2.,   3.,   5.,\n",
      "           0.,   1.,  12.,   6.,   0.,   1.,   4., 148.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion_pause[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[158.,   5.,   6.,   1.,   0.,   0.,   0.,   0.,   0.,   3.,   1.,   1.,\n",
      "           1.,   1.,   2.,   0.,  12.,   1.,   3.,   0.],\n",
      "        [ 18., 116.,   1.,   2.,   0.,   1.,   0.,   5.,   0.,   3.,  20.,   1.,\n",
      "           0.,   8.,   9.,   1.,   4.,   9.,   0.,   2.],\n",
      "        [  5.,   1., 142.,   3.,   1.,   1.,   0.,   7.,   0.,  12.,   7.,   4.,\n",
      "           0.,   2.,   1.,   0.,   6.,   7.,   0.,   1.],\n",
      "        [  0.,   5.,   0., 152.,   0.,   0.,   0.,   4.,   1.,  17.,   3.,   5.,\n",
      "           0.,   0.,   1.,   1.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 190.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
      "           4.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   1.,   0.,   2.,   3., 157.,  11.,   0.,  19.,   2.,   1.,   0.,\n",
      "           2.,   0.,   0.,   0.,   0.,   2.,   0.,   1.],\n",
      "        [  0.,   0.,   0.,   0.,   2.,   7., 169.,   0.,   6.,   0.,   0.,   0.,\n",
      "           2.,   0.,   1.,   1.,   0.,   0.,   1.,   2.],\n",
      "        [  2.,  12.,   3.,  10.,   0.,   0.,   0., 133.,   0.,   2.,   7.,   6.,\n",
      "           0.,   3.,   0.,   0.,   3.,  21.,   1.,   1.],\n",
      "        [  0.,   0.,   0.,   2.,   1.,   5.,   7.,   0., 183.,   0.,   0.,   0.,\n",
      "           1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.],\n",
      "        [  0.,   3.,   6.,   4.,   1.,   1.,   1.,   0.,   1., 147.,   9.,   9.,\n",
      "           0.,   1.,   1.,   0.,   0.,   7.,   1.,   4.],\n",
      "        [  5.,  14.,   3.,   4.,   1.,   0.,   0.,   3.,   0.,  10., 119.,   1.,\n",
      "           0.,   5.,  12.,   2.,   1.,  14.,   0.,   0.],\n",
      "        [  5.,   1.,   2.,   5.,   0.,   0.,   0.,   2.,   1.,  12.,   0., 158.,\n",
      "           0.,   0.,   0.,   0.,   2.,   9.,   1.,   2.],\n",
      "        [  0.,   0.,   0.,   0.,   1.,   1.,   1.,   0.,   1.,   0.,   0.,   0.,\n",
      "         193.,   0.,   0.,   2.,   0.,   0.,   1.,   1.],\n",
      "        [  1.,   7.,   0.,   3.,   2.,   1.,   1.,   3.,   0.,   3.,   6.,   1.,\n",
      "           0., 138.,  15.,   9.,   1.,   2.,   6.,   2.],\n",
      "        [  3.,  10.,   1.,   1.,   1.,   0.,   7.,   1.,   0.,  10.,   9.,   0.,\n",
      "           1.,  12., 116.,   2.,   0.,   7.,   4.,  10.],\n",
      "        [  0.,   0.,   0.,   0.,   5.,   0.,   2.,   0.,   0.,   0.,   2.,   1.,\n",
      "           0.,   8.,   2., 175.,   0.,   0.,   6.,   6.],\n",
      "        [  5.,   3.,   1.,   0.,   0.,   1.,   0.,   5.,   0.,   0.,   0.,   5.,\n",
      "           0.,   1.,   0.,   2., 180.,   1.,   0.,   0.],\n",
      "        [  1.,   1.,   1.,   8.,   0.,   1.,   0.,   6.,   1.,  13.,   4.,  19.,\n",
      "           0.,   1.,   0.,   0.,   0., 134.,   0.,   3.],\n",
      "        [  0.,   1.,   0.,   0.,   0.,   0.,  10.,   0.,   1.,   1.,   1.,   0.,\n",
      "           0.,   3.,   1.,   2.,   0.,   0., 160.,  17.],\n",
      "        [  1.,   1.,   0.,   2.,   0.,   0.,   2.,   0.,   0.,   1.,   0.,   3.,\n",
      "           2.,   0.,   5.,   3.,   0.,   2.,   5., 167.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion_pause[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[145.,   6.,   2.,   2.,   0.,   0.,   0.,   0.,   0.,   1.,   4.,   0.,\n",
      "           0.,   2.,   1.,   1.,  27.,   2.,   2.,   1.],\n",
      "        [ 10., 109.,   3.,   2.,   0.,   0.,   0.,   9.,   1.,   3.,  30.,   0.,\n",
      "           0.,   7.,   6.,   0.,  12.,   6.,   0.,   0.],\n",
      "        [  6.,   2., 145.,   1.,   0.,   0.,   0.,  14.,   0.,   2.,  12.,   2.,\n",
      "           0.,   1.,   3.,   0.,   3.,  10.,   0.,   0.],\n",
      "        [  0.,   3.,   2., 156.,   0.,   0.,   0.,   5.,   2.,   7.,   7.,   6.,\n",
      "           0.,   1.,   1.,   1.,   1.,   7.,   0.,   0.],\n",
      "        [  0.,   0.,   1.,   0., 180.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
      "          11.,   0.,   0.,   3.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   2.,   1., 164.,   5.,   0.,   7.,   5.,   0.,   0.,\n",
      "           6.,   1.,   1.,   2.,   0.,   1.,   0.,   6.],\n",
      "        [  0.,   0.,   0.,   1.,   2.,  12., 156.,   0.,   6.,   1.,   0.,   0.,\n",
      "           5.,   0.,   1.,   1.,   0.,   0.,   4.,   3.],\n",
      "        [  0.,   2.,   5.,   5.,   0.,   0.,   0., 158.,   0.,   0.,   4.,   6.,\n",
      "           0.,   2.,   0.,   1.,   7.,  15.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   2.,   0.,  12.,   4.,   0., 174.,   4.,   0.,   0.,\n",
      "           1.,   0.,   0.,   1.,   0.,   0.,   0.,   2.],\n",
      "        [  2.,   3.,   9.,  15.,   1.,   1.,   0.,   2.,   1., 110.,  12.,   7.,\n",
      "           0.,   0.,   4.,   1.,   0.,  24.,   1.,   3.],\n",
      "        [  2.,   7.,   3.,   8.,   1.,   0.,   0.,   9.,   0.,   4., 134.,   1.,\n",
      "           0.,   5.,   5.,   4.,   1.,  10.,   0.,   0.],\n",
      "        [  1.,   0.,   7.,  12.,   0.,   0.,   0.,   9.,   0.,   7.,   1., 140.,\n",
      "           0.,   0.,   0.,   0.,   2.,  16.,   0.,   5.],\n",
      "        [  0.,   0.,   0.,   0.,   5.,   1.,   0.,   0.,   1.,   0.,   1.,   0.,\n",
      "         189.,   1.,   0.,   0.,   0.,   0.,   1.,   1.],\n",
      "        [  1.,  11.,   1.,   2.,   0.,   0.,   1.,   2.,   0.,   0.,  10.,   1.,\n",
      "           0., 135.,  11.,  13.,   2.,   4.,   2.,   4.],\n",
      "        [  0.,   4.,   2.,   1.,   0.,   1.,   1.,   0.,   2.,   4.,  19.,   0.,\n",
      "           2.,  14., 122.,   3.,   1.,   7.,   5.,   6.],\n",
      "        [  0.,   0.,   0.,   3.,   0.,   0.,   0.,   0.,   1.,   1.,   5.,   1.,\n",
      "           0.,   8.,   2., 181.,   0.,   0.,   2.,   2.],\n",
      "        [  4.,   3.,   2.,   1.,   0.,   0.,   0.,   7.,   0.,   0.,   1.,   1.,\n",
      "           0.,   0.,   0.,   2., 180.,   2.,   0.,   0.],\n",
      "        [  1.,   0.,   3.,  11.,   0.,   1.,   0.,  19.,   0.,   6.,   4.,   9.,\n",
      "           0.,   1.,   0.,   0.,   0., 141.,   0.,   1.],\n",
      "        [  0.,   1.,   0.,   1.,   0.,   3.,   5.,   0.,   0.,   0.,   1.,   0.,\n",
      "           1.,   2.,   9.,   3.,   0.,   0., 140.,  30.],\n",
      "        [  0.,   1.,   0.,   2.,   0.,   2.,   1.,   1.,   0.,   4.,   2.,   2.,\n",
      "           1.,   2.,   9.,   5.,   0.,   5.,   5., 151.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_confusion_pause[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1 = []\n",
    "for conf in all_confusion_pause:\n",
    "    recall = np.diag(conf.numpy()) / np.sum(conf.numpy(), axis = 1)\n",
    "    precision = np.diag(conf.numpy()) / np.sum(conf.numpy(), axis = 0)\n",
    "    recall = np.mean(recall)\n",
    "    precision = np.mean(precision)\n",
    "    all_f1.append(2 * (precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7674388008698354, 0.7792753670743345, 0.774270368373701, 0.7786558686999252, 0.7618868433470727]\n",
      "f1 score:  0.7723054496729738\n",
      "f1 score:  0.006708205486526485\n"
     ]
    }
   ],
   "source": [
    "print(all_f1)\n",
    "print(\"f1 score: \", np.mean(all_f1))\n",
    "print(\"f1 score: \", np.std(all_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[152.8   7.2   3.8   1.2   0.    0.    0.    0.8   0.    2.4   2.2   2.6\n",
      "    0.2   0.8   1.8   0.2  16.6   0.8   1.8   0.4]\n",
      " [ 13.6 112.6   2.2   1.4   0.    0.2   0.    8.    0.2   4.2  26.2   0.8\n",
      "    0.    6.4   8.4   0.2   7.2   6.8   0.    0.8]\n",
      " [  5.6   2.  148.2   1.    0.4   0.2   0.    9.    0.    8.8   9.    2.4\n",
      "    0.    1.    1.8   0.2   4.4   6.4   0.    0.2]\n",
      " [  0.    3.4   0.8 152.2   0.    0.4   0.    6.8   0.8  11.8   5.2   5.2\n",
      "    0.2   0.6   0.8   0.8   0.4   9.4   0.    0.2]\n",
      " [  0.    0.    0.2   0.2 188.2   1.    0.8   0.2   0.    0.    0.    0.\n",
      "    4.6   0.2   0.4   1.2   0.    0.    0.    0.2]\n",
      " [  0.4   0.2   0.8   2.8   1.8 157.6   7.6   0.   13.2   3.    0.6   0.8\n",
      "    2.    0.2   2.    0.4   0.    2.6   0.    3.8]\n",
      " [  0.    0.    0.2   0.4   2.4   7.6 165.4   0.    5.8   0.4   0.2   0.\n",
      "    1.8   0.2   1.2   0.8   0.    0.    2.2   2.4]\n",
      " [  1.6   5.6   5.4   7.6   0.    0.    0.  148.2   0.    0.8   6.2   4.4\n",
      "    0.    1.6   0.6   0.6   5.   15.8   0.2   0.2]\n",
      " [  0.    0.    0.    1.8   0.8   7.4   4.2   0.4 181.2   1.4   0.    0.\n",
      "    0.6   0.    0.    0.6   0.2   0.8   0.    1.2]\n",
      " [  1.4   3.4   7.4   6.8   0.8   1.2   0.2   0.8   0.8 138.    9.2   6.6\n",
      "    0.2   0.2   2.6   0.6   0.   11.8   1.2   2.2]\n",
      " [  3.6  13.6   3.8   5.4   1.    0.4   0.    3.6   0.    6.6 127.8   1.2\n",
      "    0.    5.8   9.4   3.    0.8   8.2   0.    0. ]\n",
      " [  4.    1.    4.8   8.8   0.    0.2   0.    6.8   0.2  12.8   1.2 144.8\n",
      "    0.    0.2   0.    0.2   2.   10.4   0.2   2. ]\n",
      " [  0.2   0.    0.4   0.    5.    0.8   1.4   0.    0.8   0.    0.4   0.\n",
      "  189.2   0.2   0.4   0.8   0.2   0.    0.4   0.8]\n",
      " [  1.    8.2   0.6   3.    1.4   0.8   1.    2.2   0.2   1.4   6.8   0.4\n",
      "    0.2 138.6  14.4  10.4   1.8   2.4   3.6   2.4]\n",
      " [  1.4   8.2   2.    1.    0.8   0.4   2.6   0.2   0.6   5.   12.2   0.4\n",
      "    1.   11.4 120.8   2.6   0.6   8.2   7.    8.2]\n",
      " [  0.2   0.2   0.6   1.6   2.6   0.    0.6   0.    0.6   0.8   2.4   0.4\n",
      "    0.2   9.    2.6 175.2   0.    0.2   4.8   3.8]\n",
      " [  9.8   4.2   3.4   0.2   0.    0.2   0.    7.2   0.    0.    0.2   2.2\n",
      "    0.    0.4   0.4   0.8 173.8   0.8   0.2   0. ]\n",
      " [  2.4   2.2   1.8  12.    0.    1.2   0.2  12.8   0.4  10.8   4.4  10.6\n",
      "    0.    0.8   0.6   0.    0.4 134.    0.    1.6]\n",
      " [  0.6   0.8   0.2   0.2   0.    0.6   7.2   0.2   0.2   1.    1.    0.2\n",
      "    0.6   2.    7.2   2.8   0.    0.  152.2  19.4]\n",
      " [  0.6   1.4   0.    2.6   0.    0.6   1.8   0.2   1.4   3.8   1.    2.8\n",
      "    0.8   1.    7.6   3.8   0.    3.4   5.6 155. ]]\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for conf in all_confusion_pause:\n",
    "    temp.append(np.array(conf))\n",
    "print(np.mean(np.array(temp), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def displayConfMat(confusion_matrix, save_file_name):\n",
    "    font = {'size'   : 3.5}\n",
    "    plt.rc('font', **font)\n",
    "    figure(num=None, figsize=(1080, 1080), dpi=300, facecolor='w', edgecolor='k')\n",
    "    plt_conf = ConfusionMatrixDisplay(confusion_matrix=np.array(confusion_matrix),\n",
    "                                  display_labels=np.array(classes))\n",
    "    plt_conf.plot(xticks_rotation='vertical', cmap='Blues',values_format='.5g')\n",
    "    plt.gcf().subplots_adjust(bottom=0.19)\n",
    "    plt_conf.figure_.savefig(save_file_name, dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Results for CNNLSTM - best case\n",
      "Accuracy: 0.77797\n",
      "F1: 0.77928\n",
      "===============================\n",
      "===============================\n",
      "Results for CNNLSTM - worst case\n",
      "Accuracy: 0.75857\n",
      "F1: 0.76189\n",
      "===============================\n",
      "===============================\n",
      "Results for CNNLSTM - avg case\n",
      "Accuracy: 0.77011\n",
      "F1: 0.77231\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAD/CAYAAACzQBC3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAz2klEQVR4nO2de5xV1ZXnv6veRRWPogooQKCKhyBPhYoaifEVW2PUBMWk7Uw0xo7pzpiYTz+cz+TVk+4ek+lPT09aM5mJmU40mo9JR5L4SrSjwReKiBFF8YFYIDGigIBAvYs9f5xTscC6d697a997Tl3W18/9lFSt2vucc89dtffav/Pb4pzDMAwjbZQlfQCGYRhDYcnJMIxUYsnJMIxUYsnJMIxUYsnJMIxUYsnJMIxUUpH0AYSgrGaMK6+f4I1bOL1B1Z7IcI/oiPbCNqdGKxLRHl9I0Ym2rdB/PUNfk36lFKcs8E2lbe13v3tql3PO/+HIQvmYGc71dXrjXOfO+5xz5w6nr8GURHIqr59Aw0e/6Y377Q0Xq9qrrtB9JLT3m4TOdkoOHVJ+cMp0x9fXf8gbU65sq7dfd2xVyvdCi1bXp33PDnT1qeLqqstVcVq0x1dbKduG25fr66J63p9647qevqFpuH0NpiSSk2EYBUQIP51QUFI1p2UzG7mgbRpnLZpMfU0Fl58+m6njR2WM//VDz3DzLx71trt2wxZuu+cJb9wTz77KHQ887Y2767cbuOWOx4oepz0PbXudXT1844Y7vHHa63L36g2sXvtCkOPTnoP2+DTtrd/Yzrdvuo+bfv4I3T3ZR1TaaxL6nsobKfO/ApN4chKR+YO/DoenXt3NzEmjqaks59gpY+ns6acyy7Rg6qTxbH19l7fdtoUtHFJMaU5aPFN9rONGZ06ahYrTnoe2vTW/e4XF86Z547TX5ZVtb6niQHd82munPT5fe22LWpkzYxI1VZVUlGf/aGn7LMQ9lTsCZeX+V2ASmdaJyMVEiXExsF1E+oFLRGQt8D5gM1ANbAXOBVY7535zRBtXAVcBlNVFU905k8cgAuNHV/PE5p3MmTyGaY11bH3rwJDHUVYmzDzGXyu8/tb7WTB7qjfuxVff4KX2Hd44h2PffkWBMXCc9jy07XV0dfPClj9w/hlLqKzIfHNqr8vsGRNVfyw0x6c9B+3xadp7uX0HL2/dQf2oGnr6+qgtrxpWn7nE5XK+eZHAtE6SePBXRM4DRgEnA/cBvwc+COwgWlCpBB4CzgPGAw865zZkaq+yaZbTFMRfsoL4kFhB/L2UUEH8Kedc23D6KqtvdtULL/fGdT3xT8PuazCJjJycc7+K//f2Qd8eqtjwoyIcjmEYWZFERk62WmcYhp8C1JR8WHIyDMODFGQ1zkdJJKcF08fxwL9e5I1b+W/rVO3d+bmTdR0ry3UJlZzo7tOtzNVW6f4q9inqRLsP9KjamjS2RhWnRVMPy4WKct2bpo3Tvhc1lbr3QltPDEJCOqeSSE6GYRQSgbLipwpLToZh+FGuwgbtsug9Fpj1G9v57o8f4IerHskYc9ykev502VQ+smAS42orOee4iRw3qT5j/DMvbucHtz/s7Tu0Ajt0nFYRr23v7tUbvO09tbGd7/zoP/jxHWvo8aimQ56vVr2ujdMe25Mb2/nW9+4JFhda/Z8XwtGjEBeR98heRWS0iLxHETlUbDbaFrUyfUojE8aPzhjzwpsH2L6nk95+x+SxNazbuoeGUZkFcy1TGznQ0e3vO7ACO3ScVhGvba+zu5eDnuuyZP50Jk8YR3VVBeUe1bS2X02cVr2ujdMe29L5Mzi2dVKwOG2/udx7eSHifwWm6MkpVocvE5GPichXReRSEfkU0AzME5HPiMjlIrJSRFYCSzO0c5WIrBeR9bt3vfuBG1DpbnxpO51dQxdnpzXUMn38KEZVlbNzfzcntjSwp7M34zFv2b6T2prMyWuA62+9n6YsSXEAh2Pv/o6ix2kV8dr2qisrqK6qzBrz/376IOPGjOLAwW56+/qD9KuJG1Cv+/rUxmmPbd2zr3LSklnB4rT9au+9/Ejm8ZWiK8QHqcPLgE5gLHAvcD7wKjAb6I+/fwjY4Zy7fejWIo5fusw98LB/SPvxHz6pOkbtap32b4VWgR2azp7sH7gBtKt1XYr29mVJ8oNJ/2qd7u92V6/uGmsJvVpXV102fIX4mGNc9cnXeOO6fnPtyFaID1KHH8lN8deHRWSlc+7mIh2SYRjZCDBtE5FTgFOBCcAPgbOBW5xzuzP9TipX63wjJcMwiswwp23OucdEZArwBrAL2As0ABmTU8mt1hmGERrRrtY1DdSB49dVf2whskRaABwA+oA9QNbFrlSOnHJFENXT8HdcdZKqveO/cq8q7tnrgtklF4TQCyh9ijrH2NrsBfIB+pU1E63LgbZGpK3VaGuxWm/wQ4Fru0WvY+rOc1emmpNzbhPwjUHf8uo3SiI5GYZRQAZ0TkXGkpNhGB4kEVeCkqw5aZTL4FcHL5k+jrMXNvPlC+czblQlZy9sZv6UMRnj0+4hHlKVnItftvb90CrxQ3qIh/aH1/qgh1aIm4d4gRCRPxOR+vj/h+0prlEug18d/Mxre2mdUMeb+7rY29HLvo4eDnRn/iCm3UM8pCo5F79s7fuhVeJrjk8bE9ofXuuDHlohnktcXiSgEC/6tE5Evkg0ix0LvBR/ew5wnogcC7xzhKd4C3CHc27HEe380UP8mGnTD+tDo1wGvwf2zIn1lIlQXVXG5HE1fHDeRG55dGvG9tLuIZ6LKjmkX7b2/dAq8UN6iIf2h9f6oId8L3KJywtJZlqXhEL8C0Qjtnbe9Qo/nXcV4wCvEHmKbwWmA3cdmZwGc8LSNrf6Uf/QXOu9c8JX71PFaVfrkvIQ16qXtapkjV92hXIVKduuOIPRrtZp0a7Wad8yrRe6drVO+15oCeIh3tDias78ujeu8+dXjmyFOPDGECLLoUSX/om7YRgFJ/KaOwrM5kz9bRgjDEH/IGlATEpgGIYHoazMdE55USZQo3iyXvvk+sZvflgVN+sLv1DFbblhhSouNOWBh+Kaukm58ibuVb4X5YELsaFnJ9p99bT726ET2Bedo2JaZxjGyMOSk2EYqUNEkKPVQ1xEVgy26I0dMPNGq5YN5R+9tHU8H1k6lTMWNFNTWc5ffeS4IMcXOu4Hqx7hyY3tQdpbv7GdVfc+yY9+8Shv7z0QpN+QqulcFNMa9XdI9X8u6vq0KMRFxPsKTSqSE7AIOE5EWkTkb4CpIvJFETlTRD4nIs25NqhRy4byj/5d+9u0TqynprKMQ86x6fV9QY4vdFxz01j27DsYpL22Ra3U1FTS2dXrvTG1/YZWTWuvnUb9HVL9n4u6XtNernH5cDQnp3ZgWfz/DxLZ8w4wZOV0sIf4zl07D/uZ1nc5lH/07ObRiAjjR1czuraShdPGMXlcbd7tFSquqaFepV7WtPfy1h28svVNGsfVsf9gV5B+Q/pqa68J6NTfuaj/vdcuVtfvP9hFT1/2kVPoeyBfkkhORVeIF4Jly9rcmifWe+O0q3Vab6C0r9b1KneZ1aq131H4g9cqFc79CSmmQ9/v2g+ldrWuviZsGTiEQryiaaYbd/513rjdN1864hXihmGMIITCjIx8WHIyDMOLJSfDMNKHkIiU4KhKTtpakhZtLanhxC+o4t5+4npVnPavmLaWpGWM0h9cg7al0HVCLaFHCqFrScXGRk6GYaQSS06GYaQOocQU4iJyyWDV96Dvq9TfIrIyX6V4UgpsX9yJi1q59s8/zKdXLGfC+NFceObxLMkiAk27J3kScaFU/YPRXOc0X5Nc4vJChq9zEpFTROQTInK+iDSJyGfjXYAzUkgR5rFE1runicg3ROSk2AWzaSDxiMjHROSrsYf4n4nIEhH5BxE5MW5jlIhcJyJn59p5EgpsX9y6je109/TS199Py9RG9rzTkVXAmHZP8iTiQqn6B6O9zmm9JrnG5YMyOWXcVNM59xgwF6glyg13AVmf/ChkcnoJuAeYAfyeaPthAEe0DXErUAU8M+h3+oAtwOxB33sLOFwCThiFeLHj5rY2U11VSU1VJTt27uNPli+gqzuzsDGkKrlU4kKp+gejuc5pvia5xOWLMjntcs61DXrdOOj35xNZ1k0g+oxfALyZtc80KMRFZOVwHDK1CvGkSGq1rhQIvVqnvd9L5RqHUIhXTZztmj/xL9647d/5aOkpxM261zDSS6GenfORiuRkGEa6seRkGEYqMYV4njh0+5GVBb7A/co90LS1pNlf/KUq7tl/vkAVV1ete3v3dfjdBgBGK1TO2musvXah963rU+4zV1kRtt/QtS6t40QobORkGEb6EEtOhmGkkGhTzeL3m4gTZi7KbxHRKxFj1m7Ywm33+LcnD62+febF7fzg9oe9cT5Fci6e5AN+1D++83GvFa7mPJ56rp27fvs0Dzz2fPaTIPx1DnX9cukzpK96LnGh1f/a88gPoazM/wpN0ZOTiFwGzBeRL4nIGbFC/HIRaRORz8fS9pUi8hci8qfA0lz7aFvYwiGlPiak+rZlaiMHOrq9cT5Fci6e5G2LWpk1bSJnL1/Altfe8vbtO49lC1t59bW36OruzSoQhfDXOdT1y6XPkL7qucSFVv/nch75cLR4iDugi0g1XgM8C9wPXAH8BjiXyFHDxa8hGawQ33WEQvz6W++nafxoxYGEVd9u2b6T2poqb5xPkZyLJ/nL7TvYvO1N7n/seWZOe8+jjIehOY/NW3fgcLy994DXGzz0dQ51/XLpM6Svei5xodX/2vPIC4mmdb5X8G7ToBAfLkuXtblHH3/SG5fUap22W1utG6I95bULvcoV2gsrqdW6MbXlw1Zt104+1s38zHe8cZuuO6f0FOKGYaQbW60zDCN9FGja5sOSk2EYWYmkBDZyygshfD1Jg1a9rH2yfvO/fkwV17jCP/8H2HOHzg1hVJVub7iQ1zi08jvt/Wo/3Ekp57NTGKmAj5JIToZhFBYbORmGkT4SqjkVVeeUozL8dBFpyqeftPs4a32wfQrsE+c1c81FS/n7Ty9nVHUF5500k1MXHTPs40tKNR0yTttWUk8ThFbNa88jHwbKJsVWiBdt5CQilwKXxf7gdxPZde4CmoA/AHOAjcClwBpgPnBQRM4DNjrn/Fr/QaTZx1nrg922sCWr6nvdiztoHl/Hjj0ddHT3cbCr1/sXLs2q6dBxmhjfNc61vdBxLVMbeWjdi964XM4jH5KY1hVbIT6WyA+8i3fV3wNfB6rG6zhcGT5kNXkkeogPoPXB9imw505rYN60Rg529jK6tpLqinIqs9jVpl01HTJO21ZSTxOEVs1rzyNfTCGeJ2n3ENeu1pUp3+HQq3VJqaaTQOP7Bcms/oJ+tU57dHXVZcNWbdcdM9ct/M83euPWffl0U4gbhlE8xKQEhmGkFVOIG4aRSoZbEI939z0LeJvIheTDwC3Oud2ZfqckkpND99S3cjqvnvf3KmtJWncALdpa0qL/+mtV3MZvfng4h3MYXb3Zi/wDVCinCdr96LS1JO1nTFuL7VV6kmudHyaMqVbFFRMRdQ2uSUQGF39vHNhY0zn3mIhMIVoU2xu/GoDSTk6GYRQW5chpV6aCeLzj7wLgHaK8sweYCbySqTFLToZheBluzck5twn4xqBvve77ncTWhvPxBtei9WfWqm/vXr2B1WtfyBoT2ss7ZNyS6eM4e2EzX75wPuNGVXL2wmbmTxlTtON7cmM73/rePd44rXJe028uimnN/aK9p3T3yqvc8KP7+Ok9a4t+r+RLydv0isg1IvI1EfkssFREvhi/ThKRfxKR6SLyLRGZIyL/KCLnxz/3q9AGofVn1npWv7LNr7wN7eUdMu6Z1/bSOqGON/d1sbejl30dPRzo7iva8S2dP4NjWyd547TKeU2/ufiba+4X7T2luVeOnz+Dutoaznz/fNp/X/x7JVdEjo4NDjYRPbIyMM8cqCYOKMc/BGwGTgbWk8VHPJuHuNafWau+nT1jolcxHdrLO2TczIn1lIkwqqqcyeNq+OC8iXT3Zv7ghj6+dc++yklLZnnjtMp5Tb+5KKY194v2ntLcK9//yWo6u3pY/fgmWo4p7r2SL0edQlxEVjrnbh9uO0uXtbk1a/0e4qWyWqfFVuveS+gPUdpX62orZdiq7THTj3MnXftDb9z9X3h/6SjEQyQmwzAKSw5SgqDYap1hGF6SeNTQkpNhGF7MCTNPtAbs5crrq/VnrlQ2qK3raWtd2jKhtpbU8MH/qop7+6HrvDE1lTo/8tAk5SJQVaHrt2l0TgvOXrQ1tlDYs3WGYaQOAcpt5GQYRuookMjSR5IK8ZXx1/eo20SkRUTyVpQl5fesVRFr47SK6VCe3ycunM5HT1/I5Re8j2MmjuWzF72faZPGZYzXnkeaPcTTfq+E9kLPlyR0TgVNTiJymYhcKyKfF5FLReSzInKFiHwIWCAilxMpxa8WkS+JyPtE5J+AJUCbiFwuIlfk03cSfs9aFbE2TquYDuX5ve6515jbMokpE8YyecJYOrp6qMxSQ9Keh6/fQsWl2d889LXLRRGfK0JUh/W9QlPokZMjOjcH9AK/IdrEYNwRcTuA3xNZKOwE2uNjUynE0+IhrlURa+O0iulQnt9zZ0wEYNsbe3hj5z5EhOnNDRnjteeRZg/xtN8rob3Q8yWJZ+vMQ3wYhL52oVfrtJ7fIVfrkqhNjAS094r2+mlX60J4iI9vne/O+rtbvXG3X7GsdBTihmGMDLSbb4TEkpNhGF4sORmGkToEe3wlbxz6eo0GrduAVphWoVSSa5/AD83uB/21JIDj/tZvGPfst85TtaW917XXTlur0d4n2g+jtt9sFjWDqa5M4d6ACemcSiI5GYZRWFLlSiAiFxMv4zvnfl60IzIMI1UkNa3LOIZ0zq0CWoG87PVEZIWIZLf5i+KmiEjNEN+fn0+/oPcG18ZpPbC1Su00K6vBrzY+YUYD5yyezNdWLKSuuoJPLm9hSkNtxniNrzaEvX7ac9XeA6Hfs7tXb+DmXzwarN/CK8TT5yG+C5iYZ9vHAfNF5DoROTtWgV8rIh8Xkb8WkStF5GpgGbBERL4iInNF5K9E5KPAJSJyjoh8VUTeY0CdzaZX6w2ujdN6YGuV2mlXVvvUxk9v20PrhDp27O1kTnM9nT39VGapl2l8tSH89dOcq/YeCP2edXb3cjBgv4VUiEPs/OF5Zf19kVMG9g8QkUnx11Oy/Y4vOWVUaCt4GXgSeItI9T1w/OVECvF9wFqgMe5jMzAJeBioJFKN1wDPAu9x43fO3eica3POtTU1HT5A03qDa+O0HthapXaaldXgVxvPmlRPWZkwqrqC7bs7EIGpWUZOGl9tCHv9tOeqvQdCv2fVlRVUV1UG67eQCnGR4T++4px7DHgJuAuYFX9tztpvJuVqXHNaELXr/iGPczqyvQ8Arc65W4bb1pEsXdbmHnnc7yGuJanVuqTU1Vq18fxrbbUu3367enTe6trVOu1TAiEU4hNmLXArvvXv3rjvf3zhNqLZ1gB/3PE3LtNcQrRf3R3Ax4BNzrk1mdrLWBB3zq0Skb3AbMXxe3HOPQr4J9mGYaQOZQ7OuOPvEJtqft/XmE9KcDrRA7mGYRyliBTGdcCHT0qwAchcTEgJWhFmp3JoXVets5rVDq2Tmq5pHzbValg0U7az/qd/5Qvgob89TRUXetqknU9WKQWx6oe/A08Ti31LpUqEGUsJDMMwEnGlzDqtE5E5QKdzzqZ2hnGUMmA2V2x8CfESoq3BDcM4iikT/yt4n56f3wtsDd+tjgGf8VzRKJKfeq6dXz24ga9/2z97LbaXd6HiCqFyznadF04dwzkLJvGxE6YwtraSC5ZMZuHUMRnjtSpnzfFpFdha9b/22LRx2n5D3wP5EHmEp08h3gqsCN6rAhH5OJHP+CeH2uxgsEJ8987DFeIaRfKyha1see0tmieM9cYW28u7UHGhVc6+6/zc6+/Q3XeImspypo+vZc0ruxlfl1nwmIvK2Xd8WgW2Vv2vPTZtnLZfCH+v5EOqRk4iUgWMAorvfxvhgENAP5Fi/PAfDlKIN044XCGuUSRv3rqDQ4ccHZ097Hx7f9bYYnt5FyoutMrZd51bGkfR0jiKvR099PY7ls9uZM/BnozxWpWz5vi0Cmyt+l97bNo4bb+h74F8SGqDg2wK8WuAC4F7nHP/ErzngJywrM09tGadNy4pKYHWyzs0oX2re/r8I4LQUgKtzCEpKYGWHuWIMPSOybWVMmyF+OQ5C90V/+o3JvnmR+YWzUP8HeAzoToyDGPkkrbtyG9ypbA1i2EYwyJ1CvGRlJi0ZlhjanXGn+otmlRRekJPw7oV0zDQTyW6ev1TJ+10bd5f36WKe/l/XaiK0z4wG3pVSfvQdH+/tgYwjIMpIOYhbhhG6oj++Kdo5GQYhjFAEjWnFG71MHy0okmtKFHbntaONinRZEjh3/qN7ay690l+9ItHeXvvgayx2YSJJ7Q0cM6SyXz9okU01FVxyUnTOXl207CPD8La+YayOIbo2n33xw/ww1WPBOu3kCJMJPIu871Ck2hyGvAJF5GZIvIZERlyJCcievUgetGkVpSobU9rRwvJiCZDCv/aFrVSU1NJZ1evt46TTZj49NY9zJxYz459nRxyjknjanCKal7I61dMi2OIrt30KY1MUDpXJi3CHKjpFluEWfBpnYhcRmTHeYDIereTyHb3g8B2Eeknen5vKnCRiEwjkjG8BJwF3A3MAF49ot2rgKsApk2bflifA6LJ889YQmVF5mKvVpSobU9rR+tw7Nvf6Y3LRTSpaS8X4Z+vvZe37uCVrW8ydVID+w920TC2LmPs9bfez4LZU4f82axJ9ZSJUF1VTl11Bdt2HqTKowsLef20bWnjsp3rAC+37+DlrTvo7u7lrPfPz2oTHPr48iWJgnhGEWawDkQ+BUwhSjiPEAk7NwEfAO4jMrNbEIdXAvOBZ4B1wDlEnuKLnHO3Z+pj6bI29/BjfhGmdjlUu1qn3cfT92EbIPRqnWZ1DfSrde909npj6qt1f+9Cr9aFvnZatKt1agFwTdjxQggR5rS5i9yXbvSXNf7m9FlFE2EGYQjP8Ofir78c9L1MhZoBK8+XQh6TYRg5IOkTYRqGYQAmJTAMI4UkteNvSSQnASoCPqipbaoi8HZEoWtioR8iHVMbTr6srSU1vO9qVdyeJ7+jitPWiLQPHGvjQteSikthpAI+SlLnZBhGOIQBw7nsr6xtRDv+viAi52v7Hcnp3DCMYhBGx/QEcBtQKyI1zrku3y+U5Mgp7fa2z7y4nR/c7vc9Sqq9NMaduLiVa//8w3x6xXImTxjLeact5tS2Y4fdZ0hr4JEQlw85mM01DbjTxq+rBjWzHHgImAD47WdJSXIayit8QD2eL2m2t22Z2sgBhYVsUu2lMW7ds+109/TS13+IY5rHc7Cz2zuVCKXozqW9kRCXD2Ui3hfxjr+DXjcO/L5z7mHn3EPOue86597U9JmWad0UEfk7oA54kCizjo7V4x8FXnHOHWbFd5hCfPrhCvHQqtrQSu0t23dmVQUn3V4a4+a2NlNdVUlNdSUvvfoGJx8/i/5DmZNKSEV3Lu2lPS5fktA5FVwhrjoIkS8QjR5rgX8HriQaAv6eaEv02mxWwcuWtbk1T4SzOg+tNk5qtS4Jg7DQpH21Lu2EUIi3HrfY/d2P7vbGXXHijJGlENfgnLvhiG99ddD/+x/zNwyjcIiJMA3DSCFmNmcYRmpJYpJbEsnJoasTdfXqVmVqq3TK6j7lKk/ovzrackjo+ormGof2LX973ZEz/qGZ9zf+mgjAU//9XFWctl5XoYwLXU8s7khGEqnBlURyMgyjcAjJaI4sORmG4SW0D5aGVIgwQ6NVdP/6oWe4+RePeuO06lut13hoVbL2fJPqN6R3ua/fE2Y0cM7iyXxtxULqqiv45PIWpjTUDhm7fmM7377pPn585+Ps2XcwyDkk5V+vfW/zRRSv0KQqOQ2lFM8HraJ76qTxKltd0KlvtV7joVXJ2vNNqt+Q3uW+fp/etofWCXXs2NvJnOZ6Onv6qcxgM9G2qJVZ0yZy9vIFbHktu/+79hyS8q/P5b3NFTkaNjgQkS+KyNdFZLmIXCciZ8f/vkxELgUWi8gZIvJVEfmEiHxcRP6biFwyRFtXDTzDs2vXzsN+plV0l5UJM4+Z4I1zOPbu7/DGDXiN9/Zlt2S9/tb7aVKY22v71Z5vUv3m4l0+3H5nTaqnrEwYVV3B9t0diMDUDCOnl9t3sHnbm9z/2PPMnJb9PtCeg/YeyNW/PtQ9lS8i4n0F77OYCvFYCd4A/DPRoycPArPjH1cCxwGPx/9fSaQSvwx43Dn3eKZ2ly5rc2vWPuntP+2rdSFXzaI4VViqV+u053rc3/qnXHD0rdbVVZcNW7U9e8ES98+33eeNW7Fk8ohWiL8xSA3+7fjrBs/vZHxsxTCMwiOQiNlcUZNTth1UDMNIL7bBgWEYKUSQBDTiR1Vy0l5ebZ1D61uelPND6BqWqh6iPNXQ10RbS1rxvbWquHuvXq6K044oOpT71mmvX7EdJ2zkZBhG6hiQEhQbS06GYXhJYuSUKhFmKLTq27tXb1ApxEN7iJdKe1r1cujr7IvTKL/nN4/mk++bxgWLJ1NZLpx93ETmTKzP2KZWga05B60yff3Gdr774wf44apHvP1qr3G+iOK/0KQ2OYnIaBHxKySHQKu+7ezu5aDCezu0h3iptKdVL4e+zr44jfJ70479vLang95Yl1XlqR9qFdiac9Aq09sWtTJ9SiMTFOJK7TXOhwEpQbEV4gWb1onI1XH7ncAzwCnAi8Bc4E7gU8DzQCuwF3gLWAi8GjfxNPAREbkJOP1IGUI2D3Gt+ra6soLqKv9GkaE9xEulvQH18vlnLKGyIrOoMvR19sUNKL/3d3Rx7qmLhoyZPr6WGeNH0dHTT/OYGpxzNNVXsTlDrtB6jWvOQXN8A3Evb91Bd3cvZ71/flafeO01zpeS8hAf9JxcA7AR6ANagGOAu4g2LngcGHjHy4gS2cCfiQ1EGx7MBfqdc7dl6kurEO9WKsSrK3UDSq1kP7QneVLtaVbr+vp1bWmvsRbtatjRtlrXUFcxbNX23IXHu++t+q037ox5jSNDIZ5BcDl4F4Ihld8isvKI3/VnHcMwCkZk0zvMNkQ+TzRImQf83Dm32/c7qas5mYrcMFKGYs+6+Fm/bJtq/oGohHMX0WzIi0kJDMPwohw47coyrXsLWAKcD/xS01hJJCdBV1+pUboNhK7DaeswlRW6WyAJV0LQ1Tm0qvngKN8ybS2p8ZLvqeL2rPoLVdwo5b3Xq7xXDhXTTYThe5Y75x4DctovvSSSk2EYhcUeXzEMI5Uk8eBv6griIdAqoZNSYP9g1SM8ubE9WHtJnW/o6xIqLhdltU/5feLcSVzzseP5+8tOZlR1BV+59H3DOrYBcnmKYfVa/6bXWo/zfBHxv0KTiuQkIivyVYNnQqOE1saFVmA3N431Gurn0l7ouFBK7Vz7DRWXi7Lap/xe99KbtO94h+e3vU1Hdx8b270r4EGv8SvbsnubD5CLT3s+JJGc0jKtWwycKiL3A5OBscAWoqXHg8657x/5C9kU4loldFIK7KaGeja88Fqw9pI639DXJVRcLspqn/J77jENzJvWwKbX3qa8TDh+VhNPbX6T13cP/ccl9DWePWOiahMOrcd5PkS7qxR/WldUD/GMByHycWAGsBO4Gzg9/tEE4EXn3Opsv79sWZtb88T6bCE5EVqB3av01a6sSGYgG/IeSGol8WBXnypO6w8ferVOe41Dr9Y1jBq+Qnz+4hPcLXc+5I1rax07MhTiueCc+/cjvmVCTMNIEUn8yUlFcjIMI80UZusnH5acDMPwYjqnPOl3joPd/pqDz7NnAO0bUVGuC0yqlnRAWYepryn+baDdn0379H2d8hy0tR9tLanhE/+ma++nV6rilNv5IVK8e6pQ2437KInkZBhGYbFpnWEYqcQ8xAOg9WcGvVJb65WdlKJbEzdwXW76+SN092Sf7iV1Hs+8uJ0f3P5wkPa0fYJOre1r78RjJ3LNhYu5+JSZTG2s41NnHsupCyYP+/hCq/DzRRSv0BQkOYnIfN/3hooJgdafGfRKba1XNiSj6NbEtS1qZc6MSdRUVaqcA5I4j5apjRxQ+mBr2tMem1atna29dS9H91pHdx+HDjkmj6/zGiUk8XRCXmgy0whSiK8UkcnAD4GLiBww54vIQ0Riy23AaSLyLJFv+BagnsjKdzuwHNhKZNvb5Jz77pEdDFaIHzPtXYW41p8Z9EptrVd2UopuTdyAarp+VA09fX3UlmdWTSd1Hlu278yq5s6lPW2foFNr+9qbO3UcVZVljKuvpqK8jPYd71CdZSEkqacT8iGEZUpe/RZCIS4i/4XonF4DeuJvjyJKStOINjY4FdhBlIDGAo8CTUQ+4wuIklMHUXL6P9n6O37pMvfAI/5te8Kv1qV7Vnw0rdZpCa3+D71aF/r4aitl2KrthUuWup/92v8Q9fyp9elXiDvn/ocibKiJ9FZgvYiYXa9hpAnTOUVYYjKMdJHEtC6VyckwjHRhIsw8KRPJWnzMpz0N+zp6VXFjR+k2Owxd/6tR7g2n7bej27/3WrlSNd+vfPo+tPI7NNpa0inX+fd9A3jsy2eq4g4pa3bBsGmdYRhpIyk/J0tOhmFkR4a/qWY+pHstPE+0im5tnM9nGuCp59r51YMb+Pq3V3nbS8rLW3u+mvZy8enW+Fvn0l4S1y+UGn7h1DGcu7CZi5ZOZWxtJWfMm8Dc5sx2wtp+NffosBimCFNEPiAil4vI+douC5acRGSl5+fvkb4O9b180Cq6tXE+n2mAZQtb2fLaWzRPGKs6xiS8vLXnq2kvF59ujb91Lu1BMtcvhBr+udffobuvn+rKMhyOdzr7vI4amn4192j+iOo/su/4u5ZIgF0rIjWaXgs5cpoiIl8XkRNF5GQRuUpEzoi/dxmwVESuEZFFIvIVEWmJv/f1+N+fF5HTReRzItJ8ZONxe+tFZP2unTsP+9mAoru3L3sBVxt3/a330+T50GzeuoNDhxwdnT3sfHt/1liHY+/+jqwxEN7LW3u+mvYGFOcbX9pOZ1dP1liNv3Uu7SVx/bR9+uJamkbR0lTH3o5eWpvqOGV2I91ZbJy1/Wru0XyJFOL+F/GOv4NeNw5q5lpgH5H1tuoveME8xEXkC0TJrwJYw7uPqTQOCtsHHAPsBw7x3mS5C5gD3OWcy3j3LF3W5h5+bF2wY9eu1u1XKrCTWq0LrcI+mlbrQluEJLVaV1ddNmzV9uLjl7k7H1jjjWttqk2/QhzAOXfDEd9am2dTDw7zUAzDGCa2WmcYRioxm17DMNJHQlKCkklOmjpRWeArrK0ladHWOXqU++BVBfYuH1WtNLjWEPjO05actNdO6/uurddpa0lTr7xNFbfphqyL4QXApnWGYaQMwaZ1hmGkFFOIB0Krlk2z53cucXev3sDqtS8Uvd/QCvaQftnae+Du1Ru4+RePeuNC+ptr4tpmNXHh+6bzocVTmDJ+FBe/v4WTj50wZGwuTyfki1KEGZSCJycRmTKgCPWpxof43bwm1rmoZdPq+Z1L3CvbsnulF6rf0ArskH7Z2nugs7uXgwrf8tD+5r649Vt2MbN5NDVV5ew50M07HT3MnjxmyNhcn07Ii5HoIS4iVwNjgEoiQeWNwLnAncAHgU3AfhGZARwSkX8k0i4timMujL+eDewFJgEbgFlAv4j8BbDXOfeTI/r9o4f4tOnTB/+I62+9nwWzp3qPPc2e37nEzZ4xka2v7yp6v6EV7CH9srX3QHVlBdVV/oWNkP7mmrg5k8cgIjSOrqauppLaqgo2bd87ZOzA0wndPdHTCdpHgHIhCT+nYSvEB41u3gJOAv4DOAN4HPgw8DvgTWA6UEWUyACqgbuAC+KvHyJKTv3AY0QJrh+oIUpOP810DEuXtblHH3/Se6yhV+uSIqnVuiQ8k7QrmFrFdFKrdVpCr9Y1j60atmr7+KXL3G8e8k+RJ46pTJdC/AhL3YFJ+TPx18FnlOnsvh1/3XLE938wvCMzDCMYtlpnGEYaMZtewzBSiNgGB8NBc+20dQltbSqpvdeS2sutV+EkoK1zafvU7+OmClPXkkKXJ7W1ri3/9+OquNmf/9lwDicnkhJhlqTOyTCMkU/JjJwMwygcR/XISUTGiciUQf8+TUTm5tOWVmkcWklebBVxofrVXr/QyvQkFOfaaxdSvQ76a+fzX182q5EL2qZx1uIpNNRX8Z9Om8Up8yZ6280JiR6s971CU5TkFNvx/p2IXCEiV4rI1SJyqohcJiIniMjXiPyFPyQiK2Pt1AKgc8DeN5f+tErj0EryYquIC9Wv9vqFVqYnoTjXXruQ6nXQXzuf//pTW3Yzq3kMNZXldHT1caCzj7Uv7cwYnw8acXghBlbFGjltir86ImvetcABImveMUQ2vTXxv2uB+USizp74e+/hMA/xXYe/GVqlsdZ3WevjnIuKOIQfdaH61V6/XJTpSXima+K01y4X9brmXLXXzue/HinJoXF0NaNrKykTOFQIsWwC2algHuLFZOmyNrdmrV8hrj3VtK/Whe43zat1odFulKt9y0J7cGkTi3a1bvdNlw5btb10WZt7RPEERn0Av/LBWEHcMAwvSYgwU1MQNwwjxQx/U82LROTKXLq0kZNhGF4C+TXtyanPUqg5ichOYNsR324i2vfOh8WlJy7NxzZS42Y454Z2qVMiIvfGbfuoAboG/fvGgY01ReRiYJxz7t/UHTvnSvIFrLe4kRWX5mMrpbiR8rKak2EYqcSSk2EYqaSUk9ONFjfi4tJ8bKUUNyIoiYK4YRilRymPnAzDGMFYcjIMI5VYcvIgIhNFxOtBoY1LChGpEJFgolsRmSAiDYq4GhEJv1fRCCbt90paKLnkJCJnichiT0xrLKe/SNHkx4i2rwoSF/r4RGSJiCxRHN9fAp9TtPdpEblM0d5VwHJF3AqiLcKy9dkqIhcrz3eFiHxJEXexiJykjPuEJ+ZKETlN0dZfioj3GqO/Vz4hIpco4k7THN9Io+SSE3AmcHq2AOdcO9HTQM2K9rYCryniNvPe7a2GQnt8lUC5or3FRN5XPl6NXz4OkcGm5giexPNEVawKnhO/MhKf7zZgo6Lf2YB/10pYCLxfEfcg/se46oF5irZ2oVNyv0R0v/gYF798HEu0CW1JUYrP1t0LtChjsyZnETkOOBl4Q9HWQiK/qgeztHcR8GIc52McniQhIuOBF5TtnUyUULK1VwsMJO5scarr4pxbJSJlyuM7H9iO/0N7h+/4YjYBD2QLEJEm4HKyvGcx7eg+K7XoznU+0Uaz2Y7tYqIknLU9EZkD/BroVfQ7oijFkdMMILOv6bvUAfuyTSWccy8AP+HdzUKHJL6RBP+NKUBH/PKxC3jHE7OIaKfkDynauxnw1X4mEiXFt7MFxddlA7Db16lz7mfu8I1XM/EAcF+2gDgpfpBo23sfAvimOl1E98pBT5x2VNICtCimp95anXNuFdHI2Td6nkyU2DVT7BFFKSanHvw3JcRvvHPu55kC4g/DnxBNJTIS30iVRFuw++JeI9qW3ceDeP5aO+ceIhohPK9o70pfv865bcAHgHMU7c0imvKGYi5wYraAOCkeQPf+zsEzmnDOHQBOAI73tLUP/+gKYA3wfLZ7KkYzXYeoTHBvtgDn3MPATqLRWElRisnJe1PGw/nNwCvZ4uIPw1hlv7ejqzdcgC45XUJkZ+xjubK976KbcvyMd7eTz8Y9gM4IXccYQFPY3wtonmxfg36qI57RzhtEoxMf0/AkbBFp5V3bah/HA6coY9uVcSOGUkxO3kIt0XB+B5DVtV5ERgEvE9USssU1Et2YKxTHdzue2k/Mk8CXFHG3oSvEXw00KuKWEyUKHyvRLSh4ietS29F9aM8Asu48GU+zz0BXEJ8JvJNptBP/IfsQoLEdqcDzByou/q9Ht+jwJp5RlohMJvojUXKPepRUchpUqM36oYmH8ycBd3ua/CjRKo3vRjqV6MOgGTktQDdCmA9cp4hbhG61bhW6EVGfL0BEqoH7CTetu5xoxUkz0jmIZ7Uunj6/EL98PEf2Pz4fIJrWHekXNhQH4ngfn0T32RuHf7Wug+h91az+jShKKjkNKmA/ogjvB871tHcb0bLvg564Xzrn/sE592PlofqmEQDTgWWKtrQShpPwLIfHUw7/FiMwnqh4HqrO8SDRyCkr8YhoD7pRQiW6ZDeRLH984vf275VF/VlEiczHnfgXJwB+Q/RHICPOuX3AhehGiSOKkkpOg1ZzpirCtTcS6AqwWrJOIwahrZlMIVqxUeFZnWwnWsWsy9aGc+4NolXR57T9etprJ6qvZS3sO+dWOeducM79b0WzLcruxynjNPhGYQOcCfxeEXchOgHw3YB/x88RRkklp3jkNB3dapP2RppLWA2Jtt8leJJEzC48S/rxiOgPwB88q5MXEy+HK/rtVMZ5yVEioOVxPAseMe3A68PtLD6HxXhKCvF78Tq6hZY7gbsUcYuIap4lRUklp5hn8axyxTdIJbqi5D6gNcBxDVBBNOrISJwkxuAp2Mc04Sl0xyOTBjxJLK7VrEEnTRiHrnbmJcc/KlrOwKPEj1lAgClRfA4P4SkpxO+FdpVTO3JqR/eejShKMTkdwiNyjG8Qb9xAOOCUz+Fp2Zu1wyhJbEVX/9mJborQjO4RDO9yeIx2dU3Lw8CjAdu7B5352i1EqvMQzEC3ONGibG8joNlnfgl67dSIoeQeX1HUcnKNu2F4R/Qu8YioEUVB1zl3i7LZjxAlqPWeuHfQTXO8y+ExLURJ+5M5LAT4+p1PNJUJwVlEtZ1/9MRdSPSH6voAfW5DV6x/RBl3Oro/PANT7N8pYkcMpThySi3xiKgKnWjSS5zsFqPQVznnrnfOZX3WLOYAoNlSuoNIL7ZVEauhlnCJCaLR6V7FiPcldAkgK3HN6TQ8U/aYOXgeiYnf26fxP8IEUXKtV8SNKEpu5DQCWEeOmwtmIn6wtjJEW4NoQFeLewl4yzn3bKB+m4mW9YNMFXMY8c4jqhUNt78XRGQTuhHRs764+L1dSfQ4lo/pyn5HFJacis956J6+V+Gc+0mIdgah/Qt8BpGCOVRySuov/2OE+xy0oVP/HyISa67LFCAiVeiEnxCNEkPW/1KBJafi81t0Q/+k2IHfpqOR6IFUzQcxWL8FYgmRIDdjosiBn6NzMDgXz6IIUW2yHoWcxDkXol6WOiw5FZH4OS2tpUsiKAvxpwLHET1vplpYCNRvIahFoU5XcgX+B38vJkpMWafOzrk34tjx+B+zKkksORWX04mW6k8n0Ic6CZxzvwR+mfBhhML7LKGGuCD+BopaUg7NPgxUD+e4RjK2b51hpBQR+SrRo04lOW3zYSMnw0gv96ITYZYkpnMyjPRyFZET61GJJSfDSC+/B55I+iCSwpKTYaSXXiJh6lGJJSfDSC+aR1dKFlutMwwjldjIyTCMVGLJyTCMVGLJyciIiHxBRK7J8LOVIjJz0L/fs9lB/FT9H78e+f/xv0+PH+sxjMOwmpORERG5muhZsclEFilnAb8i2pm3l8iUronIwfIiIuuRD8T/ngX0O+dujhNSY/w7A+4DzxPZ424GHnDOaQzujKMIGzkZ2djlnLubyH9qK5HuZm/8M3fE163x180MvftsD1GSG4ivIbJbCfJsm1F62MjJMIxUYiMnwzBSiSUnwzBSiSUnwzBSiSUnwzBSiSUnwzBSiSUnwzBSiSUnwzBSyf8HYa82pmrYcacAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 324000x324000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 20\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - best case\")\n",
    "best = np.argmax(test_scores_pause)\n",
    "print(\"Accuracy:\", round(test_scores_pause[best], 5))\n",
    "print(\"F1:\", round(all_f1[best], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_confusion_pause[best], \"montalbano_pause_best_12_9_2021.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - worst case\")\n",
    "worst = np.argmin(test_scores_pause)\n",
    "print(\"Accuracy:\", round(test_scores_pause[worst], 5))\n",
    "print(\"F1:\", round(all_f1[worst], 5))\n",
    "print(\"===============================\")\n",
    "displayConfMat(all_confusion_pause[worst], \"montalbano_pause_worst_12_9_2021.png\")\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"Results for CNNLSTM - avg case\")\n",
    "worst = np.mean(test_scores_pause)\n",
    "print(\"Accuracy:\", round(np.mean(test_scores_pause), 5))\n",
    "print(\"F1:\", round(np.mean(all_f1), 5))\n",
    "print(\"===============================\")\n",
    "temp = []\n",
    "for conf in all_confusion_pause:\n",
    "    temp.append(np.array(conf))\n",
    "displayConfMat(np.mean(np.array(temp), axis=0), \"montalbano_pause_avg_12_9_2021.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the model\n",
    "torch.save(all_models_pause[0].state_dict(), \"/model_pause_1_12_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_models_pause[1].state_dict(), \"/model_pause_2_12_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_models_pause[2].state_dict(), \"/model_pause_3_12_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_models_pause[3].state_dict(), \"/model_pause_4_12_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_models_pause[4].state_dict(), \"/model_pause_5_12_9_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
